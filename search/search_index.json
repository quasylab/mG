{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"libmg","text":"<p>Welcome to the home page of libmg, the Python implementation of the \\(\\mu\\mathcal{G}\\) language for programming graph neural networks.</p> <p>You can find the installation instructions here.</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>In the tutorials section you can find a quick overview of the library's functionalities.</p>"},{"location":"#how-to-guides","title":"How-to Guides","text":"<p>You can check out the how-to guides to gain quick insights on how to use the library in your work. </p>"},{"location":"#reference","title":"Reference","text":"<p>The full reference for the grammar and the public APIs.</p>"},{"location":"#explanation","title":"Explanation","text":"<p>The semantics of \\(\\mu\\mathcal{G}\\) and details on some of the algorithms in libmg can be accessed in this section.</p>"},{"location":"explanation/","title":"Explanation","text":"<p>In this part of the documentation we focus on the theoretical background of \\(\\mu\\mathcal{G}\\).</p>"},{"location":"explanation/#syntax-and-semantics-of-g","title":"Syntax and Semantics of \u03bcG","text":"<p>The \\(\\mu\\mathcal{G}\\) language has a precise syntax and semantics, which we presented in a prior publication<sup>1</sup>. Here you will find the abstract syntax, the typing, and the denotational semantics of the language.</p>"},{"location":"explanation/#normalization","title":"Normalization","text":"<p>A \\(\\mu\\mathcal{G}\\) generated according to its syntax may be amenable to simplification, which makes it easier to analyze by the compiler. Here you can find how the normalization process works and how \\(\\mu\\mathcal{G}\\) expressions are simplified when the compiler invokes the normalizer.</p>"},{"location":"explanation/#explanation-of-g-models","title":"Explanation of \u03bcG models","text":"<p>We show here what kind of explanations libmg can automatically generate from a \\(\\mu\\mathcal{G}\\) model.</p> <ol> <li> <p>Matteo Belenchia, Flavio Corradini, Michela Quadrini, and Michele Loreti. 2023. Implementing a CTL Model Checker with \u03bcG, a Language for Programming Graph Neural Networks. In Formal Techniques for Distributed Objects, Components, and Systems: 43<sup>rd</sup> IFIP WG 6.1 International Conference, FORTE 2023, Held as Part of the 18<sup>th</sup> International Federated Conference on Distributed Computing Techniques, DisCoTec 2023, Lisbon, Portugal, June 19\u201323, 2023, Proceedings. Springer-Verlag, Berlin, Heidelberg, 37\u201354. https://doi.org/10.1007/978-3-031-35355-0_4 \u21a9</p> </li> </ol>"},{"location":"explanation/explanation/","title":"Explanation of \u03bcG models","text":"<p>libmg allows for a rudimentary kind of \"explanation\" for a model's output. Given a model \\(M\\), the input graph \\(\\mathtt{G}\\), and a query node \\(q\\) it generates the sub-graph of \\(\\mathtt{G}\\) that contains the node and edges that \\(M\\) used to determine the output label of \\(q\\). It then shows this sub-graph using the visualization functions in a hierarchical perspective with the query node on top followed below by the other nodes ordered by hop count.</p> <p>The algorithm that computes the sub-graph is a \\(\\mu\\mathcal{G}\\) program itself, and is produced by transforming the expression that generated the model to be explained. In the case that the model to be explained contained \\(\\psi\\) functions that used non-local information or the <code>if-then-else</code> construct, every node can potentially depend on every node in the graph and the procedure just yields the entire graph, visualized in a hierarchical view. Otherwise, the transformation function is defined as \\(\\mathtt{explain}(\\mathcal{N}, q) = \\psi_q ; \\mathcal{E}(\\mathcal{N})\\), where \\(\\mathcal{E}\\) is defined inductively as:</p> \\[ \\begin{align*} \\mathcal{E}(\\psi) &amp;= \\psi_{id} \\\\ \\mathcal{E}(\\lhd_{\\sigma}^{\\varphi}) &amp;= \\rhd_{\\sigma_{min}}^{\\varphi_{pr_1}} \\\\ \\mathcal{E}(\\rhd_{\\sigma}^{\\varphi}) &amp;= \\lhd_{\\sigma_{min}}^{\\varphi_{pr_1}} \\\\ \\mathcal{E}(\\mathcal{N}_1 || \\mathcal{N}_2) &amp;= (\\mathcal{E}(\\mathcal{N}_1) || \\mathcal{E}(\\mathcal{N}_2)) ; \\psi_{min} \\\\ \\mathcal{E}(\\texttt{fix } X = \\mathcal{N}_1 \\texttt{ in } \\mathcal{N}_2) &amp;= \\texttt{repeat } X = \\mathcal{E}(\\mathcal{N}_1) \\texttt{ in } \\mathcal{E}(\\mathcal{N}_2) \\texttt{ for } k \\\\ \\mathcal{E}(\\mathcal{N}(\\mathcal{N}_1, \\mathcal{N}_2, \\ldots)) &amp;= \\mathcal{N}(\\mathcal{E}(\\mathcal{N}_1), \\mathcal{E}(\\mathcal{N}_2), \\ldots) \\end{align*} \\] <p>where \\(\\psi_q\\) denotes the function that sets the label of node \\(q\\) to 1 and that of all other nodes to 0, \\(\\psi_{id}\\) denotes the identity function, \\(\\psi_{min}\\) denotes the minimum function, \\(\\varphi_{pr_1}\\) is the function that returns the first argument, and \\(\\sigma_{min}\\) denotes a function that aggregates the messages by taking the minimum between the label of the receiver node and the minimum of the inbound messages plus one. The value \\(k\\) is the number of iterations performed by the corresponding fixpoint layer. The algorithm works by propagating from the query node and building up the hierarchical view of the relevant sub-graph.</p>"},{"location":"explanation/reduction/","title":"Normalization","text":"<p>A \\(\\mu\\mathcal{G}\\) expression as generated from its abstract syntax may be simplified (i.e. normalized) to a reduced, simpler form. This simplification is useful in practice because it may result in less computations to be performed for the same output, or it may make it easier to compile. All \\(\\mu\\mathcal{G}\\) expressions in libmg are automatically normalized before compilation.</p> <p>For the time being, only two types of normalizations are performed by libmg,  the first, is the simplification of fixpoint expressions where the variable does not occur in the expression body, the second is the rewriting of sequential composition expressions where the fixpoint variable occurs on the right hand side.</p>"},{"location":"explanation/reduction/#simplification-of-unoccurring-fixpoint-variables","title":"Simplification of unoccurring fixpoint variables","text":"<p>An expression of the form <code>fix X = N1 in N2</code> where <code>X</code> does not occur in <code>N2</code> is equivalent to just <code>N2</code>. For example, the expression <code>fix X = 0 in (a || b);+</code>. Clearly, the value of <code>X</code> is not necessary to compute the sum <code>a + b</code>  and we can simplify it to <code>(a || b);+</code>.</p> <p>Warning</p> <p>This simplification is also applied in the case of the <code>repeat</code> macro.</p>"},{"location":"explanation/reduction/#simplification-of-sequential-composition-of-fixpoint-variables","title":"Simplification of sequential composition of fixpoint variables","text":"<p>Expressions of the form <code>fix X = N in N1(X) ; N2(X)</code> or <code>fix X = N in N1 ; N2(X)</code> where the fixpoint variable <code>X</code>  occurs on the right-hand side (rhs) of a sequential composition expressions can be simplified to an expression of the form <code>fix X = N in N3(X) ; N4</code> where all the fixpoint variables now only occur on the left-hand side (lhs).</p> <p>Given the sequential composition of a lhs and rhs, we discard the lhs and we visit the parse tree of the rhs:</p> <ul> <li>For any fixpoint variable encountered, it is returned as-is.</li> <li>For any atomic operation \\(a\\) (function application, or any of the images) encountered, we return the sequential composition of the lhs with \\(a\\).</li> <li>For any sequential composition, the lhs is evaluated according to this algorithm and then this procedure is applied recursively to the rhs given the new lhs.</li> <li>Any other expression is returned as-is.</li> </ul> <p>For example, consider the expression <code>fix X = N in X ; X</code>. We analyze <code>X ; X</code> by discarding the lhs (<code>X</code>) and visit the rhs (<code>X</code>). Since it's the fixpoint variable, it is returned as is and we are finished with the result being just <code>X</code>, and the fixpoint expression has become <code>fix X = N in X</code>.</p> <p>Now, for a more involved example, consider <code>fix X = N in (a ; (X || b) ; c)</code>. We analyze <code>a ; (X || b) ; c</code> by discarding the lhs (<code>a</code>) and visit the rhs (<code>(X || b) ; c</code>). The variable <code>X</code> is left as-is, the function application <code>b</code> becomes <code>a ; b</code>. The parallel composition is left as-is and we have <code>(X || (a;b))</code>. Since this is the lhs of another sequential composition, this become the new lhs for the following and we repeat this procedure with the current  rhs <code>c</code>. The rhs <code>c</code> is a function application  which becomes <code>(X || (a;b)) ; c</code> according to the new lhs, and this is the output of the algorithm. The fixpoint expression thus becomes <code>fix X = N in ((X || (a;b)) ; c)</code>.</p> <p>Warning</p> <p>A fixpoint variable is always assumed to be a constant GNN that returns the node labeling function obtained from the application of the  fixpoint variable initialization GNN (N in the expressions above) to the input node labeling function of the fixpoint expression.</p>"},{"location":"explanation/semantics/","title":"Syntax and Semantics of \u03bcG","text":"<p>The syntax and denotational semantics of \\(\\mu\\mathcal{G}\\) were published in our FORTE 2023 article<sup>1</sup> The operational semantics will be added here after being published in a future article.</p>"},{"location":"explanation/semantics/#syntax","title":"Syntax of \\(\\mu\\mathcal{G}\\)","text":"<p>We start by recalling the abstract syntax of \\(\\mu\\mathcal{G}\\)</p> <p>Syntax of \\(\\mu\\mathcal{G}\\)</p> <p>Given a set \\(\\mathcal{X} = \\{X, Y, Z, \\ldots \\}\\) of variable symbols and a set \\(\\mathcal{S}\\) of function symbols, \\(\\mu\\mathcal{G}\\) expressions can be  composed with the following abstract syntax:</p> \\[\\begin{align*}  \\mathcal{N} ::=&amp; \\; \\psi \\mid \\lhd_{\\sigma}^{\\varphi} \\mid \\rhd_{\\sigma}^{\\varphi} \\mid \\mathcal{N}_1 ; \\mathcal{N}_2 \\mid \\mathcal{N}_1 || \\mathcal{N}_2 \\mid X  \\mid f (\\mathcal{N}_1, \\mathcal{N}_2, \\ldots, \\mathcal{N}_k) \\mid\\\\&amp; \\texttt{let } X = \\mathcal{N}_1 \\texttt{ in } \\mathcal{N}_2 \\mid \\texttt{def } f (\\tilde{X}) \\{ \\mathcal{N}_1 \\} \\texttt{ in } \\mathcal{N}_2  \\mid \\\\ &amp; \\texttt{if } \\mathcal{N}_1 \\texttt{ then } \\mathcal{N}_2 \\texttt{ else } \\mathcal{N}_3 \\mid \\texttt{fix } X = \\mathcal{N}_1 \\texttt{ in } \\mathcal{N}_2 \\end{align*}\\] <p>with \\(\\varphi, \\sigma, \\psi, f \\in \\mathcal{S}\\) and \\(X \\in \\mathcal{X}\\) .</p> <p>Given a graph \\(\\mathtt{G}\\) and an (optional) edge-labeling \\(\\xi\\), the meaning of a \\(\\mu\\mathcal{G}\\) expression is a graph neural network, a function between node-labeling functions. </p> <p>One of the basic \\(\\mu\\mathcal{G}\\) terms is the function application \\(\\psi\\). This represents the application of a given function (referenced by \\(\\psi\\)) to the input node-labeling function. </p> <p>Moreover, the pre-image operator \\(\\lhd_{\\sigma}^{\\varphi}\\) and the post-image operator \\(\\rhd_{\\sigma}^{\\varphi}\\) are used to compute the labeling of a node in terms of the labels of its predecessors and successors, respectively.</p> <p>Basic terms are composed by sequential composition \\(\\mathcal{N}_1 ; \\mathcal{N}_2\\) and parallel composition \\(\\mathcal{N}_1 || \\mathcal{N}_2\\). </p> <p>Local variables \\(X\\) and functions \\(f(\\tilde{X})\\) can be defined using <code>let</code> and <code>def</code>.</p> <p>The choice operator \\(\\texttt{if } \\mathcal{N}_1 \\texttt{ then } \\mathcal{N}_2 \\texttt{ else } \\mathcal{N}_3\\) allows to run different GNNs according to the result of a guard GNN.</p> <p>Finally, the fixed point operator \\(\\texttt{fix } X = \\mathcal{N}_1 \\texttt{ in } \\mathcal{N}_2\\) is used to program recursive behavior.</p>"},{"location":"explanation/semantics/#typing","title":"Typing of \\(\\mu\\mathcal{G}\\)","text":"<p>We say that a \\(\\mu\\mathcal{G}\\) expression is well-formed whenever it can be typed with the rules of the following table. These rules guarantee that any well-formed \\(\\mu\\mathcal{G}\\) expression can be interpreted as a GNN.</p> <p></p>"},{"location":"explanation/semantics/#semantics","title":"Denotational semantics of \\(\\mu\\mathcal{G}\\)","text":"<p>We are now ready to discuss the denotational semantics of \\(\\mu\\mathcal{G}\\). In the following definition, \\(\\eta\\) denotes a node-labeling function, \\(O_{\\mathtt{G}}(v)\\) and \\(I_{\\mathtt{G}}(v)\\) are the outgoing and incoming edges of node \\(v\\), and by \\((\\eta, \\xi)(E)\\) we denote the multi-set of tuples \\((\\eta(u), \\xi((u, v))), \\eta(v))\\) for each \\((u, v) \\in E\\).</p> <p>Denotational semantics of \\(\\mu\\mathcal{G}\\)</p> <p>Given a graph \\(\\mathtt{G}\\), an edge-labeling function \\(\\xi\\), and an environment \\(\\rho\\) that comprises a variable evaluation function \\(\\rho_v: \\mathcal{X} \\rightarrow \\Phi_{\\mathtt{G}, \\xi}\\) and a function evaluation function \\(\\rho_f: \\mathcal{S} \\rightarrow ((\\Phi_{\\mathtt{G}, \\xi} \\times \\ldots \\times \\Phi_{\\mathtt{G}, \\xi}) \\rightarrow \\Phi_{\\mathtt{G}, \\xi})\\) we define the semantic interpretation function \\(\u27e6\\cdot\u27e7_{\\rho}^{\\mathtt{G},\\xi}: \\mathcal{N} \\rightarrow \\Phi_{\\mathtt{G}, \\xi}\\) on \\(\\mu\\mathcal{G}\\) formulas \\(\\mathcal{N}\\) by induction in the following way:</p> <p></p> <p>For any \\(\\psi, \\varphi, \\sigma, f \\in \\mathcal{S}\\) and any \\(X \\in \\mathcal{X}\\). The functions \\(cond\\) and \\(g\\) are defined as follows:</p> \\[ cond(t, f_{1}, f_{2})(\\eta) = \\begin{cases}  f_{1}(\\eta) &amp; \\text{if } t(\\eta) = \\lambda v . \\mathtt{True} \\\\ f_{2}(\\eta) &amp; \\text{otherwise} \\end{cases} \\] \\[   g(\\Phi)(\\eta) = \\begin{cases} \\Phi(\\eta) &amp; \\text{if } \\Phi(\\eta) = \u27e6\\mathcal{N}_2\u27e7_{\\rho[X \\leftarrow \\Phi]}^{\\mathtt{G},\\xi}(\\eta)\\\\ g(\u27e6\\mathcal{N}_2\u27e7_{\\rho[X \\leftarrow \\Phi]}^{\\mathtt{G},\\xi})(\\eta) &amp; \\text{otherwise} \\end{cases}   \\]"},{"location":"explanation/semantics/#function-application","title":"Function application","text":"<p>The function symbols \\(\\psi_1, \\psi_2, \\ldots \\in \\mathcal{S}\\) are evaluated as the graph neural networks \\(\u27e6\\psi_1\u27e7_{\\rho}^{\\mathtt{G},\\xi}, \u27e6\\psi_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}, \\ldots\\) that map a node-labeling function \\(\\eta\\) to a new node-labeling function by applying a function on both local and global node information. The local information consists of applying \\(\\eta\\) to the input node, while the global information is the multiset of the labels of all the nodes in the graph. The graph neural network we obtain applies a possibly trainable function \\(f_\\psi\\) to these two pieces of information. Two particular cases arise if \\(f_\\psi\\) decides to ignore either of the two inputs. If \\(f_\\psi\\) ignores the global information, the GNN returns a node-labeling function \\(\\eta_l\\), a purely local transformation of the node labels. On the other hand, if \\(f_\\psi\\) ignores the local information, the GNN returns a node-labeling function \\(\\eta_g\\) that assigns to each node a label that summarizes the entire graph, emulating what in the GNN literature is known as a global pooling operator.</p>"},{"location":"explanation/semantics/#pre-image-and-post-image","title":"Pre-image and Post-Image","text":"<p>The pre-image symbol \\(\\lhd\\) and the post-image symbol \\(\\rhd\\), together with function symbols \\(\\varphi \\in \\mathcal{S}\\) and \\(\\sigma \\in \\mathcal{S}\\) are evaluated as the graph neural networks \\(\u27e6\\lhd_{\\sigma}^{\\varphi}\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) and \\(\u27e6\\rhd_{\\sigma}^{\\varphi}\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) for any \\(\\sigma, \\varphi \\in \\mathcal{S}\\). In the case  of the pre-image, for any symbol \\(\\varphi \\in \\mathcal{S}\\) the corresponding function \\(f_\\varphi\\) generates a message from a tuple \\((\\eta(u), \\xi((u, v)), \\eta(v))\\), and this is repeated for each \\((u, v) \\in I_{\\mathtt{G}}(v)\\). Then for any symbol \\(\\sigma \\in \\mathcal{S}\\) the corresponding function \\(f_\\sigma\\) generates a new label for a node \\(v\\) from the multiset of messages obtained from \\(f_\\varphi\\) and the current label \\(\\eta(v)\\). The functions \\(f_\\varphi\\) and \\(f_\\sigma\\) may be trainable. The case of the post-image is analogous, with the difference that \\(f_\\varphi\\) is applied to tuples \\((\\eta(v), \\xi((v, u)), \\eta(u))\\) for each \\((v, u) \\in O_{\\mathtt{G}}(v)\\) instead.</p>"},{"location":"explanation/semantics/#sequential-composition","title":"Sequential composition","text":"<p>An expression of the form \\(\\mathcal{N}_1 ; \\mathcal{N}_2\\) for \\(\\mu\\mathcal{G}\\) formulas \\(\\mathcal{N}_1, \\mathcal{N}_2\\) is evaluated as the graph neural network \\(\u27e6\\mathcal{N}_1 ; \\mathcal{N}_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) that maps a node-labeling function \\(\\eta\\) to a new node-labeling function obtained from the function composition of \\(\u27e6\\mathcal{N}_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) and \\(\u27e6\\mathcal{N}_1\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\).</p>"},{"location":"explanation/semantics/#parallel-composition","title":"Parallel composition","text":"<p>An expression of the form \\(\\mathcal{N}_1 || \\mathcal{N}_2\\) for \\(\\mu\\mathcal{G}\\) formulas \\(\\mathcal{N}_1, \\mathcal{N}_2\\) is evaluated as the graph neural network \\(\u27e6\\mathcal{N}_1 || \\mathcal{N}_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) that maps a node-labelling function \\(\\eta\\) to a new node-labelling function that maps a node \\(v\\) to the tuple \\(\\langle \u27e6\\mathcal{N}_1\u27e7_{\\rho}^{\\mathtt{G},\\xi}(\\eta)(v), \u27e6\\mathcal{N}_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}(\\eta)(v)\\rangle\\). Moreover, combining this operator with the basic function symbols \\(\\psi_1, \\psi_2, \\ldots \\in \\mathcal{S}\\) allows the execution of \\(k\\)-ary operations on the node-labelling functions: a \\(k\\)-ary function application \\(f_{op_k}(\\mathcal{N}_1, \\ldots, \\mathcal{N}_k)\\) is expressible as the \\(\\mu\\mathcal{G}\\) expression \\(((\\ldots(\\mathcal{N}_1 || \\mathcal{N}_2) || \\ldots) || \\mathcal{N}_k);op_k\\).</p>"},{"location":"explanation/semantics/#choice","title":"Choice","text":"<p>The choice operator \\(\\texttt{if } \\mathcal{N}_1 \\texttt{ then } \\mathcal{N}_2 \\texttt{ else } \\mathcal{N}_3\\) for \\(\\mu\\mathcal{G}\\) formulas \\(\\mathcal{N}_1, \\mathcal{N}_2,  \\mathcal{N}_3\\) is evaluated as the graph neural network \\(\u27e6\\mathcal{N}_2\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) if \\(\\eta' = \u27e6\\mathcal{N}_1\u27e7_{\\rho}^{\\mathtt{G},\\xi}(\\eta)\\) is a node-labelling function such that \\(\\forall v \\in \\mathtt{G}, \\eta'(v) = \\mathtt{True}\\). Otherwise it is evaluated as the graph neural network \\(\u27e6\\mathcal{N}_3\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\).</p>"},{"location":"explanation/semantics/#fixed-points","title":"Fixed points","text":"<p>The fixed point operator is evaluated as the graph neural network that is obtained by applying \\(\u27e6\\mathcal{N}_1\u27e7_{\\rho}^{\\mathtt{G},\\xi}\\) to the least fixed point of the functional \\(\\mathcal{F}\\) associated to \\(g\\). Formally, it is \\(\\bigsqcup\\{\\mathcal{F}^n \\mid n \\geq 0\\}(\u27e6\\mathcal{N}_1\u27e7_{\\rho}^{\\mathtt{G},\\xi})\\) where</p> \\[ \\begin{align*} &amp;\\mathcal{F}^0 = \\lambda \\Phi . \\lambda \\eta . \\bot \\\\ &amp;\\mathcal{F}^{n+1} = \\mathcal{F}(\\mathcal{F}^n) \\\\ \\end{align*} \\] <ol> <li> <p>Matteo Belenchia, Flavio Corradini, Michela Quadrini, and Michele Loreti. 2023. Implementing a CTL Model Checker with \u03bcG, a Language for Programming Graph Neural Networks. In Formal Techniques for Distributed Objects, Components, and Systems: 43<sup>rd</sup> IFIP WG 6.1 International Conference, FORTE 2023, Held as Part of the 18<sup>th</sup> International Federated Conference on Distributed Computing Techniques, DisCoTec 2023, Lisbon, Portugal, June 19\u201323, 2023, Proceedings. Springer-Verlag, Berlin, Heidelberg, 37\u201354. https://doi.org/10.1007/978-3-031-35355-0_4 \u21a9</p> </li> </ol>"},{"location":"how-to_guides/","title":"How-to Guides","text":"<p>Here you can find some guides that will help you get up to speed in using libmg in your work.</p>"},{"location":"how-to_guides/#installation","title":"Installation","text":"<p>libmg can be installed with pip (recommended) or from source. In this section you will find the prerequisites  and the means to verify your installation.</p>"},{"location":"how-to_guides/#how-to-define-a-dataset","title":"How to define a dataset","text":"<p>A \\(\\mu\\mathcal{G}\\) model is run on a given dataset. Here you will find how to define a dataset of graphs.</p>"},{"location":"how-to_guides/#how-to-define-functions","title":"How to define functions","text":"<p>A \\(\\mu\\mathcal{G}\\) is built by assembling the basic terms from \\(\\psi\\), \\(\\phi\\) and \\(\\sigma\\) functions. Here you will find how these functions can be defined.</p>"},{"location":"how-to_guides/#how-to-define-a-compiler","title":"How to define a compiler","text":"<p>The compiler for \\(\\mu\\mathcal{G}\\) expressions must be instantiated given the functions that are to be used and some configuration information. Here you will  find what kind of configuration is needed.</p>"},{"location":"how-to_guides/#how-to-create-run-and-optimize-models","title":"How to create, run, and optimize models","text":"<p>Creating a \\(\\mu\\mathcal{G}\\) model consists in compiling a \\(\\mu\\mathcal{G}\\) expression. The model can then receive graphs in input using the adequate dataset loader. Models can also be traced by the compiler using dummy data.  </p>"},{"location":"how-to_guides/#visualization-procedures","title":"Visualization procedures","text":"<p>Here you will find information on how to visualize graphs and model outputs on your web browser.</p>"},{"location":"how-to_guides/compiler/","title":"How to define a compiler","text":"<p>This guide shows you how to create a compiler instance for \\(\\mu\\mathcal{G}\\) expressions. For this purpose we will need to import the following:</p> <pre><code>from libmg import MGCompiler, CompilerConfig, NodeConfig, EdgeConfig\n</code></pre>"},{"location":"how-to_guides/compiler/#node-and-edge-configuration","title":"Node and edge configuration","text":"<p>We start by creating the <code>NodeConfig</code> and, if needed, the <code>EdgeConfig</code> instances. The <code>NodeConfig</code> specifies the type and size of the feature vectors  associated to each node, that is, the elements on each row of the node features matrix \\(X\\). If, for example, we have that each node is assigned a  floating-point value, we will have </p> <pre><code>node_conf = NodeConfig(tf.float32, 1)\n</code></pre> <p>The first argument for <code>NodeConfig</code> is the type, expressed using TensorFlow types, while the latter is the integer value specifying the size of the feature  vectors. In the same way we can specify the type and size of the feature vectors associated to each edge, if any. For example, an edge configuration that  specifies two Boolean values on each edge is:</p> <pre><code>edge_conf = EdgeConfig(tf.bool, 2)\n</code></pre>"},{"location":"how-to_guides/compiler/#compiler-configuration","title":"Compiler configuration","text":"<p>The <code>CompilerConfig</code> instance will hold the <code>NodeConfig</code> and <code>EdgeConfig</code> objects, plus the type of values in the adjacency matrix and the dictionary of the  tolerance values for the fixpoint computations. The type of the adjacency matrix entries will typically be integers, so <code>tf.int32</code> or even <code>tf.uint8</code> are  recommended, but the correct type to use depends on the way the adjacency matrix was defined in the <code>Graph</code> objects. </p> <p>The tolerance values are specified as a dictionary, where the key is the name of a numeric type (e.g. <code>float32</code>, <code>int64</code>, etc.) and the value is a  floating-point number, typically in the order of the thousandth (0.001) to the billionth (0.000000001), specifying the maximum absolute tolerance between two  values to declare them to be the same value.</p> <p>The <code>CompilerConfig</code> instance is supposed to be created by using one of the static constructor methods offered:</p> <ul> <li><code>xa_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float])</code> with alias <code>single_graph_no_edges_config</code></li> <li><code>xai_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float])</code> with alias <code>multiple_graphs_no_edges_config</code></li> <li><code>xae_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float])</code> with alias    <code>single_graph_with_edges_config</code> </li> <li><code>xaei_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float])</code> with alias <code>multiple_graphs_with_edges_config</code></li> </ul> <p>The correct method to use depends on the graphs and loader that will be used with the compiler. The <code>xa_config</code> method is used for datasets containing a  single graph with no edge labels, <code>xae_config</code> is for datasets containing a single graph with edge labels, and <code>xai_config</code> and <code>xaei_config</code> are used for  datasets containing multiple graphs. To create the <code>CompilerConfig</code> simply call the correct method based on your use case by passing in the required  <code>NodeConfig</code>, <code>EdgeConfig</code> and the other parameters.</p> <pre><code>conf = CompilerConfig.xae_config(node_conf, edge_conf, tf.uint8, {'float32': 0.001})\n</code></pre>"},{"location":"how-to_guides/compiler/#instantiating-the-compiler","title":"Instantiating the compiler","text":"<p>The <code>MGCompiler</code> class is instantiated by passing in the dictionaries of the \\(\\psi\\), \\(\\varphi\\), and \\(\\sigma\\) functions (see How to define functions) and the <code>CompilerConfig</code> instance.</p> <pre><code>compiler = MGCompiler(psi_functions=..., phi_functions=..., sigma_functions=..., config=conf)\n</code></pre>"},{"location":"how-to_guides/datasets/","title":"How to define a dataset","text":"<p>This guide shows you how to define datasets of graphs, which are the inputs of \\(\\mu\\mathcal{G}\\) models. <code>Dataset</code>s are containers for one or more <code>Graph</code>  objects, and they are implemented in Spektral. libmg imports  their implementation, and in fact you will see that this guide is very similar to the one linked above. Datasets are defined by subclassing them and  overriding the <code>read</code> and <code>download</code> methods. Then they can be instantiated by providing a name. In the following sections, we will go over these steps.</p>"},{"location":"how-to_guides/datasets/#defining-the-dataset","title":"Defining the dataset","text":"<p>We start by importing the <code>Dataset</code> and <code>Graph</code> classes from <code>libmg</code>. We will also be importing <code>os</code>, <code>numpy</code> and <code>scipy</code> which will be useful later.</p> <pre><code>import os\nimport numpy as np\nfrom scipy.sparse import coo_matrix\nfrom libmg import Dataset, Graph\n</code></pre> <p>We can define a new dataset by subclassing <code>Dataset</code>. In the <code>__init__</code> method we are only required to pass a string name to the parent class, but as usual we can also provide additional arguments that we may need. </p> <p><pre><code>class MyDataset(Dataset):\n    def __init__(self, name, arg1, arg2, ...):\n        super().__init__(name)\n        self.arg1 = arg1\n        self.arg2 = arg2\n        ...\n\n    def read(self):\n        pass\n\n    def download(self):\n        pass\n</code></pre> When a <code>Dataset</code> is instantiated, the <code>__init__</code> will call <code>download</code> first (if necessary, see below), followed by <code>read</code>. The list of <code>Graph</code> objects  returned by <code>read</code> will constitute the contents of our dataset.</p> <p>The <code>download</code> method is supposed to create the raw data of the dataset. It is called if a directory named <code>~/spektral/datasets/[ClassName]</code> is missing. In such directory the <code>download</code> method should store the data, so that in future instantiations <code>read</code> can load this data without calling <code>download</code> again. Thus, the  <code>download</code> method will usually create this directory and save some data there, e.g. <code>.npz</code> files, <code>.csv</code> files, etc.</p> <p>The <code>read</code> method is called on every instantiation and must return a <code>list</code> of <code>Graph</code> objects. If we defined a <code>download</code> method, these <code>Graph</code> objects will  usually come from the files we saved in the <code>~/spektral/datasets/[ClassName]</code> directory. If we didn't define a <code>download</code> method, we also have the possibility of  generating these graphs on-the-fly.</p>"},{"location":"how-to_guides/datasets/#defining-a-graph","title":"Defining a <code>Graph</code>","text":"<p>A <code>Graph</code> object can be instantiated using the constructor <code>Graph(x=None, a=None, e=None, y=None)</code>. All these four arguments are optional, but in \\(\\mu\\mathcal{G}\\) you should always at least provide <code>x</code> and <code>a</code>. </p>"},{"location":"how-to_guides/datasets/#the-node-features-matrix-x","title":"The node features matrix <code>X</code>","text":"<p>Each node in the graph is assigned a vector of features. For example, when considering a citation network, each node represents a paper and will be assigned a  vector of floating-point numbers that encode the contents of that paper.</p> <p>The node features matrix <code>X</code> stores all these vectors, such that in row \\(i\\) is stored the feature vector for node \\(i\\). Therefore, this matrix will have rows  equal to the number of nodes in the graph, and columns equal to the length of the feature vectors (which will all have the same length). </p> <p>When creating a <code>Graph</code>, the <code>x</code> argument that encodes the node features matrix should be passed in as a NumPy array (<code>np.array</code>).</p>"},{"location":"how-to_guides/datasets/#the-adjacency-matrix-a","title":"The adjacency matrix <code>A</code>","text":"<p>The adjacency matrix encodes the connections of a graph. In \\(\\mu\\mathcal{G}\\) adjacency matrices are binary, i.e. they only contain zeros and ones. A value of  1 at row \\(i\\) and column \\(j\\) means that there exists a directed edge going from node \\(i\\) to node \\(j\\). A value of 0 means that there is no such edge. </p> <p>The adjacency matrix is supposed to be created as a SciPy sparse matrix in COOrdinate format (<code>coo_matrix</code> from <code>scipy.sparse</code>). A sparse matrix only contains the coordinates of the non-zero elements. In this format the adjacency matrix is specified  from three arrays: the row indices, the column indices, and the values. The values will be an array of 1s equal to the number of edges in the graph. The row and column indices are the indices of the nodes that the edges connect: the row index is the source node of the edge and the column index is the target node of  the edge. The indices should be in row-major order, that is, they are ordered according to the rows first and to the columns second.</p>"},{"location":"how-to_guides/datasets/#the-edge-features-matrix-e","title":"The edge features matrix <code>E</code>","text":"<p>As is the case for the nodes, edges can have features as well. The edge features matrix <code>E</code> has a row for each edge in the graph and columns equal to the length of their feature vectors. In this format, the feature vector in row \\(i\\) corresponds to the \\(i\\)-th edge of the graph. The \\(i\\)-th edge of the graph is the  edge corresponding to the \\(i\\)-th row index and column index of the adjacency matrix \\(A\\).</p> <p>When creating a <code>Graph</code>, the <code>e</code> argument that encodes the edge features matrix should be passed in as a NumPy array (<code>np.array</code>).</p>"},{"location":"how-to_guides/datasets/#the-true-labels-matrix-y","title":"The true labels matrix <code>Y</code>","text":"<p>Usually in machine learning we are also provided the true labels to be used for training or testing models. In \\(\\mu\\mathcal{G}\\) the labels are always meant to be node labels, that is, we are given a label for each node of the graph. </p> <p>The true labels features matrix <code>Y</code> stores the true labels vector, such that in row \\(i\\) is stored the true labels vector for node \\(i\\). Therefore, this matrix  will have rows equal to the number of nodes in the graph, and columns equal to the length of the true label vectors. </p> <p>When creating a <code>Graph</code>, the <code>y</code> argument that encodes the true labels matrix should be passed in as a NumPy array (<code>np.array</code>).</p>"},{"location":"how-to_guides/datasets/#overriding-download","title":"Overriding <code>download</code>","text":"<p>The <code>download</code> method will usually create the <code>~/spektral/datasets/[ClassName]</code> (available through <code>self.path</code>) directory and populate it with data. The data  can be generated according to some specification or downloaded from the web. So the general structure of a <code>download</code> method will be:</p> <pre><code>def download(self):\n    os.mkdir(self.path)\n    data = ...  # Obtain the data\n\n    # Save the data\n    np.savez('mydata', ...)\n</code></pre>"},{"location":"how-to_guides/datasets/#overriding-read","title":"Overriding <code>read</code>","text":"<p>The <code>read</code> method will either load up the data in <code>~/spektral/datasets/[ClassName]</code> or create it on-the-fly. What it matters is that it returns a <code>list</code> of <code>Graph</code> objects.</p> <pre><code>def read(self):\n    output = []\n    mydata = np.load(os.path.join(self.path, 'mydata.npz'))\n\n    ...\n\n    X = np.array(...)\n    A = coo_matrix(...)\n    output.append(Graph(x=X, a=A))\n\n    ...\n\n    return output\n</code></pre>"},{"location":"how-to_guides/datasets/#instantiating-the-dataset","title":"Instantiating the dataset","text":"<p>The dataset can now be instantiated by calling the constructor.  <pre><code>mydataset = MyDataset('MyFirstDataset', ...)\n</code></pre></p>"},{"location":"how-to_guides/functions/","title":"How to define functions","text":"<p>The definition of functions is the most important part of the \\(\\mu\\mathcal{G}\\) workflow. The compiler for \\(\\mu\\mathcal{G}\\) expressions is instantiated by  providing the dictionaries for the \\(\\psi\\), \\(\\varphi\\) and \\(\\sigma\\) functions. This guide will show you how to instantiate these functions and how to create  these dictionaries.</p> <p>There are three main interfaces for creating function. For the most part they are equivalent, but differences arise when defining trainable functions, as  will be explained in the following sections.</p>"},{"location":"how-to_guides/functions/#types-of-functions","title":"Types of functions","text":"<p>The three main types of functions are \\(\\psi\\), \\(\\varphi\\), and \\(\\sigma\\). The \\(\\psi\\) functions are used to transform the node labels without using edge or  neighbour information. The \\(\\varphi\\) functions are used to generate messages from a nodes neighbour, and the \\(\\sigma\\) functions are used to aggregate these  messages and update the labels of the node. These two types of functions are used in tandem in the pre-image and post-image expressions.</p> <p>The \\(\\psi\\) functions can be defined using the following classes:</p> <ul> <li><code>PsiLocal</code>: used for functions that transform individual node labels using only local (the node label itself) information without using global (the labels    of all other nodes)    information.<ul> <li><code>Constant</code>: used for functions that transform a node label to some constant value.</li> <li><code>Pi</code>: used for projection functions that transform a node label to a projection of itself.</li> </ul> </li> <li><code>PsiNonLocal</code>: used for functions that transform individual node labels using both local (the node label itself) and global (the labels of all other nodes)    information. </li> <li><code>PsiGlobal</code>: used for functions that transform all node labels to some value obtained using only global (the labels of all other nodes) information.</li> </ul> <p>The \\(\\varphi\\) functions be defined with the <code>Phi</code> class and the \\(\\sigma\\) functions can be defined with the <code>Sigma</code> class.</p> <p>Note</p> <p>When defining functions, make sure that their output maintains the shape of the node features matrix \\(X\\), which is always a rank-2 tensor. Even if the  nodes have one single feature, the node features matrix should never become a rank-1 tensor.</p> <p>Note</p> <p>The two subclasses of <code>PsiLocal</code>, <code>Constant</code> and <code>Pi</code>, are meant to be used with the constructor interface only.</p>"},{"location":"how-to_guides/functions/#constructor-interface","title":"Constructor interface","text":"<p>The constructor interface consists in instantiating the functions via their constructor. Typically, this consists in passing a lambda function to the  constructor of the class. A name can also be passed to the constructor to be used for model summaries.</p> <pre><code>import tensorflow as tf\nfrom libmg import PsiLocal, Phi, Sigma\n\nsuccessor = PsiLocal(lambda x: x + 1)\nprojection_1 = Phi(lambda i, e, j: i)\naggregate_sum = Sigma(lambda m, i, n, x: tf.math.segment_sum(m, i))\n</code></pre> <p>We can create trainable functions using <code>tf.keras.layers.Dense</code>:</p> <pre><code>dense = PsiLocal(tf.keras.layers.Dense(5, activation='relu'))\n</code></pre> <p>Using the constructor interface, whenever we reference to this <code>dense</code> function, the same <code>PsiLocal</code> instance is being used, therefore there is only one set  of weights being shared across all usages of this function.</p>"},{"location":"how-to_guides/functions/#subclassing-interface","title":"Subclassing interface","text":"<p>The subclassing interface consists in subclassing the function classes and overriding their <code>func</code> method (for <code>PsiLocal</code>, <code>Phi</code>, and <code>Sigma</code> subclasses) or  their <code>single_graph_op</code> and/or <code>multiple_graph_op</code> method (for <code>PsiNonLocal</code> and <code>PsiGlobal</code> subclasses). The model summary in this case uses the (sub)class  name. The classes are not to be instantiated as this point, as the compiler will do it when needed.</p> <pre><code>import tensorflow as tf\nfrom libmg import PsiLocal, Phi, Sigma\n\nclass Successor(PsiLocal):\n    def func(self, x: tf.Tensor) -&gt; tf.Tensor:\n        return x + 1\n\nclass Projection1(Phi):\n    def func(self, src: tf.Tensor, e: tf.Tensor, tgt: tf.Tensor) -&gt; tf.Tensor:\n        return src\n\nclass AggregateSum(Sigma):\n    def func(self, m: tf.Tensor, i: tf.Tensor, n: int, x: tf.Tensor) -&gt; tf.Tensor:\n        return tf.math.segment_sum(m, i)\n</code></pre> <p>We can create trainable functions using <code>tf.keras.layers.Dense</code>:</p> <pre><code>class MyDense(PsiLocal):\n    def __init__(self):\n        self.dense = tf.keras.layers.Dense(5, activation='relu')\n\n    def func(self, x: tf.Tensor) -&gt; tf.Tensor:\n        return self.dense(x)\n</code></pre> <p>Using the subclassing interface, whenever we refer to the <code>MyDense</code> function a new instance of the class is created, therefore a new function with its own  set of weights is being used.</p>"},{"location":"how-to_guides/functions/#make-interface","title":"Make interface","text":"<p>The make interface consists in calling the static factory method <code>make</code> on the class of the function we want to create. As for the constructor interface, we  will typically pass a lambda function and a name to be used in summaries. The <code>make</code> function does not return an instance of the class, but rather a  function that returns an instance of the class. The compiler will call this function as needed to get the instance.</p> <pre><code>import tensorflow as tf\nfrom libmg import PsiLocal, Phi, Sigma\n\nsuccessor = PsiLocal.make('successor', lambda x: x + 1)\nprojection_1 = Phi.make('projection_1', lambda i, e, j: i)\naggregate_sum = Sigma.make('aggregate_sum', lambda m, i, n, x: tf.math.segment_sum(m, i))\n</code></pre> <p>We can create trainable functions using <code>tf.keras.layers.Dense</code>: <pre><code>dense = PsiLocal.make('dense', tf.keras.layers.Dense(5, activation='relu'))\n</code></pre></p> <p>This time, using the <code>make</code> interface, each instance of <code>dense</code> will have its own set of weights, as in the subclassing interface. This is because <code>make</code>  regenerates any <code>tf.keras.layer.Layer</code> that it receives in input. </p>"},{"location":"how-to_guides/functions/#constants-and-projections","title":"Constants and Projections","text":"<p>The subclasses of <code>PsiLocal</code> can be used to define constant and projection functions. We can define a constant function by instantiating <code>Constant</code> with the rank-1 <code>Tensor</code> we want the function to map to.</p> <pre><code>import tensorflow as tf\nfrom libmg import Constant\nzero = Constant(tf.constant([0]))\nthree_ones = Constant(tf.constant([1, 1, 1]))\n</code></pre> <p>We can define a projection function by instantiating <code>Pi</code> with the initial index and the final index of the projection:</p> <pre><code>from libmg import Pi\n# maps every node label to its first element\nfirst = Pi(0, 1)\n# maps every node label to the sub-tensor consisting of its third and fourth element\ntwo_to_four = Pi(2, 4)\n</code></pre>"},{"location":"how-to_guides/functions/#dictionaries-of-functions","title":"Dictionaries of functions","text":"<p>We create dictionaries of functions using the standard Python <code>dict</code>. The keys in this dictionary will be the terms with which we can refer to our function  in a \\(\\mu\\mathcal{G}\\) expression. The values are the functions we have defined with either interface: class instances if we used the constructor interface,  (sub)classes if we used the subclassing interface, or functions returning class instances if we used the <code>make</code> interface.</p> <pre><code>psi_functions = {'succ': successor, ...}\nphi_functions = {'p1': projection_1, ...}\nsigma_functions = {'+': aggregate_sum, ...}\n</code></pre> <p>Note</p> <p>The dictionary key is the string that should be used in a \\(\\mu\\mathcal{G}\\) expression to refer to the corresponding function. The <code>name</code> argument used  in the constructor interface or the make interface is just part of the layer name that is shown using <code>model.summary()</code>.</p>"},{"location":"how-to_guides/functions/#parametrized-functions","title":"Parametrized functions","text":"<p>Sometimes we might want to create many functions which all share the same basic structure except for some value that changes. For example, we might want to  have not only a successor function, but also an \"add two\" function, and an \"add three\" function, and so on. For this purpose, \\(\\mu\\mathcal{G}\\) has a  special syntax that allows to send a parameter from the \\(\\mu\\mathcal{G}\\) expression to the function.</p> <p>Let \\(a\\) be a function name (as specified in the dictionary). When we write the \\(\\mu\\mathcal{G}\\) expression \\(a[1]\\) the compiler will send the string <code>\"1\"</code> as  input to what is saved in the dictionary. </p> <p>You can create a parametrized function using any of the constructor, subclassing or make interfaces:</p> <ul> <li>Using the constructor interface, simply wrap your instance in a one-argument lambda: <code>lambda y: PsiLocal(lambda x: x + int(y))</code></li> <li>Using the subclassing interface, add one argument to the <code>__init__</code> of the subclass    <pre><code>class Add(PsiLocal):\n  def __init__(self, y):\n      self.y = int(y)\n  def func(self, x: tf.Tensor) -&gt; tf.Tensor:\n      return x + self.y\n</code></pre></li> <li>Using the <code>make</code> interface, call instead the <code>make_parametrized</code> method by passing in either a two-argument lambda or a curried version of it: <code>PsiLocal.make_parametrized('add', lambda y, x: x + int(y))</code> or <code>PsiLocal.make_parametrized('add', lambda y: lambda x: x + int(y))</code></li> </ul> <p>For example, if we have bound any of these functions to the word <code>add</code> in the compiler, and we write the expression \\(add[2]\\) the compiler will generate and  use a \\(\\psi\\) function that adds 2 to the input node labels.</p>"},{"location":"how-to_guides/functions/#operators","title":"Operators","text":"<p>In \\(\\mu\\mathcal{G}\\) functions are typically written in reverse Polish notation using sequential and parallel composition. For example, to compute the sum  \\(+\\) of the outputs of two \\(\\psi\\) functions \\(a\\) and \\(b\\) one typically has to write </p> \\[ (a || b) ; + \\] <p>It is possible to instead write the same expression in Polish notation without all the boilerplate code, that is</p> \\[ +(a, b) \\] <p>For this purpose it is necessary to define functions with yet another interface, as operators. Three types of operators can be defined:</p> <ul> <li> <p>Unary operators can be defined using <code>make_uoperator</code>:   <pre><code>from libmg import make_uoperator\nnot = make_uoperator(tf.math.logical_not)\n</code></pre>   Usage in \\(\\mu\\mathcal{G}\\), assuming the compiler was passed the \\(\\psi\\) function dictionary <code>{'~': not, 'a': ...}</code>:</p> \\[ \\sim(a) \\] </li> <li> <p>Binary operators can be defined using <code>make_boperator</code>(<code>tf.math.add</code> is a function in exactly 2 arguments):     <pre><code>import tensorflow as tf\nfrom libmg import make_boperator\nadd = make_boperator(tf.math.add)\n</code></pre>   Usage in \\(\\mu\\mathcal{G}\\), assuming the compiler was passed the \\(\\psi\\) function dictionary <code>{'+': add, 'a': ..., 'b': ...}</code>:</p> \\[ +(a, b) \\] </li> <li> <p>K-ary operators can be defined using <code>make_koperator</code>(<code>tf.math.add_n</code> is a function in \\(n\\) arguments): <pre><code>import tensorflow as tf\nfrom libmg import make_koperator\naddk = make_koperator(tf.math.add_n)\n</code></pre>   Usage in \\(\\mu\\mathcal{G}\\), assuming the compiler was passed the \\(\\psi\\) function dictionary <code>{'+': addk, 'a': ..., 'b': ..., 'c': ...}</code>:</p> \\[ +(a, b, c) \\] </li> </ul> <p>The operators attempt to automatically find their arguments given the node label, which is always a rank-1 tensor. Unary operators are applied directly  to the node labels in the same way as other \\(\\psi\\) functions. Binary operators split in half the node labels tensor, and consider the first partition as  the left operand and the second partition as the right operand. K-ary operators split the node labels evenly in \\(k\\) slices and treat each slice as an  operand.</p>"},{"location":"how-to_guides/installation/","title":"Installation","text":"<p>This guide shows you how to install libmg.</p>"},{"location":"how-to_guides/installation/#pre-requisites","title":"Pre-requisites","text":"<ul> <li>A Linux operating system (preferably Ubuntu 16.04 or later as per the TensorFlow recommendation).</li> <li>Python 3.11 environment.</li> </ul> <p>The library can run both on the CPU or the GPU. To enable the GPU, the specific dependencies needed are those of TensorFlow 2.12, that is:</p> <ul> <li>GCC 9.3.1</li> <li>Bazel 5.3.0</li> <li>NVIDIA GPU drivers version 450.80.02 or higher</li> <li>CUDA 11.8</li> <li>cuDNN 8.6</li> <li>(Optional) TensorRT 7</li> </ul>"},{"location":"how-to_guides/installation/#installation_1","title":"Installation","text":"<p>libmg can be installed via pip or from source.</p>"},{"location":"how-to_guides/installation/#pip-installation","title":"Pip installation","text":"<p>libmg can be installed from the Python Package Index PyPI, by simply running the following command in your  shell or virtual environment:</p> <pre><code>$ pip install libmg\n</code></pre>"},{"location":"how-to_guides/installation/#source-installation","title":"Source installation","text":"<p>You can install libmg from source using git. You can start by downloading the repo archive or by cloning the repo:</p> <pre><code>git clone https://github.com/quasylab/mG.git\n</code></pre> <p>Then proceed by opening a shell into the <code>mG</code> directory you have just downloaded. To build the library you will need to use Poetry. Run the following command:</p> <p><pre><code>poetry install\n</code></pre> and Poetry will install libmg in your Python environment. To install the development dependencies as well, install with:</p> <pre><code>poetry install --with tests --with docs\n</code></pre> <p>This will add the testing dependencies (pytest, mypy, and flake8) as well as the documentation dependencies (mkdocs and plugins).</p>"},{"location":"how-to_guides/installation/#verify-the-installation","title":"Verify the installation","text":"<p>To verify your installation, you can print the library's version number:</p> <pre><code>&gt;&gt;&gt; import libmg\n&gt;&gt;&gt; libmg.__version__\n'1.0.5'\n</code></pre> <p>Additionally, it is possible to run the libmg test suite. For that, it is necessary to have pytest and pytest-cov installed.</p> <p>If you are installing from source, these dependencies can be installed by running <code>poetry install</code> with the <code>--with tests</code> option.</p> <p>The tests can be run using the <code>run_tests</code> function:</p> <pre><code>&gt;&gt;&gt; import libmg\n&gt;&gt;&gt; libmg.run_tests()\n============================= test session starts ==============================\nplatform linux -- Python 3.11.6, pytest-7.4.3, pluggy-1.3.0\nrootdir: /home/matteo/PycharmProjects/mG\nconfigfile: pyproject.toml\ntestpaths: libmg\nplugins: cov-4.1.0\ncollected 169 items\nlibmg/compiler/functions.py ...........                                  [  6%]\nlibmg/language/grammar.py .                                              [  7%]\nlibmg/normalizer/normalizer.py .                                         [  7%]\nlibmg/tests/test_compiler.py ............s...                            [ 17%]\nlibmg/tests/test_cuda.py .s                                              [ 18%]\nlibmg/tests/test_explainer.py .s                                         [ 19%]\nlibmg/tests/test_functions.py ...............................s..s..s..s. [ 44%]\n.s..s.s...s                                                              [ 50%]\nlibmg/tests/test_grammar.py .................................            [ 70%]\nlibmg/tests/test_normalizer.py ......................................... [ 94%]\n......                                                                   [ 98%]\nlibmg/tests/test_visualizer.py ..s                                       [100%]\n---------- coverage: platform linux, python 3.11.6-final-0 -----------\nName                             Stmts   Miss  Cover\n----------------------------------------------------\nlibmg/__init__.py                   10     10     0%\nlibmg/compiler/__init__.py           3      3     0%\nlibmg/compiler/compiler.py         680    157    77%\nlibmg/compiler/functions.py        171     69    60%\nlibmg/compiler/layers.py           172     50    71%\nlibmg/data/__init__.py               4      4     0%\nlibmg/data/dataset.py               10      7    30%\nlibmg/data/loaders.py               51     23    55%\nlibmg/explainer/__init__.py          2      2     0%\nlibmg/explainer/explainer.py       109     34    69%\nlibmg/language/__init__.py           2      2     0%\nlibmg/language/grammar.py           23     12    48%\nlibmg/normalizer/__init__.py         2      2     0%\nlibmg/normalizer/normalizer.py     105     36    66%\nlibmg/tests/__init__.py              2      2     0%\nlibmg/tests/run_tests.py             3      3     0%\nlibmg/tests/test_compiler.py       174      0   100%\nlibmg/tests/test_cuda.py             7      0   100%\nlibmg/tests/test_explainer.py       38      0   100%\nlibmg/tests/test_functions.py      213      3    99%\nlibmg/tests/test_grammar.py        147      0   100%\nlibmg/tests/test_normalizer.py      21      0   100%\nlibmg/tests/test_visualizer.py      46      0   100%\nlibmg/visualizer/__init__.py         2      2     0%\nlibmg/visualizer/visualizer.py      94     24    74%\n----------------------------------------------------\nTOTAL                             2091    445    79%\n================== 157 passed, 12 skipped in 92.74s (0:01:32) ==================\n</code></pre>"},{"location":"how-to_guides/models/","title":"How to create, run and optimize models","text":"<p>This guide shows you how to create models using the \\(\\mu\\mathcal{G}\\) compiler, how to run them on your datasets, and how to let the compiler trace them before you run them.</p>"},{"location":"how-to_guides/models/#creating-a-model","title":"Creating a model","text":"<p>To create a model, simply pass its \\(\\mu\\mathcal{G}\\) expression to the <code>compile</code> method on the <code>MGCompiler</code> instance. </p> <pre><code>compiler = MGCompiler(...)\nmodel = compiler.compile('(a || b);c')\n</code></pre> <p>It is possible to automatically print a summary of the compiled model by setting the <code>verbose</code> argument to <code>True</code>:</p> <pre><code>model = compiler.compile('(a || b);c', verbose=True)\n</code></pre> <p>Finally, to enable automatic memoization, set the <code>memoize</code> argument to <code>True</code>:</p> <pre><code># 'a' is computed only once inside the model\nmodel = compiler.compile('((a || b);c) || a', memoize=True)  \n</code></pre>"},{"location":"how-to_guides/models/#running-a-model","title":"Running a model","text":"<p>The model can be run using any of the TensorFlow APIs that exist for this purpose: directly calling the model, using <code>call</code>, using <code>predict</code>, or using  <code>predict_on_batch</code>. In each of these cases, it is necessary to use the adequate loader for the dataset we will be feeding into the model. For datasets  consisting of a single graph we will be using the <code>SingleGraphLoader</code>, while for datasets consisting of multiple graphs, we will use the  <code>MultipleGraphLoader</code>. Both of these loaders are implemented in Spektral. The loaders convert a graph, or a batch of  graphs, into a list of TensorFlow <code>Tensor</code> objects, one each corresponding to the <code>x</code>, <code>a</code>, <code>e</code> and <code>y</code> arguments used to define the graph. The output of a  <code>SingleGraphLoader</code> will be a two-element tuple <code>((x, a, [e]), y)</code> if <code>y</code> was specified in the graph, otherwise it will be a one-element tuple <code>((x, a, [e]),)</code>. The <code>MultipleGraphLoader</code> shares the same output structure as the <code>SingleGraphLoader</code>, additionally adding a <code>i</code> tensor to the tail of the three-element  tuple on the left (e.g. <code>((x, a, [e], i), y)</code>). The meaning of this tensor will be explained below.</p> <p>Note</p> <p>The loader chosen here must correspond the <code>CompilerConfig</code> object we used to instantiate the compiler.</p> <p>To load a dataset with the <code>SingleGraphLoader</code> pass the dataset to it:</p> <pre><code>from libmg import SingleGraphLoader\n\ndataset = MyDataset(...)\nloader = SingleGraphLoader(dataset)\n</code></pre> <p>It is possible to specify the number of <code>epochs</code> that the loader will generate. Set epochs to 1 if you want that the loader simply returns the single graph in  the dataset once. If epochs is left as <code>None</code>, the loader will keep returning the graph every time it is called.</p> <pre><code>loader = SingleGraphLoader(dataset, epochs=1)\n</code></pre> <p>Similarly, to load a dataset with the <code>MultipleGraphLoader</code>, just pass the dataset to it.</p> <pre><code>from libmg import MultipleGraphLoader\n\ndataset = MyDataset(...)\nloader = MultipleGraphLoader(dataset)\n</code></pre> <p>It is again possible to specify the number of <code>epochs</code>. This time, since the dataset used with this loader has more than one graph, the epochs number specify how many times to cycle through all the graphs. Set it to 1 to just show each graph once, <code>None</code> to keep cycling, or any other integer to cycle that amount of  times. The number of graphs to batch together is specified with the <code>batch_size</code> argument. For example, if <code>batch_size=2</code> and the dataset has 10 graphs, the  loader will return 5 batches of 2 graphs. Each batch is structured according to Spektral's disjoint mode. The loader therefore adds an <code>i</code> Tensor to its outputs. This tensor is used to specify to which graph belongs each row of the node features matrix <code>x</code>. For  example, if we batched two graphs and the <code>x</code> features matrix is <code>[[1], [2], [3], [4]]</code> (each node has one integer feature) and <code>i</code> is <code>[0, 0, 1, 1]</code> it  means that the first graph in the batch has two nodes with features <code>[1]</code> and <code>[2]</code> and the second graph has two nodes with features <code>[3]</code> and <code>[4]</code>.</p> <pre><code>loader = MultipleGraphLoader(dataset, batch_size=2, epochs=1)\n</code></pre>"},{"location":"how-to_guides/models/#using-call-or-by-directly-calling-the-model","title":"Using <code>call</code> or by directly calling the model","text":"<p>When using <code>call</code> on the model, we should obtain the graphs from the loader by iterating on the <code>load</code> method:</p> <pre><code>for inputs in loader.load():\n    model.call(inputs)\n</code></pre> <p>equivalently, we can also directly call the model:</p> <pre><code>for inputs in loader.load():\n    model(inputs)\n</code></pre> <p>If there is only one graph, it can also be obtained directly with:</p> <pre><code>inputs = next(iter(loader.load()))\n</code></pre> <p>If the true labels are present, they are not to be passed as inputs to the model:</p> <p><pre><code>for inputs, y in loader.load():\n    model.call(inputs)\n</code></pre> <pre><code>inputs, y = next(iter(loader.load()))\n</code></pre></p>"},{"location":"how-to_guides/models/#using-predict","title":"Using <code>predict</code>","text":"<p>Using <code>predict</code>, you should fill in the <code>steps</code> arguments with the <code>steps_per_epoch</code> attribute of the loader:</p> <pre><code>outputs = model.predict(loader.load(), steps=loader.steps_per_epoch)\n</code></pre>"},{"location":"how-to_guides/models/#using-predict_on_batch","title":"Using <code>predict_on_batch</code>","text":"<p>Using <code>predict_on_batch</code>, you should just pass the batch of graphs:</p> <pre><code>outputs = model.predict_on_batch(next(iter(loader.load())))\n</code></pre> <p>Warning</p> <p>For the time being, due to TensorFlow assertion checks, it is not possible to use the <code>predict_on_batch</code> API on datasets whose graphs have edge labels.</p>"},{"location":"how-to_guides/models/#training-a-model","title":"Training a model","text":"<p>Models can be trained in the usual manner as in TensorFlow. The model must first be compiled by TensorFlow (a distinct operation from compiling in  \\(\\mu\\mathcal{G}\\)), specifying the optimizer and the loss function:</p> <pre><code># Using stochastic gradient descent and mean squared error\nmodel.compile(optimizer='sgd', loss='mse')\n</code></pre> <p>Then, we can train the model using <code>fit</code>, by specifying the <code>steps_per_epoch</code> using the loader and the number of epochs: <pre><code>model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=100)\n</code></pre></p> <p>Note</p> <p>The loader in this case should have been instantiated with <code>epochs=None</code>, since the <code>fit</code> method will specify the epochs.</p>"},{"location":"how-to_guides/models/#tracing-a-model","title":"Tracing a model","text":"<p>Tracing is performed automatically by running the model on some data. At any rate, it is possible to create dummy data which the compiler will use to trace  the model for you. In order to do that, we simply call the <code>trace</code> method on the compiler by passing in the model and the API we will be using to run the  model (either <code>call</code>, <code>predict</code>, or <code>predict_on_batch</code>):</p> <pre><code># Tracing the model for the 'predict' API\ntraced_model = compiler.trace(model, 'predict')\n</code></pre> <p>Warning</p> <p>The <code>trace</code> method with the second argument set to <code>call</code> returns a <code>@tf.function</code> that runs <code>model.call</code> on its inputs. Therefore the return value is  no longer a TensorFlow model, but a Python <code>Callable</code> and the typical TensorFlow methods (<code>predict</code>, <code>fit</code>, etc.) and the libmg model attributes will be no longer available.</p>"},{"location":"how-to_guides/visualization/","title":"Visualization procedures","text":"<p>libmg offers visualization procedure both for Spektral <code>Graph</code> objects and \\(\\mu\\mathcal{G}\\) model outputs.  Currently two visualization engines are supported: PyVis and Cosmograph. PyVis is a  Python binding for vis.js, a Javascript library for dynamic visualizations in the browser. Cosmograph is a React library built on  cosmos, a GPU-accelerated Force Graph layout algorithm and rendering engine.</p>"},{"location":"how-to_guides/visualization/#engine-features-and-recommendations","title":"Engine features and recommendations","text":""},{"location":"how-to_guides/visualization/#pyvis","title":"Pyvis","text":"<p> On the web page generated using PyVis it is possible to move around nodes by dragging them with the mouse pointer, to make the graph clearer to see. The  entire graph can be moved by dragging on an empty spot. Hovering a node with the pointer will show its ID. On the search bar at the top is possible to  select a node by ID. Below the search bar there is a filter bar that allows to filter nodes and edges according to their ID or label.</p> <p>Note</p> <p>We recommend the usage of PyVis only for small graph instances (nodes in the order of the hundreds). </p> <p>Note</p> <p>The pyvis engine generates a single index.html file</p>"},{"location":"how-to_guides/visualization/#cosmograph","title":"Cosmograph","text":"<p> On the web page generated using Cosmograph it is not possible to move individual nodes, but right-clicking on an empty spot will \"shake\" the simulation a  little. The entire graph can be moved by dragging on an empty spot. Hovering a node will temporarily replace its label with its ID. On the search bar at the  top is possible to select a node by ID or by label. Finally, a button below the search bar allows to toggle the shape of the edges, which can be either  straight (default) or curved. Curved edges may slow the simulation but it may make it easier to see the links between the nodes.</p> <p>Note</p> <p>For the time being, the Cosmograph engine doesn't show edge labels.</p> <p>Note</p> <p>The pyvis engine generates a directory, inside which is situated the index.html that should be opened in the browser.</p>"},{"location":"how-to_guides/visualization/#visualizing-a-graph","title":"Visualizing a <code>Graph</code>","text":"<p>To visualize a graph, simply call the <code>print_graph</code> function on a <code>Graph</code> instance:</p> <pre><code>from libmg import Graph, print_graph\n\ngraph = Graph(...)\nprint_graph(graph, show_labels=False, open_browser=True, filename='mygraph', engine='pyvis')\n</code></pre> <p>The other arguments of this function can be used to specify whether tho show the true labels (if present in the <code>Graph</code> instance), whether to automatically  open the default web browser, a name for the generated file or directory, and the engine to be used.</p>"},{"location":"how-to_guides/visualization/#visualizing-a-model-output","title":"Visualizing a model output","text":"<p>To visualize the output of a model, call the <code>print_layer</code> function by passing in the model, its inputs and the layer to print. To print the output of a  model it means to print the output of its last layer (with index -1):</p> <pre><code>model = compiler.compile(...)\ninputs, = next(iter(loader.load()))\nprint_layer(model, inputs, layer_idx=-1, filename='myoutputs', open_browser=True, engine='cosmo')\n</code></pre> <p>Note</p> <p>Since a loader always returns a tuple (containing either only the actual inputs, or the inputs and the true labels), remember to unpack the tuple with <code>inputs, = next(iter(loader.load()))</code> or <code>inputs, y = next(iter(loader.load()))</code>.</p> <p>The layer to print does not have to be the last, but can be any layer of the model. To specify the layer, we can provide either the integer index or its name. The integer index of a layer can be found calling <code>model.summary()</code> and it can be positive or negative, with negative numbers have the usual meaning in  Python. The name of a layer is the \\(\\mu\\mathcal{G}\\) expression corresponding to it, expressed either as string or as parse tree (which is obtained by  calling <code>mg_parser.parse</code> on the expression string). The name of the layer will be parsed by <code>mg_parser</code> if passed in as string, therefore parentheses can  be used as needed.</p> <pre><code>print_layer(model, inputs, layer_name='(a || b)', filename='myoutputs', open_browser=True, engine='cosmo')\n# equivalent to\nprint_layer(model, inputs, layer_name='a || b', filename='myoutputs', open_browser=True, engine='cosmo')\n</code></pre> <p>Warning</p> <p>Do not print any of the layers corresponding the edge labels, the adjacency matrix, or the indices produced by the <code>MultipleGraphLoader</code>.</p>"},{"location":"reference/","title":"Reference","text":"<p>Here are the links to the \\(\\mu\\mathcal{G}\\) grammar and the code reference for the public APIs of libmg</p>"},{"location":"reference/#grammar-reference","title":"Grammar reference","text":"<p>\\(\\mu\\mathcal{G}\\) expressions are composed according to a context-free grammar expressed in Backus-Naur form.</p> <ul> <li>Grammar reference</li> </ul>"},{"location":"reference/#api-reference","title":"API Reference","text":"<p>On the following links, the public APIs documentation can be accessed, grouped by package.</p> <ul> <li>compiler package</li> <li>data package</li> <li>explainer package</li> <li>language package</li> <li>normalizer package</li> <li>visualizer package</li> </ul>"},{"location":"reference/grammar/","title":"Grammar Reference","text":"<p>The grammar that generates \\(\\mu\\mathcal{G}\\) expressions is the following:</p> <pre><code>start ::= c_formula\n\nc_formula ::=  gnn_formula\n                | gnn_formula \";\" c_formula \n                | gnn_formula ( \"||\" gnn_formula )  \n\ngnn_formula ::= label                                                                              \n                 | \"&lt;\" label? \"|\" label                                                                   \n                 | \"|\" label? \"&gt;\" label                                                                   \n                 | label \"(\" (c_formula \",\")* c_formula \")\"                                               \n                 | \"let\" (label_decl \"=\" c_formula \",\")* label_decl \"=\" c_formula \"in\" c_formula          \n                 | \"def\" label_decl \"(\" (label_decl \",\")* label_decl \")\" \"{\" c_formula \"}\" \"in\" c_formula\n                 | \"if\" c_formula \"then\" c_formula \"else\" c_formula                                      \n                 | \"fix\" label_decl \"=\" c_formula \"in\" c_formula                                          \n                 | \"repeat\" label_decl \"=\" c_formula \"in\" c_formula \"for\" NUMBER                   \n                 | \"(\" c_formula \")\"\n\nlabel ::= /[a-zA-Z_0-9\\+\\*\\^\\-\\!\\%\\&amp;\\~\\/\\@]+/ |  FUNC_GEN\n\nFUNC_GEN ::= /[a-zA-Z_0-9\\+\\*\\^\\-\\!\\%\\&amp;\\~\\/\\@]+/ \"[\" /[^\\]\\[]+/ \"]\"\n\nlabel_decl ::= /[a-zA-Z_0-9\\+\\*\\^\\-\\!\\%\\&amp;\\~\\/\\@]+/\n\nCOMMENT ::= \"#\" /[^\\n]/*\n\n%ignore COMMENT\n%import common.WS\n%import common.NUMBER\n%import common.UCASE_LETTER\n%ignore WS\n</code></pre> <p>The reserved symbols and words of \\(\\mu\\mathcal{G}\\), which therefore cannot be used as function labels or variable names, are:</p> <pre><code>|| fix let if then else def in repeat for | &lt; &gt; , ( ) ; [ ] # { }\n</code></pre> <p>Warning</p> <p>Variables should not be named with two initial underscores __, e.g. don't name variables such as __X. Such variables names are used internally for operators.</p>"},{"location":"reference/API_reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>compiler</li> <li>data</li> <li>explainer</li> <li>language</li> <li>normalizer</li> <li>verifier</li> <li>visualizer</li> </ul>"},{"location":"reference/API_reference/compiler/","title":"compiler","text":"<p>Defines the mG compiler and the basic mG functions.</p> <p>This package defines a compiler for mG programs and the data structures to instantiate it. It also provides the various classes that allow for the definition of mG functions.</p> <p>The package contains the following classes:</p> <ul> <li><code>NodeConfig</code></li> <li><code>EdgeConfig</code></li> <li><code>CompilerConfig</code></li> <li><code>MGCompiler</code></li> <li><code>PsiNonLocal</code></li> <li><code>PsiLocal</code></li> <li><code>PsiGlobal</code></li> <li><code>Phi</code></li> <li><code>Sigma</code></li> <li><code>Constant</code></li> <li><code>Pi</code></li> </ul> <p>The module contains the following functions:</p> <ul> <li><code>make_uoperator(op, name)</code></li> <li><code>make_boperator(op, name)</code></li> <li><code>make_koperator(op, name)</code></li> </ul>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiLocal","title":"PsiLocal","text":"<pre><code>PsiLocal(f: Callable | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>Psi</code></p> <p>A psi function of the mG language that only applies a local transformation of node labels.</p> <p>A local transformation of node labels \\(\\psi: T \\rightarrow U\\).</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiLocal(lambda x: x + 1, name='Add1')\n&lt;PsiLocal ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>f</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor X with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and return a tensor of shape <code>(n_nodes, n_new_node_features)</code>, containing the transformed labels. The function is not supposed to use global information, but this is not enforced.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, f: Callable | None = None, name: str | None = None):\n    \"\"\"Initializes the instance with a function and a name.\n\n    Args:\n        f: The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor X with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and return a tensor of shape ``(n_nodes, n_new_node_features)``, containing the transformed labels.\n            The function is not supposed to use global information, but this is not enforced.\n        name: The name of the function.\n    \"\"\"\n    if f is None:\n        f = self.func\n    super().__init__(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiLocal.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiLocal.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, f: Callable) -&gt; Callable[[], T_A]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n&lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function.</p> </li> <li> <code>f</code>             (<code>Callable</code>)         \u2013          <p>The function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls: Type[T_A], name: str | None, f: Callable) -&gt; Callable[[], T_A]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n        &lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function.\n        f: The function that will be used to instantiate ``cls``.\n    \"\"\"\n    if isinstance(f, tf.keras.layers.Layer):  # if f is a layer, regenerate from config to get new weights\n        return lambda: cls(type(f).from_config(f.get_config()), name)  # type: ignore\n    else:\n        return lambda: cls(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiLocal.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code>  returns an instance of <code>cls</code> initialized with the result of the application of <code>a</code> to the function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The function <code>f</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>f</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>f</code>.</p> </li> <li> <code>f</code>             (<code>Callable[[str], Callable] | Callable[..., Any]</code>)         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls: Type[T_A], name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a``\n     returns an instance of ``cls`` initialized with the result of the application of ``a`` to the function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function. The function ``f`` may have the form ``lambda x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned\n    by this function corresponds to the ``x`` argument of ``f``.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``f``.\n        f: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``.\n    \"\"\"\n    # check if f has only a single argument e.g. lambda x: lambda y: foo(x)(y)\n    if f.__code__.co_argcount == 1:\n        return lambda a: cls(f(a), name + '_' + a if name is not None else None)\n    else:  # e.g. lambda x, y: foo(x)(y)\n        return lambda a: cls(partial(f, a), name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiGlobal","title":"PsiGlobal","text":"<pre><code>PsiGlobal(single_op: Callable | None = None, multiple_op: Callable | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>PsiNonLocal</code></p> <p>A psi function of the mG language that only applies a global transformation of node labels.</p> <p>A global transformation (i.e. pooling) of node labels \\(\\psi: T^* \\rightarrow U\\). For single graph datasets, which use the <code>SingleGraphLoader</code>, only the <code>single_op</code> parameter is necessary. For multiple graph datasets, using the <code>MultipleGraphLoader</code>, only the <code>multiple_op</code> parameter is necessary. The <code>multiple_op</code> argument is a function which takes an additional parameter to distinguish which values in the first argument refer to which graph. For more information, refer to the disjoint data mode in the Spektral library <code>documentation &lt;https://graphneural.network/data-modes/#disjoint-mode/&gt;</code>_.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiGlobal(single_op=lambda x: tf.reduce_sum(x, axis=0, keepdims=True), multiple_op=lambda x, i: tf.math.segment_sum(x, i), name='SumPooling')\n&lt;PsiGlobal ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>single_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function to be used in conjunction with a <code>SingleGraphLoader</code>. The function is expected to take in input a tensor X with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and return a tensor of shape <code>(n_pooled_features, )</code>, containing the pooled output. This output is then broadcast to label every node.</p> </li> <li> <code>multiple_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function to be used in conjunction with a <code>MultipleGraphLoader</code>. The function is expected to take in input a tensor with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and a tensor of graph indices of shape <code>(n_nodes, 1)</code>, that mark to which graph every node belongs. The function is expected to return a tensor of shape <code>(n_graphs, n_pooled_features)</code> containing the pooled outputs for each distinct graph index. This output is then broadcast to label every node of each graph with the pooled features for that graph.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, single_op: Callable | None = None,\n             multiple_op: Callable | None = None, name: str | None = None):\n    \"\"\"Initializes the instance with a function for a single graph and/or a function a multiple graph operation, and a name.\n\n    Args:\n        single_op: The function to be used in conjunction with a ``SingleGraphLoader``.\n            The function is expected to take in input a tensor X with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and return a tensor of shape ``(n_pooled_features, )``, containing the pooled output. This output is then broadcast to\n            label every node.\n        multiple_op: The function to be used in conjunction with a ``MultipleGraphLoader``.\n            The function is expected to take in input a tensor with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and a tensor of graph indices of shape ``(n_nodes, 1)``, that mark to which graph every node belongs. The function is\n            expected to return a tensor of shape ``(n_graphs, n_pooled_features)`` containing the pooled outputs for each distinct graph index. This output\n            is then broadcast to label every node of each graph with the pooled features for that graph.\n        name: The name of the function.\n    \"\"\"\n    super().__init__(single_op=single_op, multiple_op=multiple_op, name=name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiGlobal.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiGlobal.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, single_op: Callable | None = None, multiple_op: Callable | None = None) -&gt; Callable[[], PsiNonLocal]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided functions <code>single_op</code> and/or <code>multiple_op</code> and <code>name</code>.</p> <p>Calling this method on a <code>PsiNonLocal</code> class creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiNonLocal.make('Successor', lambda x: x + 1)\n&lt;function PsiNonLocal.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>single_op</code> and <code>multiple_op</code>.</p> </li> <li> <code>single_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that will be used to instantiate <code>cls</code> as the <code>single_op</code> argument.</p> </li> <li> <code>multiple_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that will be used to instantiate <code>cls</code> as the <code>multiple_op</code> argument.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>Neither <code>single_op</code> nor <code>multiple_op</code> have been provided.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls, name: str | None, single_op: Callable | None = None, multiple_op: Callable | None = None) -&gt; Callable[[], PsiNonLocal]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided functions ``single_op`` and/or\n    ``multiple_op`` and ``name``.\n\n    Calling this method on a ``PsiNonLocal`` class creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that\n    function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights,\n    which are not supposed to be shared with other instances of the function.\n\n    Examples:\n        &gt;&gt;&gt; PsiNonLocal.make('Successor', lambda x: x + 1)\n        &lt;function PsiNonLocal.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``single_op`` and ``multiple_op``.\n        single_op: The function that will be used to instantiate ``cls`` as the ``single_op`` argument.\n        multiple_op: The function that will be used to instantiate ``cls`` as the ``multiple_op`` argument.\n\n    Raises:\n        ValueError: Neither ``single_op`` nor ``multiple_op`` have been provided.\n    \"\"\"\n    if single_op is None and multiple_op is None:\n        raise ValueError(\"At least one function must be provided.\")\n    args = {}\n    if single_op is not None:\n        args['single_op' if cls is not PsiLocal else 'f'] = single_op\n    if multiple_op is not None:\n        args['multiple_op'] = multiple_op\n    return lambda: cls(**{k: type(v).from_config(v.get_config()) if isinstance(v, tf.keras.layers.Layer) else v for k, v in args.items()}, name=name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiGlobal.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(\n    name: str | None,\n    single_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n    multiple_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n) -&gt; Callable[[str], PsiNonLocal]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code> returns an instance of <code>cls</code> initialized with the result of the  application of <code>a</code> to the function <code>single_op</code> and/or <code>multiple_op</code>, and <code>name</code>.</p> <p>Calling this method on a <code>PsiNonLocal</code> class creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The functions <code>single_op</code> and <code>multiple_op</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>single_op</code> and <code>multiple_op</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiNonLocal.make_parametrized(name='Add', single_op=lambda y: lambda x: x + y, multiple_op=lambda y: lambda x, i: x + y)\n&lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; PsiNonLocal.make_parametrized( name='Add', single_op=lambda y, x: x + y, multiple_op=lambda y, x, i: x + y)\n&lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>single_op</code> and <code>multiple_op</code>.</p> </li> <li> <code>single_op</code>             (<code>Callable[[str], Callable] | Callable[..., Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>as the <code>single_op</code> argument.</p> </li> <li> <code>multiple_op</code>             (<code>Callable[[str], Callable] | Callable[..., Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code> as the <code>multiple_op</code> argument.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>Neither <code>single_op</code> nor <code>multiple_op</code> have been provided.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls, name: str | None, single_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n                      multiple_op: Callable[[str], Callable] | Callable[..., Any] | None = None) -&gt; Callable[[str], PsiNonLocal]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a`` returns an instance of ``cls`` initialized with the result of the\n     application of ``a`` to the function ``single_op`` and/or ``multiple_op``, and ``name``.\n\n    Calling this method on a ``PsiNonLocal`` class creates a one-argument lambda that returns an instance of such subclass. This way, whenever that\n    function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights,\n    which are not supposed to be shared with other instances of the function. The functions ``single_op`` and ``multiple_op`` may have the form ``lambda\n    x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned by this function corresponds to the ``x`` argument of\n    ``single_op`` and ``multiple_op``.\n\n    Examples:\n        &gt;&gt;&gt; PsiNonLocal.make_parametrized(name='Add', single_op=lambda y: lambda x: x + y, multiple_op=lambda y: lambda x, i: x + y)\n        &lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; PsiNonLocal.make_parametrized( name='Add', single_op=lambda y, x: x + y, multiple_op=lambda y, x, i: x + y)\n        &lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``single_op`` and ``multiple_op``.\n        single_op: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``as the\n            ``single_op`` argument.\n        multiple_op: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls`` as the\n            ``multiple_op`` argument.\n\n    Raises:\n        ValueError: Neither ``single_op`` nor ``multiple_op`` have been provided.\n    \"\"\"\n    if single_op is None and multiple_op is None:\n        raise ValueError(\"At least one function must be provided.\")\n    args = {}\n    if single_op is not None:\n        args['single_op' if cls is not PsiLocal else 'f'] = single_op\n    if multiple_op is not None:\n        args['multiple_op'] = multiple_op\n    return lambda a: cls(**{k: v(a) if v.__code__.co_argcount == 1 else partial(v, a) for k, v in args.items()},\n                         name=name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiNonLocal","title":"PsiNonLocal","text":"<pre><code>PsiNonLocal(single_op: Callable | None = None, multiple_op: Callable | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>Psi</code></p> <p>A psi function of the mG language.</p> <p>A non-local function applied on node labels \\(\\psi: T^* \\times T \\rightarrow U\\). For single graph datasets, which use the <code>SingleGraphLoader</code>, only the <code>single_op</code> parameter is necessary. For multiple graph datasets, using the <code>MultipleGraphLoader</code>, only the <code>multiple_op</code> parameter is necessary. The <code>multiple_op</code> argument is a function which takes an additional parameter to distinguish which values in the first argument refer to which graph. For more information, refer to the disjoint data mode in the Spektral library <code>documentation &lt;https://graphneural.network/data-modes/#disjoint-mode/&gt;</code>_.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiNonLocal(single_op=lambda x: x + 1, multiple_op=lambda x, i: x + 1, name='Add1')\n&lt;PsiNonLocal ...&gt;\n</code></pre> <p>Attributes:</p> <ul> <li> <code>single_op</code>         \u2013          <p>A function to be used in conjunction with a <code>SingleGraphLoader</code></p> </li> <li> <code>multiple_op</code>         \u2013          <p>A function to be used in conjunction with a <code>MultipleGraphLoader</code></p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>single_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function to be used in conjunction with a <code>SingleGraphLoader</code>. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor X with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and return a tensor of shape <code>(n_nodes, n_new_node_features)</code>, containing the transformed labels. The function can use broadcasting to emulate the tuple (T*, T) in the definition of psi.</p> </li> <li> <code>multiple_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function to be used in conjunction with a <code>MultipleGraphLoader</code>. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor X with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and a tensor of graph indices of shape <code>(n_nodes, 1)</code>, that mark to which graph every node belongs. The function is expected to return a tensor of shape <code>(n_nodes, n_new_node_features)</code> containing the transformed labels. The function can use broadcasting to emulate the tuple (T*, T) in the definition of psi.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, single_op: Callable | None = None,\n             multiple_op: Callable | None = None,\n             name: str | None = None):\n    \"\"\"Initializes the instance with a function for a single graph and/or a function a multiple graph operation, and a name.\n\n    Args:\n        single_op: The function to be used in conjunction with a ``SingleGraphLoader``. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor X with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and return a tensor of shape ``(n_nodes, n_new_node_features)``, containing the transformed labels.\n            The function can use broadcasting to emulate the tuple (T*, T) in the definition of psi.\n        multiple_op: The function to be used in conjunction with a ``MultipleGraphLoader``. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor X with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and a tensor of graph indices of shape ``(n_nodes, 1)``, that mark to which graph every node belongs. The function is\n            expected to return a tensor of shape ``(n_nodes, n_new_node_features)`` containing the transformed labels.\n            The function can use broadcasting to emulate the tuple (T*, T) in the definition of psi.\n        name: The name of the function.\n    \"\"\"\n    if single_op is None:\n        self.single_op = self.single_graph_op\n    else:\n        self.single_op = single_op  # type: ignore\n    if multiple_op is None:\n        self.multiple_op = self.multiple_graph_op\n    else:\n        self.multiple_op = multiple_op  # type: ignore\n    super().__init__(self.single_op, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiNonLocal.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiNonLocal.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, single_op: Callable | None = None, multiple_op: Callable | None = None) -&gt; Callable[[], PsiNonLocal]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided functions <code>single_op</code> and/or <code>multiple_op</code> and <code>name</code>.</p> <p>Calling this method on a <code>PsiNonLocal</code> class creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiNonLocal.make('Successor', lambda x: x + 1)\n&lt;function PsiNonLocal.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>single_op</code> and <code>multiple_op</code>.</p> </li> <li> <code>single_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that will be used to instantiate <code>cls</code> as the <code>single_op</code> argument.</p> </li> <li> <code>multiple_op</code>             (<code>Callable | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that will be used to instantiate <code>cls</code> as the <code>multiple_op</code> argument.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>Neither <code>single_op</code> nor <code>multiple_op</code> have been provided.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls, name: str | None, single_op: Callable | None = None, multiple_op: Callable | None = None) -&gt; Callable[[], PsiNonLocal]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided functions ``single_op`` and/or\n    ``multiple_op`` and ``name``.\n\n    Calling this method on a ``PsiNonLocal`` class creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that\n    function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights,\n    which are not supposed to be shared with other instances of the function.\n\n    Examples:\n        &gt;&gt;&gt; PsiNonLocal.make('Successor', lambda x: x + 1)\n        &lt;function PsiNonLocal.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``single_op`` and ``multiple_op``.\n        single_op: The function that will be used to instantiate ``cls`` as the ``single_op`` argument.\n        multiple_op: The function that will be used to instantiate ``cls`` as the ``multiple_op`` argument.\n\n    Raises:\n        ValueError: Neither ``single_op`` nor ``multiple_op`` have been provided.\n    \"\"\"\n    if single_op is None and multiple_op is None:\n        raise ValueError(\"At least one function must be provided.\")\n    args = {}\n    if single_op is not None:\n        args['single_op' if cls is not PsiLocal else 'f'] = single_op\n    if multiple_op is not None:\n        args['multiple_op'] = multiple_op\n    return lambda: cls(**{k: type(v).from_config(v.get_config()) if isinstance(v, tf.keras.layers.Layer) else v for k, v in args.items()}, name=name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.PsiNonLocal.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(\n    name: str | None,\n    single_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n    multiple_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n) -&gt; Callable[[str], PsiNonLocal]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code> returns an instance of <code>cls</code> initialized with the result of the  application of <code>a</code> to the function <code>single_op</code> and/or <code>multiple_op</code>, and <code>name</code>.</p> <p>Calling this method on a <code>PsiNonLocal</code> class creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The functions <code>single_op</code> and <code>multiple_op</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>single_op</code> and <code>multiple_op</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; PsiNonLocal.make_parametrized(name='Add', single_op=lambda y: lambda x: x + y, multiple_op=lambda y: lambda x, i: x + y)\n&lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; PsiNonLocal.make_parametrized( name='Add', single_op=lambda y, x: x + y, multiple_op=lambda y, x, i: x + y)\n&lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>single_op</code> and <code>multiple_op</code>.</p> </li> <li> <code>single_op</code>             (<code>Callable[[str], Callable] | Callable[..., Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>as the <code>single_op</code> argument.</p> </li> <li> <code>multiple_op</code>             (<code>Callable[[str], Callable] | Callable[..., Any] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code> as the <code>multiple_op</code> argument.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>Neither <code>single_op</code> nor <code>multiple_op</code> have been provided.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls, name: str | None, single_op: Callable[[str], Callable] | Callable[..., Any] | None = None,\n                      multiple_op: Callable[[str], Callable] | Callable[..., Any] | None = None) -&gt; Callable[[str], PsiNonLocal]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a`` returns an instance of ``cls`` initialized with the result of the\n     application of ``a`` to the function ``single_op`` and/or ``multiple_op``, and ``name``.\n\n    Calling this method on a ``PsiNonLocal`` class creates a one-argument lambda that returns an instance of such subclass. This way, whenever that\n    function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights,\n    which are not supposed to be shared with other instances of the function. The functions ``single_op`` and ``multiple_op`` may have the form ``lambda\n    x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned by this function corresponds to the ``x`` argument of\n    ``single_op`` and ``multiple_op``.\n\n    Examples:\n        &gt;&gt;&gt; PsiNonLocal.make_parametrized(name='Add', single_op=lambda y: lambda x: x + y, multiple_op=lambda y: lambda x, i: x + y)\n        &lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; PsiNonLocal.make_parametrized( name='Add', single_op=lambda y, x: x + y, multiple_op=lambda y, x, i: x + y)\n        &lt;function PsiNonLocal.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``single_op`` and ``multiple_op``.\n        single_op: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``as the\n            ``single_op`` argument.\n        multiple_op: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls`` as the\n            ``multiple_op`` argument.\n\n    Raises:\n        ValueError: Neither ``single_op`` nor ``multiple_op`` have been provided.\n    \"\"\"\n    if single_op is None and multiple_op is None:\n        raise ValueError(\"At least one function must be provided.\")\n    args = {}\n    if single_op is not None:\n        args['single_op' if cls is not PsiLocal else 'f'] = single_op\n    if multiple_op is not None:\n        args['multiple_op'] = multiple_op\n    return lambda a: cls(**{k: v(a) if v.__code__.co_argcount == 1 else partial(v, a) for k, v in args.items()},\n                         name=name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Phi","title":"Phi","text":"<pre><code>Phi(f: Callable[[tuple[tf.Tensor, ...], tf.Tensor, tuple[tf.Tensor, ...]], tf.Tensor | tuple[tf.Tensor, ...]] | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>Function</code></p> <p>A phi function of the mG language.</p> <p>A function \\(\\varphi: T \\times U \\times T \\rightarrow V\\) to compute the message sent by a node i to a node j through edge e.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi(lambda i, e, j: i * e, name='EdgeProd')\n&lt;Phi ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>f</code>             (<code>Callable[[tuple[Tensor, ...], Tensor, tuple[Tensor, ...]], Tensor | tuple[Tensor, ...]] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor X1 with shape <code>(n_edges, n_node_features)</code>, containing the labels of all nodes sending a message, a tensor E with shape <code>(n_edges, n_edge_features)</code>, containing the labels of all edges in the graph, and a tensor X2, containing the labels of all nodes receiving a message. The function is expected to return a tensor with shape <code>(n_edges, n_message_features)</code>, containing the messages to be sent to the destination nodes.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, f: Callable[[tuple[tf.Tensor, ...], tf.Tensor, tuple[tf.Tensor, ...]], tf.Tensor | tuple[tf.Tensor, ...]] | None = None,\n             name: str | None = None):\n    \"\"\"Initializes the instance with a function and a name.\n\n    Args:\n        f: The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor X1 with shape ``(n_edges, n_node_features)``, containing the labels of\n            all nodes sending a message, a tensor E with shape ``(n_edges, n_edge_features)``, containing the labels of all edges in the graph, and a tensor\n            X2, containing the labels of all nodes receiving a message. The function is expected to return a tensor with shape\n            ``(n_edges, n_message_features)``, containing the messages to be sent to the destination nodes.\n        name: The name of the function.\n    \"\"\"\n    if f is None:\n        f = self.func\n    super().__init__(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Phi.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Phi.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, f: Callable) -&gt; Callable[[], T_A]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n&lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function.</p> </li> <li> <code>f</code>             (<code>Callable</code>)         \u2013          <p>The function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls: Type[T_A], name: str | None, f: Callable) -&gt; Callable[[], T_A]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n        &lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function.\n        f: The function that will be used to instantiate ``cls``.\n    \"\"\"\n    if isinstance(f, tf.keras.layers.Layer):  # if f is a layer, regenerate from config to get new weights\n        return lambda: cls(type(f).from_config(f.get_config()), name)  # type: ignore\n    else:\n        return lambda: cls(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Phi.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code>  returns an instance of <code>cls</code> initialized with the result of the application of <code>a</code> to the function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The function <code>f</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>f</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>f</code>.</p> </li> <li> <code>f</code>             (<code>Callable[[str], Callable] | Callable[..., Any]</code>)         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls: Type[T_A], name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a``\n     returns an instance of ``cls`` initialized with the result of the application of ``a`` to the function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function. The function ``f`` may have the form ``lambda x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned\n    by this function corresponds to the ``x`` argument of ``f``.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``f``.\n        f: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``.\n    \"\"\"\n    # check if f has only a single argument e.g. lambda x: lambda y: foo(x)(y)\n    if f.__code__.co_argcount == 1:\n        return lambda a: cls(f(a), name + '_' + a if name is not None else None)\n    else:  # e.g. lambda x, y: foo(x)(y)\n        return lambda a: cls(partial(f, a), name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Sigma","title":"Sigma","text":"<pre><code>Sigma(f: Callable[[tuple[tf.Tensor, ...], tf.Tensor, int, tuple[tf.Tensor, ...]], tf.Tensor | tuple[tf.Tensor, ...]] | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>Function</code></p> <p>A sigma function of the mG language.</p> <p>A function \\(\\sigma: T^* \\times U \\rightarrow V\\) to aggregate the messages sent to a node, including the current label of the node.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Sigma(lambda m, i, n, x: tf.math.segment_max(m, i), name='Max')\n&lt;Sigma ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>f</code>             (<code>Callable[[tuple[Tensor, ...], Tensor, int, tuple[Tensor, ...]], Tensor | tuple[Tensor, ...]] | None</code>, default:                 <code>None</code> )         \u2013          <p>The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor M with shape <code>(n_edges, n_message_features)</code>, containing the generated messages, a tensor IDX with shape <code>(n_edges,)</code>, containing the ids of the node each message is being sent to, the total number of the nodes involved, and a tensor X, containing the current node labels. The function is expected to return a tensor with shape <code>(n_nodes, n_new_node_features)</code>, containing the new node labels.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, f: Callable[[tuple[tf.Tensor, ...], tf.Tensor, int, tuple[tf.Tensor, ...]], tf.Tensor | tuple[tf.Tensor, ...]] | None = None,\n             name: str | None = None):\n    \"\"\"Initializes the instance with a function and a name.\n\n    Args:\n        f: The function that this object will run when called. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor M with shape ``(n_edges, n_message_features)``, containing the generated messages,\n            a tensor IDX with shape ``(n_edges,)``, containing the ids of the node each message is being sent to, the total number of the nodes involved,\n            and a tensor X, containing the current node labels. The function is expected to return a tensor with shape\n            ``(n_nodes, n_new_node_features)``, containing the new node labels.\n        name: The name of the function.\n    \"\"\"\n    if f is None:\n        f = self.func\n    super().__init__(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Sigma.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Sigma.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, f: Callable) -&gt; Callable[[], T_A]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n&lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function.</p> </li> <li> <code>f</code>             (<code>Callable</code>)         \u2013          <p>The function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls: Type[T_A], name: str | None, f: Callable) -&gt; Callable[[], T_A]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n        &lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function.\n        f: The function that will be used to instantiate ``cls``.\n    \"\"\"\n    if isinstance(f, tf.keras.layers.Layer):  # if f is a layer, regenerate from config to get new weights\n        return lambda: cls(type(f).from_config(f.get_config()), name)  # type: ignore\n    else:\n        return lambda: cls(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Sigma.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code>  returns an instance of <code>cls</code> initialized with the result of the application of <code>a</code> to the function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The function <code>f</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>f</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>f</code>.</p> </li> <li> <code>f</code>             (<code>Callable[[str], Callable] | Callable[..., Any]</code>)         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls: Type[T_A], name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a``\n     returns an instance of ``cls`` initialized with the result of the application of ``a`` to the function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function. The function ``f`` may have the form ``lambda x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned\n    by this function corresponds to the ``x`` argument of ``f``.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``f``.\n        f: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``.\n    \"\"\"\n    # check if f has only a single argument e.g. lambda x: lambda y: foo(x)(y)\n    if f.__code__.co_argcount == 1:\n        return lambda a: cls(f(a), name + '_' + a if name is not None else None)\n    else:  # e.g. lambda x, y: foo(x)(y)\n        return lambda a: cls(partial(f, a), name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Constant","title":"Constant","text":"<pre><code>Constant(v: tf.Tensor, name: str | None = None)\n</code></pre> <p>             Bases: <code>PsiLocal</code></p> <p>A constant psi function of the mG language.</p> <p>A constant function \\(\\psi: T \\rightarrow U\\) that maps every node label to a constant value.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Constant(tf.constant(False), name='False')\n&lt;Constant ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>v</code>             (<code>Tensor</code>)         \u2013          <p>A scalar or tensor value that identifies the constant function.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, v: tf.Tensor, name: str | None = None):\n    \"\"\"Initializes the instance with a function and a name.\n\n    Args:\n        v: A scalar or tensor value that identifies the constant function.\n        name: The name of the function.\n    \"\"\"\n    if tf.reduce_all(tf.equal(tf.cast(v, dtype=tf.float32), 1.0)):\n        def f(x):\n            return tf.ones((tf.shape(x)[0], tf.size(v)), dtype=v.dtype)\n    elif tf.reduce_all(tf.equal(tf.cast(v, dtype=tf.float32), 0.0)):\n        def f(x):\n            return tf.zeros((tf.shape(x)[0], tf.size(v)), dtype=v.dtype)\n    elif tf.size(v) == 1:\n        def f(x):\n            return tf.fill((tf.shape(x)[0], tf.size(v)), value=v)\n    else:\n        def f(x):\n            return tf.tile([v], [tf.shape(x)[0], 1])\n    super().__init__(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Constant.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Constant.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, f: Callable) -&gt; Callable[[], T_A]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n&lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function.</p> </li> <li> <code>f</code>             (<code>Callable</code>)         \u2013          <p>The function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls: Type[T_A], name: str | None, f: Callable) -&gt; Callable[[], T_A]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n        &lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function.\n        f: The function that will be used to instantiate ``cls``.\n    \"\"\"\n    if isinstance(f, tf.keras.layers.Layer):  # if f is a layer, regenerate from config to get new weights\n        return lambda: cls(type(f).from_config(f.get_config()), name)  # type: ignore\n    else:\n        return lambda: cls(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Constant.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code>  returns an instance of <code>cls</code> initialized with the result of the application of <code>a</code> to the function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The function <code>f</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>f</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>f</code>.</p> </li> <li> <code>f</code>             (<code>Callable[[str], Callable] | Callable[..., Any]</code>)         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls: Type[T_A], name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a``\n     returns an instance of ``cls`` initialized with the result of the application of ``a`` to the function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function. The function ``f`` may have the form ``lambda x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned\n    by this function corresponds to the ``x`` argument of ``f``.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``f``.\n        f: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``.\n    \"\"\"\n    # check if f has only a single argument e.g. lambda x: lambda y: foo(x)(y)\n    if f.__code__.co_argcount == 1:\n        return lambda a: cls(f(a), name + '_' + a if name is not None else None)\n    else:  # e.g. lambda x, y: foo(x)(y)\n        return lambda a: cls(partial(f, a), name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Pi","title":"Pi","text":"<pre><code>Pi(i: int, j: int | None = None, name: str | None = None)\n</code></pre> <p>             Bases: <code>PsiLocal</code></p> <p>A projection psi function of the mG language.</p> <p>A projection function \\(\\psi: T^n \\rightarrow T^m\\) that maps every node label to a projection of itself.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Pi(0, 2, name='FirstTwo')\n&lt;Pi ...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>i</code>             (<code>int</code>)         \u2013          <p>0-based index, start position of the projection function, inclusive.</p> </li> <li> <code>j</code>             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>end position of the projection function, exclusive. Defaults to i + 1.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the function.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>start and end index are equal.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def __init__(self, i: int, j: int | None = None, name: str | None = None):\n    \"\"\"Initializes the instance with the projection indexes and a name.\n\n    Args:\n        i: 0-based index, start position of the projection function, inclusive.\n        j: end position of the projection function, exclusive. Defaults to i + 1.\n        name: The name of the function.\n\n    Raises:\n        ValueError: start and end index are equal.\n    \"\"\"\n    j = i + 1 if j is None else j\n    if i == j:\n        raise ValueError(\"Start index and end index cannot be equal.\")\n\n    def f(x): return x[:, i:j]\n\n    super().__init__(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Pi.fname","title":"fname  <code>property</code>","text":"<pre><code>fname\n</code></pre> <p>The name of this function. This can be either the name provided during initialization, if it was provided, or the dynamic class name.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Pi.make","title":"make  <code>classmethod</code>","text":"<pre><code>make(name: str | None, f: Callable) -&gt; Callable[[], T_A]\n</code></pre> <p>Returns a zero-argument function that when called returns an instance of <code>cls</code> initialized with the provided function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n&lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function.</p> </li> <li> <code>f</code>             (<code>Callable</code>)         \u2013          <p>The function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make(cls: Type[T_A], name: str | None, f: Callable) -&gt; Callable[[], T_A]:\n    \"\"\"Returns a zero-argument function that when called returns an instance of ``cls`` initialized with the provided function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a zero-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make('Proj3', lambda i, e, j: j)\n        &lt;function Function.make.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function.\n        f: The function that will be used to instantiate ``cls``.\n    \"\"\"\n    if isinstance(f, tf.keras.layers.Layer):  # if f is a layer, regenerate from config to get new weights\n        return lambda: cls(type(f).from_config(f.get_config()), name)  # type: ignore\n    else:\n        return lambda: cls(f, name)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.Pi.make_parametrized","title":"make_parametrized  <code>classmethod</code>","text":"<pre><code>make_parametrized(name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]\n</code></pre> <p>Returns a one-argument function that when called with the argument <code>a</code>  returns an instance of <code>cls</code> initialized with the result of the application of <code>a</code> to the function <code>f</code> and <code>name</code>.</p> <p>The class <code>cls</code> is supposed to be a <code>Function</code> subclass. Calling this method on a suitable <code>Function</code> subclass creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances of the function. The function <code>f</code> may have the form <code>lambda x: lambda ... : ...</code> or <code>lambda x, ... : ...</code>. The one argument of the lambda returned by this function corresponds to the <code>x</code> argument of <code>f</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n&gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n&lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n</code></pre> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str | None</code>)         \u2013          <p>The name of the function returned by <code>f</code>.</p> </li> <li> <code>f</code>             (<code>Callable[[str], Callable] | Callable[..., Any]</code>)         \u2013          <p>The function that when applied to some argument <code>a</code> returns the function that will be used to instantiate <code>cls</code>.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>@classmethod\ndef make_parametrized(cls: Type[T_A], name: str | None, f: Callable[[str], Callable] | Callable[..., Any]) -&gt; Callable[[str], T_A]:\n    \"\"\"Returns a one-argument function that when called with the argument ``a``\n     returns an instance of ``cls`` initialized with the result of the application of ``a`` to the function ``f`` and ``name``.\n\n    The class ``cls`` is supposed to be a ``Function`` subclass. Calling this method on a suitable ``Function`` subclass\n    creates a one-argument lambda that returns an instance of such subclass. This way, whenever that function is used in a mG program, the dictionary\n    automatically regenerates the instance. This is useful when the function has trainable weights, which are not supposed to be shared with other instances\n    of the function. The function ``f`` may have the form ``lambda x: lambda ... : ...`` or ``lambda x, ... : ...``. The one argument of the lambda returned\n    by this function corresponds to the ``x`` argument of ``f``.\n\n    Examples:\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y: lambda i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n        &gt;&gt;&gt; Phi.make_parametrized('Add', lambda y, i, e, j: j + int(y) * e)\n        &lt;function Function.make_parametrized.&lt;locals&gt;.&lt;lambda&gt; at 0x...&gt;\n\n    Args:\n        name: The name of the function returned by ``f``.\n        f: The function that when applied to some argument ``a`` returns the function that will be used to instantiate ``cls``.\n    \"\"\"\n    # check if f has only a single argument e.g. lambda x: lambda y: foo(x)(y)\n    if f.__code__.co_argcount == 1:\n        return lambda a: cls(f(a), name + '_' + a if name is not None else None)\n    else:  # e.g. lambda x, y: foo(x)(y)\n        return lambda a: cls(partial(f, a), name + '_' + a if name is not None else None)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.NodeConfig","title":"NodeConfig","text":"<pre><code>NodeConfig(node_type: tf.DType, node_size: int)\n</code></pre> <p>             Bases: <code>LabelConfig</code></p> <p>Defines the signature of a node label.</p> <p>Parameters:</p> <ul> <li> <code>node_type</code>             (<code>DType</code>)         \u2013          <p>Type of the node labels.</p> </li> <li> <code>node_size</code>             (<code>int</code>)         \u2013          <p>Dimension of the node labels.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def __init__(self, node_type: tf.DType, node_size: int):\n    \"\"\"Initializes the instance with the given type and dimension.\n\n    Args:\n        node_type: Type of the node labels.\n        node_size: Dimension of the node labels.\n    \"\"\"\n    super().__init__(node_type, node_size)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.EdgeConfig","title":"EdgeConfig","text":"<pre><code>EdgeConfig(edge_type: tf.DType, edge_size: int)\n</code></pre> <p>             Bases: <code>LabelConfig</code></p> <p>Defines the signature of an edge label.</p> <p>Parameters:</p> <ul> <li> <code>edge_type</code>             (<code>DType</code>)         \u2013          <p>Type of the edge labels.</p> </li> <li> <code>edge_size</code>             (<code>int</code>)         \u2013          <p>Dimension of the edge labels.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def __init__(self, edge_type: tf.DType, edge_size: int):\n    \"\"\"Initializes the instance with the given type and dimension.\n\n    Args:\n        edge_type: Type of the edge labels.\n        edge_size: Dimension of the edge labels.\n    \"\"\"\n    super().__init__(edge_type, edge_size)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig","title":"CompilerConfig","text":"<pre><code>CompilerConfig(node_config: NodeConfig, edge_config: EdgeConfig | None, matrix_type: tf.DType, tolerance: dict[str, float], multiple_loader: bool)\n</code></pre> <p>Defines the configuration for the mG compiler.</p> <p>It is recommended to use the static constructor methods to instantiate this class.</p> <p>Parameters:</p> <ul> <li> <code>node_config</code>             (<code>NodeConfig</code>)         \u2013          <p>The signature of the initial node labels of the graphs.</p> </li> <li> <code>edge_config</code>             (<code>EdgeConfig | None</code>)         \u2013          <p>The signature of the initial edge labels of the graphs.</p> </li> <li> <code>matrix_type</code>             (<code>DType</code>)         \u2013          <p>The signature of the adjacency matrix of the graphs.</p> </li> <li> <code>tolerance</code>             (<code>dict[str, float]</code>)         \u2013          <p>The tolerance values for the data types (typically floats) that require an approximate fixed point solution.</p> </li> <li> <code>multiple_loader</code>             (<code>bool</code>)         \u2013          <p>True if the models generated by the mG compiler will receive their inputs by a <code>MultipleGraphLoader</code>.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def __init__(self, node_config: NodeConfig, edge_config: EdgeConfig | None, matrix_type: tf.DType, tolerance: dict[str, float], multiple_loader: bool):\n    \"\"\"Initializes the instance with the initial signature of the node labels and, if present, edge labels, the signature of the adjacency matrix, the\n    tolerance values by data type and which loader will be used for the graphs.\n\n    Args:\n        node_config: The signature of the initial node labels of the graphs.\n        edge_config: The signature of the initial edge labels of the graphs.\n        matrix_type: The signature of the adjacency matrix of the graphs.\n        tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.\n        multiple_loader: True if the models generated by the mG compiler will receive their inputs by a ``MultipleGraphLoader``.\n    \"\"\"\n    self._node_config = node_config\n    self._edge_config = edge_config\n    self._matrix_type = matrix_type\n    self._tolerance = tolerance\n    self._multiple_loader = multiple_loader\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.node_feature_type","title":"node_feature_type  <code>property</code>","text":"<pre><code>node_feature_type: DType\n</code></pre> <p>Returns the type of the node labels.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.node_feature_size","title":"node_feature_size  <code>property</code>","text":"<pre><code>node_feature_size: int\n</code></pre> <p>Returns the dimension of the node labels.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.edge_feature_type","title":"edge_feature_type  <code>property</code>","text":"<pre><code>edge_feature_type: DType | None\n</code></pre> <p>Returns the type of the edge labels, if present.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.edge_feature_size","title":"edge_feature_size  <code>property</code>","text":"<pre><code>edge_feature_size: int | None\n</code></pre> <p>Returns the dimension of the edge labels, if present.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.matrix_type","title":"matrix_type  <code>property</code>","text":"<pre><code>matrix_type: DType\n</code></pre> <p>Returns the type of the adjacency matrix.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.use_edges","title":"use_edges  <code>property</code>","text":"<pre><code>use_edges: bool\n</code></pre> <p>Returns whether the mG compiler expects edge labels in the graph.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.tolerance","title":"tolerance  <code>property</code>","text":"<pre><code>tolerance: dict[str, float]\n</code></pre> <p>Returns the mapping between types and tolerance values.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.use_multiple_loader","title":"use_multiple_loader  <code>property</code>","text":"<pre><code>use_multiple_loader: bool\n</code></pre> <p>Returns whether the mG compiler expects the usage of the <code>MultipleGraphLoader</code>.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.input_spec","title":"input_spec  <code>property</code>","text":"<pre><code>input_spec: tuple[TensorSpec, ...]\n</code></pre> <p>Returns the input signature that the mG compiler expects for every model it will produce.</p>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.xa_config","title":"xa_config  <code>staticmethod</code>","text":"<pre><code>xa_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig\n</code></pre> <p>Returns a <code>CompilationConfig</code> object that tells the mG compiler to expect node labels but no edge labels in the graph  and the use of the <code>SingleGraphLoader</code> for the inputs to the model.</p> <p>Parameters:</p> <ul> <li> <code>node_config</code>             (<code>NodeConfig</code>)         \u2013          <p>The signature of the initial node labels of the graphs.</p> </li> <li> <code>matrix_type</code>             (<code>DType</code>)         \u2013          <p>The signature of the adjacency matrix of the graphs.</p> </li> <li> <code>tolerance</code>             (<code>dict[str, float]</code>)         \u2013          <p>The tolerance values for the data types (typically floats) that require an approximate fixed point solution.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>@staticmethod\ndef xa_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig:\n    \"\"\"Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels but no edge labels in the graph\n     and the use of the ``SingleGraphLoader`` for the inputs to the model.\n\n    Args:\n        node_config: The signature of the initial node labels of the graphs.\n        matrix_type: The signature of the adjacency matrix of the graphs.\n        tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.\n    \"\"\"\n    return CompilerConfig(node_config, None, matrix_type, tolerance, False)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.xai_config","title":"xai_config  <code>staticmethod</code>","text":"<pre><code>xai_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig\n</code></pre> <p>Returns a <code>CompilationConfig</code> object that tells the mG compiler to expect nodes labels but no edge labels in the graph  and the use of the <code>MultipleGraphLoader</code> for the inputs to the model.</p> <p>Parameters:</p> <ul> <li> <code>node_config</code>             (<code>NodeConfig</code>)         \u2013          <p>The signature of the initial node labels of the graphs.</p> </li> <li> <code>matrix_type</code>             (<code>DType</code>)         \u2013          <p>The signature of the adjacency matrix of the graphs.</p> </li> <li> <code>tolerance</code>             (<code>dict[str, float]</code>)         \u2013          <p>The tolerance values for the data types (typically floats) that require an approximate fixed point solution.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>@staticmethod\ndef xai_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig:\n    \"\"\"Returns a ``CompilationConfig`` object that tells the mG compiler to expect nodes labels but no edge labels in the graph\n     and the use of the ``MultipleGraphLoader`` for the inputs to the model.\n\n    Args:\n        node_config: The signature of the initial node labels of the graphs.\n        matrix_type: The signature of the adjacency matrix of the graphs.\n        tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.\n    \"\"\"\n    return CompilerConfig(node_config, None, matrix_type, tolerance, True)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.xae_config","title":"xae_config  <code>staticmethod</code>","text":"<pre><code>xae_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig\n</code></pre> <p>Returns a <code>CompilationConfig</code> object that tells the mG compiler to expect node labels and edge labels in the graph  and the use of the <code>SingleGraphLoader</code> for the inputs to the model.</p> <p>Parameters:</p> <ul> <li> <code>node_config</code>             (<code>NodeConfig</code>)         \u2013          <p>The signature of the initial node labels of the graphs.</p> </li> <li> <code>edge_config</code>             (<code>EdgeConfig</code>)         \u2013          <p>The signature of the initial edge labels of the graphs.</p> </li> <li> <code>matrix_type</code>             (<code>DType</code>)         \u2013          <p>The signature of the adjacency matrix of the graphs.</p> </li> <li> <code>tolerance</code>             (<code>dict[str, float]</code>)         \u2013          <p>The tolerance values for the data types (typically floats) that require an approximate fixed point solution.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>@staticmethod\ndef xae_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig:\n    \"\"\"Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph\n     and the use of the ``SingleGraphLoader`` for the inputs to the model.\n\n    Args:\n        node_config: The signature of the initial node labels of the graphs.\n        edge_config: The signature of the initial edge labels of the graphs.\n        matrix_type: The signature of the adjacency matrix of the graphs.\n        tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.\n    \"\"\"\n    return CompilerConfig(node_config, edge_config, matrix_type, tolerance, False)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.CompilerConfig.xaei_config","title":"xaei_config  <code>staticmethod</code>","text":"<pre><code>xaei_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig\n</code></pre> <p>Returns a <code>CompilationConfig</code> object that tells the mG compiler to expect node labels and edge labels in the graph  and the use of the <code>MultipleGraphLoader</code> for the inputs to the model.</p> <p>Parameters:</p> <ul> <li> <code>node_config</code>             (<code>NodeConfig</code>)         \u2013          <p>The signature of the initial node labels of the graphs.</p> </li> <li> <code>edge_config</code>             (<code>EdgeConfig</code>)         \u2013          <p>The signature of the initial edge labels of the graphs.</p> </li> <li> <code>matrix_type</code>             (<code>DType</code>)         \u2013          <p>The signature of the adjacency matrix of the graphs.</p> </li> <li> <code>tolerance</code>             (<code>dict[str, float]</code>)         \u2013          <p>The tolerance values for the data types (typically floats) that require an approximate fixed point solution.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>@staticmethod\ndef xaei_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -&gt; CompilerConfig:\n    \"\"\"Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph\n     and the use of the ``MultipleGraphLoader`` for the inputs to the model.\n\n    Args:\n        node_config: The signature of the initial node labels of the graphs.\n        edge_config: The signature of the initial edge labels of the graphs.\n        matrix_type: The signature of the adjacency matrix of the graphs.\n        tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.\n    \"\"\"\n    return CompilerConfig(node_config, edge_config, matrix_type, tolerance, True)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.MGCompiler","title":"MGCompiler","text":"<pre><code>MGCompiler(\n    psi_functions: dict[str, Psi | Callable[[], Psi] | Callable[[str], Psi]],\n    sigma_functions: dict[str, Sigma | Callable[[], Sigma] | Callable[[str], Sigma]],\n    phi_functions: dict[str, Phi | Callable[[], Phi] | Callable[[str], Phi]],\n    config: CompilerConfig,\n)\n</code></pre> <p>The compiler for mG programs.</p> <p>A program is transformed into a TensorFlow model using the <code>compile</code> method.</p> <p>Attributes:</p> <ul> <li> <code>config</code>         \u2013          <p>The configuration for this compiler instance.</p> </li> <li> <code>model_inputs</code>         \u2013          <p>The input layers for the models generated by the compiler.</p> </li> <li> <code>model_input_spec</code>         \u2013          <p>The input signature for the models generated by the compiler.</p> </li> <li> <code>dummy_dataset</code>         \u2013          <p>The dataset used to trace the models generated by the compiler.</p> </li> <li> <code>visitor</code>         \u2013          <p>The visitor that traverses an expression tree to construct the model.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>psi_functions</code>             (<code>dict[str, Psi | Callable[[], Psi] | Callable[[str], Psi]]</code>)         \u2013          <p>The psi functions that this compiler will recognize.</p> </li> <li> <code>sigma_functions</code>             (<code>dict[str, Sigma | Callable[[], Sigma] | Callable[[str], Sigma]]</code>)         \u2013          <p>The sigma functions that this compiler will recognize.</p> </li> <li> <code>phi_functions</code>             (<code>dict[str, Phi | Callable[[], Phi] | Callable[[str], Phi]]</code>)         \u2013          <p>The phi functions that this compiler will recognize.</p> </li> <li> <code>config</code>             (<code>CompilerConfig</code>)         \u2013          <p>The configuration for the compiler.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def __init__(self, psi_functions: dict[str, Psi | Callable[[], Psi] | Callable[[str], Psi]],\n             sigma_functions: dict[str, Sigma | Callable[[], Sigma] | Callable[[str], Sigma]],\n             phi_functions: dict[str, Phi | Callable[[], Phi] | Callable[[str], Phi]], config: CompilerConfig):\n    \"\"\"Initializes the instance with the psi, phi and sigma functions that this compiler will recognize and the compiler configuration.\n\n    Args:\n        psi_functions: The psi functions that this compiler will recognize.\n        sigma_functions: The sigma functions that this compiler will recognize.\n        phi_functions: The phi functions that this compiler will recognize.\n        config: The configuration for the compiler.\n    \"\"\"\n    if config.node_feature_type == tf.float64 or config.edge_feature_type == tf.float64:\n        tf.keras.backend.set_floatx('float64')\n    elif config.node_feature_type == tf.float16 or config.edge_feature_type == tf.float16:\n        tf.keras.backend.set_floatx('float16')\n    self.config = config\n    self.model_inputs = [tf.keras.Input(shape=(config.node_feature_size, ), name=\"INPUT_X\", dtype=config.node_feature_type),\n                         tf.keras.Input(shape=(None,), sparse=True, name=\"INPUT_A\", dtype=config.matrix_type)]\n    if config.use_edges:\n        self.model_inputs.append(tf.keras.Input(shape=(config.edge_feature_size, ), name=\"INPUT_E\", dtype=config.edge_feature_type))\n    if config.use_multiple_loader:\n        self.model_inputs.append(tf.keras.Input(shape=(), name=\"INPUT_I\", dtype=tf.int64))\n    self.model_input_spec = config.input_spec\n    self.dummy_dataset = DummyDataset(NodeConfig(config.node_feature_type, config.node_feature_size), config.matrix_type,\n                                      EdgeConfig(config.edge_feature_type, config.edge_feature_size) if config.use_edges else None)  # type: ignore\n    self.visitor = self._TreeToTF(FunctionDict(psi_functions), FunctionDict(sigma_functions), FunctionDict(phi_functions), config.tolerance)\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.MGCompiler.compile","title":"compile","text":"<pre><code>compile(expr: str | Tree, verbose: bool = False, memoize: bool = False) -&gt; MGModel\n</code></pre> <p>Compiles a mG program into a TensorFlow model.</p> <p>Parameters:</p> <ul> <li> <code>expr</code>             (<code>str | Tree</code>)         \u2013          <p>The mG program to compile.</p> </li> <li> <code>verbose</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true, prints some debugging information during the compilation step.</p> </li> <li> <code>memoize</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true, memoize intermediate outputs during compilation.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>MGModel</code>         \u2013          <p>The TensorFlow model that implements <code>expr</code>.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def compile(self, expr: str | Tree, verbose: bool = False, memoize: bool = False) -&gt; MGModel:\n    \"\"\"Compiles a mG program into a TensorFlow model.\n\n    Args:\n        expr: The mG program to compile.\n        verbose: If true, prints some debugging information during the compilation step.\n        memoize: If true, memoize intermediate outputs during compilation.\n\n    Returns:\n        The TensorFlow model that implements ``expr``.\n    \"\"\"\n    x, a = self.model_inputs[:2]\n    e, i = None, None\n    if self.config.use_edges and self.config.use_multiple_loader:\n        e = self.model_inputs[-2]\n        i = self.model_inputs[-1]\n    elif self.config.use_edges:\n        e = self.model_inputs[-1]\n        i = None\n    elif self.config.use_multiple_loader:\n        e = None\n        i = self.model_inputs[-1]\n\n    self.visitor.initialize(IntermediateOutput(mg_parser.parse('__INPUT__'), (x,), a, e, i), memoize)\n    tf.keras.backend.clear_session()\n    normalized_expr_tree = mg_normalizer.normalize(expr if isinstance(expr, Tree) else mg_parser.parse(expr))\n    outputs = self.visitor.visit(normalized_expr_tree)\n    model = MGModel(self.model_inputs, outputs.x, normalized_expr_tree, self.visitor.layers, self.config,\n                    self.visitor.used_psi, self.visitor.used_phi, self.visitor.used_sigma)\n    if verbose is True:\n        model.summary(expand_nested=True, show_trainable=True)\n    return model\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.MGCompiler.trace","title":"trace","text":"<pre><code>trace(model: MGModel, api: Literal['call', 'predict', 'predict_on_batch']) -&gt; Tuple[MGModel | Callable, float]\n</code></pre> <p>Performs tracing on the model, and returns it, together with the time in seconds elapsed for tracing.</p> <p>The <code>predict_on_batch</code> API cannot be used if the graphs have edge labels as of TF 2.4.</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>MGModel</code>)         \u2013          <p>The model to trace.</p> </li> <li> <code>api</code>             (<code>Literal['call', 'predict', 'predict_on_batch']</code>)         \u2013          <p>The TensorFlow API intended to be used with the model. Options are <code>call</code> for the <code>model()</code> API, <code>predict</code> for the <code>model.predict()</code> API and <code>predict_on_batch</code> for the <code>model.predict_on_batch()</code> API.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Tuple[MGModel | Callable, float]</code>         \u2013          <p>The model and the elapsed time in seconds for tracing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>The <code>predict_on_batch</code> API has been selected and the compiler's configuration is set for graphs with edge labels.</p> </li> </ul> Source code in <code>libmg/compiler/compiler.py</code> <pre><code>def trace(self, model: MGModel, api: Literal[\"call\", \"predict\", \"predict_on_batch\"]) -&gt; Tuple[MGModel | Callable, float]:\n    \"\"\"Performs tracing on the model, and returns it, together with the time in seconds elapsed for tracing.\n\n    The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.\n\n    Args:\n        model: The model to trace.\n        api: The TensorFlow API intended to be used with the model. Options are ``call`` for the ``model()`` API,\n            ``predict`` for the ``model.predict()`` API and ``predict_on_batch`` for the ``model.predict_on_batch()`` API.\n\n    Returns:\n        The model and the elapsed time in seconds for tracing.\n\n    Raises:\n        ValueError: The ``predict_on_batch`` API has been selected and the compiler's configuration is set for graphs with edge labels.\n    \"\"\"\n    if api == 'predict_on_batch' and self.config.use_edges:\n        raise ValueError(\"The predict_on_batch API, as of TF2.4, isn't compatible with graphs with edge labels.\")\n    if self.config.use_multiple_loader:\n        dummy_loader = MultipleGraphLoader(self.dummy_dataset, batch_size=1, shuffle=False, epochs=1)\n    else:\n        dummy_loader = SingleGraphLoader(self.dummy_dataset, epochs=1)\n    traced_model = MGCompiler._graph_mode_constructor(model, self.model_input_spec, api)\n    compile_time = MGCompiler._dummy_run(traced_model, dummy_loader, api)\n    return traced_model, compile_time\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.make_uoperator","title":"make_uoperator","text":"<pre><code>make_uoperator(op: Callable[[tf.Tensor], tf.Tensor], name: str | None = None) -&gt; type\n</code></pre> <p>Returns a unary operator psi function.</p> <p>A unary operator is equivalent to a <code>PsiLocal</code>.</p> <p>Parameters:</p> <ul> <li> <code>op</code>             (<code>Callable[[Tensor], Tensor]</code>)         \u2013          <p>The unary operator function. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input a tensor X with shape <code>(n_nodes, n_node_features)</code>, containing the labels of every node in the graph, and return a tensor of shape <code>(n_nodes, n_new_node_features)</code>, containing the transformed labels.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the unary operator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type</code>         \u2013          <p>A subclass of Operator that implements the operator.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def make_uoperator(op: Callable[[tf.Tensor], tf.Tensor], name: str | None = None) -&gt; type:\n    \"\"\"Returns a unary operator psi function.\n\n    A unary operator is equivalent to a ``PsiLocal``.\n\n    Args:\n        op: The unary operator function. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input a tensor X with shape ``(n_nodes, n_node_features)``, containing the labels of\n            every node in the graph, and return a tensor of shape ``(n_nodes, n_new_node_features)``, containing the transformed labels.\n        name: The name of the unary operator.\n\n    Returns:\n        A subclass of Operator that implements the operator.\n    \"\"\"\n\n    def func(self, x: tf.Tensor) -&gt; tf.Tensor:\n        return op(x)\n\n    return type(name or \"UOperator\", (Operator,), {'func': func})\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.make_boperator","title":"make_boperator","text":"<pre><code>make_boperator(op: Callable[[tf.Tensor, tf.Tensor], tf.Tensor], name: str | None = None) -&gt; type\n</code></pre> <p>Returns a binary operator psi function.</p> <p>A binary operator is a binary local transformation of node labels, psi: (T, T) -&gt; U.</p> <p>Parameters:</p> <ul> <li> <code>op</code>             (<code>Callable[[Tensor, Tensor], Tensor]</code>)         \u2013          <p>The binary operator function. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input two tensors X1, X2 with shape <code>(n_nodes, n_node_features/2)</code>. The first tensor contains for every node the first half of the node features, while the second tensor contains the second half. The operator returns a tensor of shape <code>(n_nodes, n_new_node_features)</code>, containing the transformed labels.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the binary operator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type</code>         \u2013          <p>A subclass of Operator that implements the operator.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def make_boperator(op: Callable[[tf.Tensor, tf.Tensor], tf.Tensor], name: str | None = None) -&gt; type:\n    \"\"\"Returns a binary operator psi function.\n\n    A binary operator is a binary local transformation of node labels, psi: (T, T) -&gt; U.\n\n    Args:\n        op: The binary operator function. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input two tensors X1, X2 with shape ``(n_nodes, n_node_features/2)``. The first tensor contains for every\n            node the first half of the node features, while the second tensor contains the second half. The operator returns a tensor of shape\n            ``(n_nodes, n_new_node_features)``, containing the transformed labels.\n        name: The name of the binary operator.\n\n    Returns:\n        A subclass of Operator that implements the operator.\n    \"\"\"\n\n    def func(self, *x: list[tf.Tensor]) -&gt; tf.Tensor:\n        return op(x[0], x[1])\n\n    return type(name or \"BOperator\", (Operator,), {'func': func})\n</code></pre>"},{"location":"reference/API_reference/compiler/#libmg.compiler.make_koperator","title":"make_koperator","text":"<pre><code>make_koperator(op: Callable[..., tf.Tensor], name: str | None = None) -&gt; type\n</code></pre> <p>Returns a k-ary operator psi function.</p> <p>A k-ary operator is a k-ary local transformation of node labels, psi: (T^k) -&gt; U.</p> <p>Parameters:</p> <ul> <li> <code>op</code>             (<code>Callable[..., Tensor]</code>)         \u2013          <p>The k-ary operator function. The function must be compatible with TensorFlow's broadcasting rules. The function is expected to take in input k tensors X1, X2, ..., Xk with shape <code>(n_nodes, n_node_features/k)</code>. The first tensor contains for every node the first k of the node features, the second tensor contains the next k features, and so on. The operator returns a tensor of shape <code>(n_nodes, n_new_node_features)</code>, containing the transformed labels.</p> </li> <li> <code>name</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the k-ary operator.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>type</code>         \u2013          <p>A subclass of Operator that implements the operator.</p> </li> </ul> Source code in <code>libmg/compiler/functions.py</code> <pre><code>def make_koperator(op: Callable[..., tf.Tensor], name: str | None = None) -&gt; type:\n    \"\"\"Returns a k-ary operator psi function.\n\n    A k-ary operator is a k-ary local transformation of node labels, psi: (T^k) -&gt; U.\n\n    Args:\n        op: The k-ary operator function. The function must be compatible with TensorFlow's broadcasting\n            rules. The function is expected to take in input k tensors X1, X2, ..., Xk with shape ``(n_nodes, n_node_features/k)``. The first tensor contains\n            for every node the first k of the node features, the second tensor contains the next k features, and so on. The operator returns a tensor of shape\n            ``(n_nodes, n_new_node_features)``, containing the transformed labels.\n        name: The name of the k-ary operator.\n\n    Returns:\n        A subclass of Operator that implements the operator.\n    \"\"\"\n\n    def func(self, *x: tf.Tensor) -&gt; tf.Tensor:\n        return op(*x)\n\n    return type(name or \"KOperator\", (Operator,), {'func': func})\n</code></pre>"},{"location":"reference/API_reference/data/","title":"data","text":"<p>Defines a container for graphs and the means to process them in TensorFlow.</p> <p>This package defines a container (dataset) of graphs, based on the class of the same name in Spektral. The package also defines two loaders for datasets, based on the loaders defined in Spektral. The <code>Graph</code> class from Spektral can be imported from this package, and used for the definition of datasets.</p> <p>The package contains the following classes:</p> <ul> <li><code>Graph</code></li> <li><code>Dataset</code></li> <li><code>SingleGraphLoader</code></li> <li><code>MultipleGraphLoader</code></li> </ul>"},{"location":"reference/API_reference/data/#libmg.data.Dataset","title":"Dataset","text":"<pre><code>Dataset(name: str, **kwargs: Any)\n</code></pre> <p>             Bases: <code>Dataset</code></p> <p>Container for graphs.</p> <p>This class is supposed to be extended by overriding the <code>read</code> and <code>download</code> methods. See Spektral's documentation for additional information, as this class is directly derived from Spektral's <code>Dataset</code> class.</p> <p>Attributes:</p> <ul> <li> <code>name</code>         \u2013          <p>A string name for the dataset.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>name</code>             (<code>str</code>)         \u2013          <p>The name for the dataset.</p> </li> <li> <code>**kwargs</code>             (<code>Any</code>, default:                 <code>{}</code> )         \u2013          <p>The keyword arguments to pass to Spektral's <code>Dataset</code> class constructor.</p> </li> </ul> Source code in <code>libmg/data/dataset.py</code> <pre><code>def __init__(self, name: str, **kwargs: Any):\n    \"\"\"Initializes the instance with the given name, then the superclass will call the ``download`` and ``read`` methods as needed.\n\n    Args:\n        name: The name for the dataset.\n        **kwargs: The keyword arguments to pass to Spektral's ``Dataset`` class constructor.\n    \"\"\"\n    self.name = name\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"reference/API_reference/data/#libmg.data.SingleGraphLoader","title":"SingleGraphLoader","text":"<pre><code>SingleGraphLoader(dataset, epochs=None, sample_weights=None)\n</code></pre> <p>             Bases: <code>SingleLoader</code></p> <p>Loads a dataset made up by a single graph to be used by a TensorFlow model.</p> <p>Once instantiated, call the <code>load</code> method to obtain the generator that can be used with TensorFlow APIs. See Spektral's documentation for additional information, as this class is directly derived from Spektral's <code>SingleLoader</code> class.</p> Source code in <code>libmg/data/loaders.py</code> <pre><code>def __init__(self, dataset, epochs=None, sample_weights=None):\n    super().__init__(dataset, epochs, sample_weights)\n</code></pre>"},{"location":"reference/API_reference/data/#libmg.data.MultipleGraphLoader","title":"MultipleGraphLoader","text":"<pre><code>MultipleGraphLoader(dataset, node_level=True, batch_size=1, epochs=None, shuffle=True)\n</code></pre> <p>             Bases: <code>DisjointLoader</code></p> <p>Loads a dataset made up by more than one graph to be used by a TensorFlow model.</p> <p>Once instantiated, call the <code>load</code> method to obtain the generator that can be used with TensorFlow APIs. See Spektral's documentation for additional information, as this class is directly derived from Spektral's <code>DisjointLoader</code> class.</p> Source code in <code>libmg/data/loaders.py</code> <pre><code>def __init__(self, dataset, node_level=True, batch_size=1, epochs=None, shuffle=True):\n    super().__init__(dataset, node_level=node_level, batch_size=batch_size, epochs=epochs, shuffle=shuffle)\n</code></pre>"},{"location":"reference/API_reference/explainer/","title":"explainer","text":"<p>Defines an explainer for mG models.</p> <p>This package defines the means to generate the sub-graph of all nodes that influenced the final label of some query node.</p> <p>The package contains the following classes:</p> <ul> <li><code>MGExplainer</code></li> </ul>"},{"location":"reference/API_reference/explainer/#libmg.explainer.MGExplainer","title":"MGExplainer","text":"<pre><code>MGExplainer(model: MGModel)\n</code></pre> <p>             Bases: <code>Interpreter</code></p> <p>Generates an explanation for a mG model output.</p> <p>Generates the sub-graph of nodes that are responsible for the label of a given node.</p> <p>Attributes:</p> <ul> <li> <code>model</code>         \u2013          <p>The model to explain.</p> </li> <li> <code>query_node</code>             (<code>int | None</code>)         \u2013          <p>The node of the input graph for which the sub-graph of relevant nodes will be generated.</p> </li> <li> <code>context</code>         \u2013          <p>The context in which the current expression is being evaluated.</p> </li> <li> <code>compiler</code>         \u2013          <p>The compiler for the explainer.</p> </li> </ul> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>MGModel</code>)         \u2013          <p>The model to explain.</p> </li> </ul> Source code in <code>libmg/explainer/explainer.py</code> <pre><code>def __init__(self, model: MGModel):\n    \"\"\"Initializes the instance with the model to explain.\n\n    Args:\n        model: The model to explain.\n    \"\"\"\n    super().__init__()\n    self.model = model\n    if model.config is None:\n        raise ValueError(\"Explained model must have a valid config!\")\n    self.query_node: int | None = None\n    self.context = Context()\n    self.compiler = MGCompiler(psi_functions={'node': MGExplainer.localize_node, 'or': MGExplainer.or_fun},\n                               sigma_functions={'or': MGExplainer.or_agg},\n                               phi_functions={'p3': MGExplainer.proj1},\n                               config=model.config)\n</code></pre>"},{"location":"reference/API_reference/explainer/#libmg.explainer.MGExplainer.explain","title":"explain","text":"<pre><code>explain(\n    query_node: int, inputs: tuple[tf.Tensor, ...], filename: str | None = None, open_browser: bool = True, engine: Literal[\"pyvis\", \"cosmo\"] = \"pyvis\"\n) -&gt; Graph\n</code></pre> <p>Explain the label of a query node by generating the sub-graph of nodes that affected its value.</p> <p>Using the pyvis engine, the explanation is saved in a html file in the working directory. Using the cosmograph engine, the explanation is saved as a directory, containing an index.html file, in the working directory.</p> <p>Parameters:</p> <ul> <li> <code>query_node</code>             (<code>int</code>)         \u2013          <p>The node for which to generate the explanation.</p> </li> <li> <code>inputs</code>             (<code>tuple[Tensor, ...]</code>)         \u2013          <p>The inputs for the model to explain. This is the graph to which the query node belongs.</p> </li> <li> <code>filename</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the .html file to save in the working directory. The string <code>graph_</code> will be prepended to it.</p> </li> <li> <code>open_browser</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true, opens the default web browser and loads up the generated .html page.</p> </li> <li> <code>engine</code>             (<code>Literal['pyvis', 'cosmo']</code>, default:                 <code>'pyvis'</code> )         \u2013          <p>The visualization engine to use. Options are <code>pyvis</code> for PyVis or <code>cosmo</code> for Cosmograph.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Graph</code>         \u2013          <p>The generated sub-graph.</p> </li> </ul> Source code in <code>libmg/explainer/explainer.py</code> <pre><code>def explain(self, query_node: int, inputs: tuple[tf.Tensor, ...], filename: str | None = None, open_browser: bool = True,\n            engine: Literal[\"pyvis\", \"cosmo\"] = 'pyvis') -&gt; Graph:\n    \"\"\"Explain the label of a query node by generating the sub-graph of nodes that affected its value.\n\n    Using the pyvis engine, the explanation is saved in a html file in the working directory. Using the cosmograph engine, the explanation is saved as a\n    directory, containing an index.html file, in the working directory.\n\n    Args:\n        query_node: The node for which to generate the explanation.\n        inputs: The inputs for the model to explain. This is the graph to which the query node belongs.\n        filename: The name of the .html file to save in the working directory. The string ``graph_`` will be prepended to it.\n        open_browser: If true, opens the default web browser and loads up the generated .html page.\n        engine: The visualization engine to use. Options are ``pyvis`` for PyVis or ``cosmo`` for Cosmograph.\n\n    Returns:\n        The generated sub-graph.\n    \"\"\"\n    if self.model.expr is None:\n        raise ValueError(\"Explained model must have a valid expr!\")\n    # Build the model\n    self.query_node = query_node\n    self.context.clear()\n    actual_outputs = self.model.call(inputs)\n    try:\n        right_branch = self.visit(deepcopy(self.model.expr))\n    except VisitError:\n        right_branch = mg_parser.parse(MGExplainer.all_nodes_expr)\n    left_branch = mg_parser.parse('node[' + str(self.query_node) + ']')\n    explainer_expr_tree = mg_parser.parse('left ; right')\n    explainer_expr_tree.children = [left_branch, right_branch]\n    explainer_model = self.compiler.compile(explainer_expr_tree)\n\n    # Run the model\n    hierarchy = tf.squeeze(explainer_model.call(inputs))\n    explanation = tf.math.less(hierarchy, MGExplainer.INF)\n    graph = make_graph(explanation, hierarchy, inputs, actual_outputs)\n    if filename is not None:\n        print_graph(graph, id_generator=self._get_original_ids_func(explanation), hierarchical=True,\n                    show_labels=True, filename=filename, open_browser=open_browser, engine=engine)\n    return graph\n</code></pre>"},{"location":"reference/API_reference/language/","title":"language","text":"<p>Defines the parser and reconstructor of the mG language</p> <p>This package defines the LALR parser, and the (experimental) reconstructor.</p> <p>The package contains the following objects:</p> <ul> <li><code>mg_parser</code></li> <li><code>mg_reconstructor</code></li> </ul>"},{"location":"reference/API_reference/language/#libmg.language.mg_parser","title":"mg_parser  <code>module-attribute</code>","text":"<pre><code>mg_parser = Lark(mg_grammar, maybe_placeholders=False, parser='lalr')\n</code></pre> <p>Parser instance on which to call <code>parse</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mg_parser.parse('(a || b) ; c')\nTree('sequential_composition', [Tree('parallel_composition', ...)])\n</code></pre>"},{"location":"reference/API_reference/language/#libmg.language.mg_reconstructor","title":"mg_reconstructor  <code>module-attribute</code>","text":"<pre><code>mg_reconstructor = MGReconstructor(mg_parser)\n</code></pre> <p>Reconstructor (unparser) instance on which to call <code>reconstruct</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from lark import Tree, Token\n&gt;&gt;&gt; mg_reconstructor.reconstruct(Tree('rhd', [Tree(Token('RULE', 'label'), [Token('__ANON_1', 'a')]), Tree(Token('RULE', 'label'),\n...                              [Token('__ANON_1', 'b')])]))\n'|a&gt;b'\n</code></pre>"},{"location":"reference/API_reference/language/#libmg.language.grammar.MGReconstructor","title":"MGReconstructor","text":"<p>             Bases: <code>Reconstructor</code></p> <p>Reconstructor for the mG language.</p> <p>The reconstructor transforms a parse tree into the corresponding string, reversing the operation of parsing. This implementation is a slight modification of Lark's <code>Reconstructor</code> class to allow the addition of white spaces between the appropriate tokens.</p>"},{"location":"reference/API_reference/language/#libmg.language.grammar.MGReconstructor.reconstruct","title":"reconstruct","text":"<pre><code>reconstruct(tree: ParseTree, postproc: Callable[[Iterable[str]], Iterable[str]] | None = None, insert_spaces: bool = True) -&gt; str\n</code></pre> <p>Reconstructs a string from a parse tree.</p> <p>Parameters:</p> <ul> <li> <code>tree</code>             (<code>ParseTree</code>)         \u2013          <p>The tree to reconstruct.</p> </li> <li> <code>postproc</code>             (<code>Callable[[Iterable[str]], Iterable[str]] | None</code>, default:                 <code>None</code> )         \u2013          <p>The post-processing function to apply to each word of the reconstructed string.</p> </li> <li> <code>insert_spaces</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true, add spaces between any two words of the reconstructed string.</p> </li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from lark import Tree, Token\n&gt;&gt;&gt; mg_reconstructor.reconstruct(Tree('rhd', [Tree(Token('RULE', 'label'), [Token('__ANON_1', 'a')]), Tree(Token('RULE', 'label'),\n...                              [Token('__ANON_1', 'b')])]))\n'|a&gt;b'\n</code></pre> <p>Returns:</p> <ul> <li> <code>str</code>         \u2013          <p>The reconstructed string.</p> </li> </ul> Source code in <code>libmg/language/grammar.py</code> <pre><code>def reconstruct(self, tree: ParseTree, postproc: Callable[[Iterable[str]], Iterable[str]] | None = None, insert_spaces: bool = True) -&gt; str:\n    \"\"\"Reconstructs a string from a parse tree.\n\n    Args:\n        tree: The tree to reconstruct.\n        postproc: The post-processing function to apply to each word of the reconstructed string.\n        insert_spaces: If true, add spaces between any two words of the reconstructed string.\n\n    Examples:\n        &gt;&gt;&gt; from lark import Tree, Token\n        &gt;&gt;&gt; mg_reconstructor.reconstruct(Tree('rhd', [Tree(Token('RULE', 'label'), [Token('__ANON_1', 'a')]), Tree(Token('RULE', 'label'),\n        ...                              [Token('__ANON_1', 'b')])]))\n        '|a&gt;b'\n\n    Returns:\n        The reconstructed string.\n    \"\"\"\n    x = self._reconstruct(tree)\n    if postproc:\n        x = postproc(x)\n    y = []\n    prev_item = ''\n    for item in x:\n        if insert_spaces and prev_item and item:\n            if (prev_item not in {'&lt;', '&gt;', '(', '{', '|'} and item not in {'&lt;', '&gt;', ')', '}', ',', '|'}\n                    and not (item == '(' and prev_item not in mg_reserved)\n                    or item == ';' or prev_item == ';'):\n                y.append(' ')\n        y.append(item)\n        prev_item = item\n    return ''.join(y)\n</code></pre>"},{"location":"reference/API_reference/normalizer/","title":"normalizer","text":"<p>Defines a normalizer to transform mG expressions in normal form.</p> <p>This package defines the functions to normalize mG expressions when provided either as strings or expression trees.</p> <p>The package contains the following objects:</p> <ul> <li><code>mg_normalizer</code></li> </ul>"},{"location":"reference/API_reference/normalizer/#libmg.normalizer.mg_normalizer","title":"mg_normalizer  <code>module-attribute</code>","text":"<pre><code>mg_normalizer = Normalizer()\n</code></pre> <p>Normalizer instance on which to call <code>normalize</code>.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; mg_normalizer.normalize('fix Y = a in (fix X = b in (Y || c))')\n\"fix Y = a in (Y || c)\"\n</code></pre>"},{"location":"reference/API_reference/normalizer/#libmg.normalizer.normalizer.Normalizer","title":"Normalizer","text":"<pre><code>Normalizer()\n</code></pre> <p>             Bases: <code>Interpreter[Token, Tree]</code></p> <p>Normalizer for mG expression trees.</p> <p>Transforms a mG expression in normal form, by acting on its parse tree. Currently, this class only removes fixpoint/repeat expressions in which the fixpoint variable does not occur in the fixpoint body, and rewrites expressions so that fixpoint variables never occur on the right hand side of a sequential composition expression.</p> <p>Attributes:</p> <ul> <li> <code>bound_vars</code>         \u2013          <p>The dictionary of local variables encountered during traversal of the tree.</p> </li> <li> <code>bound_funs</code>         \u2013          <p>The dictionary of local functions encountered during traversal of the tree.</p> </li> </ul> Source code in <code>libmg/normalizer/normalizer.py</code> <pre><code>def __init__(self):\n    \"\"\"Initializes the instance.\n    \"\"\"\n    self.bound_vars = {}\n    self.bound_funs = {}\n</code></pre>"},{"location":"reference/API_reference/normalizer/#libmg.normalizer.normalizer.Normalizer.normalize","title":"normalize","text":"<pre><code>normalize(expr: Any) -&gt; Any\n</code></pre> <p>Normalizes a mG expression.</p> <p>If the expression is provided as a parse tree, a normalized parse tree is returned. If the expression is provided as string, a normalized string is returned.</p> <p>Parameters:</p> <ul> <li> <code>expr</code>             (<code>Any</code>)         \u2013          <p>The expression to normalize.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>Any</code>         \u2013          <p>The normalized expression.</p> </li> </ul> Source code in <code>libmg/normalizer/normalizer.py</code> <pre><code>@singledispatchmethod\ndef normalize(self, expr: Any) -&gt; Any:\n    \"\"\"Normalizes a mG expression.\n\n    If the expression is provided as a parse tree, a normalized parse tree is returned.\n    If the expression is provided as string, a normalized string is returned.\n\n    Args:\n        expr: The expression to normalize.\n\n    Returns:\n        The normalized expression.\n\n    \"\"\"\n    raise NotImplementedError(f\"Cannot format value of type {type(expr)}\")\n</code></pre>"},{"location":"reference/API_reference/verifier/","title":"verifier","text":""},{"location":"reference/API_reference/visualizer/","title":"visualizer","text":"<p>Defines a visualizer for graphs.</p> <p>This package defines the functions to view graphs and mG model outputs on a web browser in an interactive way.</p> <p>The package contains the following functions:</p> <ul> <li><code>print_graph(graph, node_names_func='id', hierarchical=False, show_labels=False, open_browser=True)</code></li> <li><code>print_layer(model, inputs, labels=None, layer_name=None, layer_idx=None, open_browser=True)</code></li> </ul>"},{"location":"reference/API_reference/visualizer/#libmg.visualizer.print_graph","title":"print_graph","text":"<pre><code>print_graph(\n    graph: Graph,\n    id_generator: Callable[[int], int] = lambda x: x,\n    hierarchical: bool = False,\n    show_labels: bool = False,\n    filename: str | None = None,\n    open_browser: bool = True,\n    rounding=2,\n    engine: Literal[\"pyvis\", \"cosmo\"] = \"pyvis\",\n) -&gt; None\n</code></pre> <p>Visualizes a graph.</p> <p>Parameters:</p> <ul> <li> <code>graph</code>             (<code>Graph</code>)         \u2013          <p>The graph to visualize.</p> </li> <li> <code>id_generator</code>             (<code>Callable[[int], int]</code>, default:                 <code>lambda x: x</code> )         \u2013          <p>Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.</p> </li> <li> <code>hierarchical</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true, visualize the graph in hierarchical mode. The graph must have a <code>hierarchy</code> attribute containing the hierarchy.</p> </li> <li> <code>show_labels</code>             (<code>bool</code>, default:                 <code>False</code> )         \u2013          <p>If true, show the node labels alongside the node features.</p> </li> <li> <code>filename</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the .html file to save in the working directory. The string <code>graph_</code> will be prepended to it.</p> </li> <li> <code>open_browser</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true, opens the default web browser and loads up the generated .html page.</p> </li> <li> <code>rounding</code>         \u2013          <p>How many decimal digits to show for floating-point labels, defaults to 2.</p> </li> <li> <code>engine</code>             (<code>Literal['pyvis', 'cosmo']</code>, default:                 <code>'pyvis'</code> )         \u2013          <p>The visualization engine to use. Options are <code>pyvis</code> or <code>cosmo</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>Nothing.</p> </li> </ul> Source code in <code>libmg/visualizer/visualizer.py</code> <pre><code>def print_graph(graph: Graph, id_generator: Callable[[int], int] = lambda x: x, hierarchical: bool = False, show_labels: bool = False,\n                filename: str | None = None, open_browser: bool = True, rounding=2, engine: Literal[\"pyvis\", \"cosmo\"] = 'pyvis') -&gt; None:\n    \"\"\"Visualizes a graph.\n\n    Args:\n        graph: The graph to visualize.\n        id_generator: Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and\n            so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.\n        hierarchical: If true, visualize the graph in hierarchical mode. The graph must have a ``hierarchy`` attribute containing the hierarchy.\n        show_labels: If true, show the node labels alongside the node features.\n        filename: The name of the .html file to save in the working directory. The string ``graph_`` will be prepended to it.\n        open_browser: If true, opens the default web browser and loads up the generated .html page.\n        rounding: How many decimal digits to show for floating-point labels, defaults to 2.\n        engine: The visualization engine to use. Options are ``pyvis`` or ``cosmo``.\n\n    Returns:\n        Nothing.\n    \"\"\"\n    engines[engine](graph.x, zip(graph.a.row, graph.a.col), graph.e, graph.y if show_labels else None, graph.hierarchy if hierarchical else None, id_generator,\n                    filename if filename is not None else str(graph), open_browser, rounding)\n</code></pre>"},{"location":"reference/API_reference/visualizer/#libmg.visualizer.print_layer","title":"print_layer","text":"<pre><code>print_layer(\n    model: MGModel,\n    inputs: tuple[tf.Tensor, ...],\n    labels: tf.Tensor | None = None,\n    layer_name: str | Tree | None = None,\n    layer_idx: int | None = None,\n    id_generator: Callable[[int], int | str] = lambda x: x,\n    filename: str | None = None,\n    open_browser: bool = True,\n    rounding=2,\n    engine: Literal[\"pyvis\", \"cosmo\"] = \"pyvis\",\n) -&gt; None\n</code></pre> <p>Visualizes the outputs of a model's layer.</p> <p>Layer must be identified either by name or index. If both are given, index takes precedence.</p> <p>Parameters:</p> <ul> <li> <code>model</code>             (<code>MGModel</code>)         \u2013          <p>The mG model where the layer to visualize is to be found.</p> </li> <li> <code>inputs</code>             (<code>tuple[Tensor, ...]</code>)         \u2013          <p>The inputs of the model that are used to generate the output to visualize.</p> </li> <li> <code>labels</code>             (<code>Tensor | None</code>, default:                 <code>None</code> )         \u2013          <p>If provided, also show the node labels alongside the node features generated by the visualized layer.</p> </li> <li> <code>layer_name</code>             (<code>str | Tree | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the layer to find.</p> </li> <li> <code>layer_idx</code>             (<code>int | None</code>, default:                 <code>None</code> )         \u2013          <p>The index of the layer to find.</p> </li> <li> <code>id_generator</code>             (<code>Callable[[int], int | str]</code>, default:                 <code>lambda x: x</code> )         \u2013          <p>Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.</p> </li> <li> <code>filename</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the .html file to save in the working directory. The string <code>graph_</code> will be prepended to it and the provided index or layer name will be appended to it.</p> </li> <li> <code>open_browser</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true, opens the default web browser and loads up the generated .html page.</p> </li> <li> <code>rounding</code>         \u2013          <p>How many decimal digits to show for floating-point labels, defaults to 2.</p> </li> <li> <code>engine</code>             (<code>Literal['pyvis', 'cosmo']</code>, default:                 <code>'pyvis'</code> )         \u2013          <p>The visualization engine to use. Options are <code>pyvis</code> or <code>cosmo</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li> <code>None</code>         \u2013          <p>Nothing.</p> </li> </ul> <p>Raises:</p> <ul> <li> <code>ValueError</code>           \u2013          <p>Neither a name nor an index have been given.</p> </li> <li> <code>KeyError</code>           \u2013          <p>No layer of the given name is present in the model.</p> </li> </ul> Source code in <code>libmg/visualizer/visualizer.py</code> <pre><code>def print_layer(model: MGModel, inputs: tuple[tf.Tensor, ...], labels: tf.Tensor | None = None, layer_name: str | Tree | None = None,\n                layer_idx: int | None = None, id_generator: Callable[[int], int | str] = lambda x: x, filename: str | None = None, open_browser: bool = True,\n                rounding=2, engine: Literal[\"pyvis\", \"cosmo\"] = 'pyvis') -&gt; None:\n    \"\"\"Visualizes the outputs of a model's layer.\n\n    Layer must be identified either by name or index. If both are given, index takes precedence.\n\n    Args:\n        model: The mG model where the layer to visualize is to be found.\n        inputs: The inputs of the model that are used to generate the output to visualize.\n        labels: If provided, also show the node labels alongside the node features generated by the visualized layer.\n        layer_name: The name of the layer to find.\n        layer_idx: The index of the layer to find.\n        id_generator: Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and\n            so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.\n        filename: The name of the .html file to save in the working directory. The string ``graph_`` will be prepended to it and the provided index or layer\n            name will be appended to it.\n        open_browser: If true, opens the default web browser and loads up the generated .html page.\n        rounding: How many decimal digits to show for floating-point labels, defaults to 2.\n        engine: The visualization engine to use. Options are ``pyvis`` or ``cosmo``.\n\n    Returns:\n        Nothing.\n\n    Raises:\n        ValueError: Neither a name nor an index have been given.\n        KeyError: No layer of the given name is present in the model.\n    \"\"\"\n    layer = fetch_layer(model, layer_name, layer_idx)\n    debug_model = MGModel(model.inputs, layer.output, None, None, None, None, None, None)\n    idx_or_name = layer_idx if layer_idx is not None else layer.name\n    _, a, e, _ = unpack_inputs(inputs)\n    print_labels(debug_model(inputs), a, e, labels, id_generator, filename + '_' + str(idx_or_name) if filename is not None else str(idx_or_name), open_browser,\n                 rounding, engine)\n</code></pre>"},{"location":"reference/API_reference/visualizer/#libmg.visualizer.print_labels","title":"print_labels","text":"<pre><code>print_labels(\n    x: tuple[tf.Tensor, ...],\n    a: tf.SparseTensor,\n    e: tf.Tensor | None = None,\n    y: tf.Tensor | None = None,\n    id_generator: Callable[[int], int | str] = lambda x: x,\n    filename: str | None = None,\n    open_browser: bool = True,\n    rounding=2,\n    engine: Literal[\"pyvis\", \"cosmo\"] = \"pyvis\",\n)\n</code></pre> <p>Visualizes the labeling of a graph.</p> <p>Parameters:</p> <ul> <li> <code>x</code>             (<code>tuple[Tensor, ...]</code>)         \u2013          <p>The node labels.</p> </li> <li> <code>a</code>             (<code>SparseTensor</code>)         \u2013          <p>The adjacency matrix.</p> </li> <li> <code>e</code>             (<code>Tensor | None</code>, default:                 <code>None</code> )         \u2013          <p>The edge labels, if any.</p> </li> <li> <code>y</code>             (<code>Tensor | None</code>, default:                 <code>None</code> )         \u2013          <p>If provided, also show the node labels alongside the node features generated by the visualized layer.</p> </li> <li> <code>id_generator</code>             (<code>Callable[[int], int | str]</code>, default:                 <code>lambda x: x</code> )         \u2013          <p>Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.</p> </li> <li> <code>filename</code>             (<code>str | None</code>, default:                 <code>None</code> )         \u2013          <p>The name of the .html file to save in the working directory. The string <code>graph_</code> will be prepended to it and the provided index or layer name will be appended to it.</p> </li> <li> <code>open_browser</code>             (<code>bool</code>, default:                 <code>True</code> )         \u2013          <p>If true, opens the default web browser and loads up the generated .html page.</p> </li> <li> <code>rounding</code>         \u2013          <p>How many decimal digits to show for floating-point labels, defaults to 2.</p> </li> <li> <code>engine</code>             (<code>Literal['pyvis', 'cosmo']</code>, default:                 <code>'pyvis'</code> )         \u2013          <p>The visualization engine to use. Options are <code>pyvis</code> or <code>cosmo</code>.</p> </li> </ul> <p>Returns:</p> <ul> <li>         \u2013          <p>Nothing.</p> </li> </ul> Source code in <code>libmg/visualizer/visualizer.py</code> <pre><code>def print_labels(x: tuple[tf.Tensor, ...], a: tf.SparseTensor, e: tf.Tensor | None = None, y: tf.Tensor | None = None,\n                 id_generator: Callable[[int], int | str] = lambda x: x, filename: str | None = None, open_browser: bool = True,\n                 rounding=2, engine: Literal[\"pyvis\", \"cosmo\"] = 'pyvis'):\n    \"\"\"Visualizes the labeling of a graph.\n\n    Args:\n        x: The node labels.\n        a: The adjacency matrix.\n        e: The edge labels, if any.\n        y: If provided, also show the node labels alongside the node features generated by the visualized layer.\n        id_generator: Used for determining the ID of a node. Usually this is the identity function so that the first node has ID = 0, the second has ID = 1, and\n            so on. In other situations, it might be necessary to have different mappings for integer IDs or to use string IDs.\n        filename: The name of the .html file to save in the working directory. The string ``graph_`` will be prepended to it and the provided index or layer\n            name will be appended to it.\n        open_browser: If true, opens the default web browser and loads up the generated .html page.\n        rounding: How many decimal digits to show for floating-point labels, defaults to 2.\n        engine: The visualization engine to use. Options are ``pyvis`` or ``cosmo``.\n\n    Returns:\n        Nothing.\n    \"\"\"\n    labels = y.numpy() if y is not None else None\n    engines[engine](tuple(t.numpy() for t in x), a.indices.numpy(), e.numpy() if e is not None else None, labels, None, id_generator,\n                    filename if filename is not None else 'output', open_browser, rounding)\n</code></pre>"},{"location":"tutorials/","title":"Tutorials","text":"<p>In this section you can find some tutorials to get started with libmg. </p>"},{"location":"tutorials/#ctl-model-checking","title":"CTL Model Checking","text":"<p>The most complete tutorial to date, here you will get acquainted with the main features of the library by implementing a CTL model checker. This model  checker was also described in a previous publication<sup>1</sup>.</p>"},{"location":"tutorials/#graph-convolutional-networks","title":"Graph Convolutional Networks","text":"<p>In this tutorial you will implement a Graph Convolutional Network layer, and get acquainted with trainable functions in \\(\\mu\\mathcal{G}\\).</p> <ol> <li> <p>Matteo Belenchia, Flavio Corradini, Michela Quadrini, and Michele Loreti. 2023. Implementing a CTL Model Checker with \u03bcG, a Language for Programming Graph Neural Networks. In Formal Techniques for Distributed Objects, Components, and Systems: 43<sup>rd</sup> IFIP WG 6.1 International Conference, FORTE 2023, Held as Part of the 18<sup>th</sup> International Federated Conference on Distributed Computing Techniques, DisCoTec 2023, Lisbon, Portugal, June 19\u201323, 2023, Proceedings. Springer-Verlag, Berlin, Heidelberg, 37\u201354. https://doi.org/10.1007/978-3-031-35355-0_4 \u21a9</p> </li> </ol>"},{"location":"tutorials/ctl-mc/","title":"CTL Model Checking","text":"<p>In this tutorial you will create a Computation Tree Logic (CTL) model checker using libmg.</p> <p>We start by defining a <code>Dataset</code> that contains a single <code>Graph</code>. For this tutorial, we will simply obtain the graph on-the-fly using the <code>read</code> method.</p> <pre><code>import numpy as np\nfrom scipy.sparse import coo_matrix\nfrom libmg import Dataset, Graph\n\n\nclass KripkeDataset(Dataset):\n    def __init__(self, name):\n        super().__init__(name)\n        self.atomic_propositions = {'a', 'b', 'c'}\n\n    def read(self):\n        X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 1], [0, 0, 0]], dtype=np.uint8)\n        A = coo_matrix(([1, 1, 1, 1, 1, 1, 1, 1], ([0, 0, 1, 1, 2, 2, 3, 4], [1, 2, 2, 3, 1, 3, 4, 1])), shape=(5, 5), dtype=np.uint8)\n        return [Graph(x=X, a=A)]\n\n\ndataset = KripkeDataset(\"MyDataset\")\n</code></pre> <p>The node features are three-dimensional multi-hot vectors of type <code>uint8</code>. Each node label is a multi-hot encoding of the atomic propositions that are satisfied  in the corresponding state, given a specific ordering. For this tutorial we will consider three atomic propositions \\(a, b, c\\) with order \\(a &lt; b &lt; c\\).  For example, in the first state we have the label <code>[1, 0, 0]</code> which means that in that state only proposition \\(a\\) is satisfied. In the fourth state we have <code>[1, 0, 1]</code> which means that propositions \\(a\\) and \\(c\\) are satisfied; in the last state, no proposition is satisfied.</p> <p>The adjacency matrix will have <code>uint8</code> binary entries, and we don't have edge labels.</p> <p>The dataset we have just defined and instantiated contains this graph<sup>1</sup>:</p> <p></p> <p>To implement CTL model checking, we need to define the basic propositional logic operators for logic truth, falsity, disjunction, conjunction and negation as \\(\\psi\\) functions. Additionally, we need a \\(\\varphi\\) function that just returns the label of the sender node and a \\(\\sigma\\) function that aggregates messages using logical disjunction.</p> <pre><code>import tensorflow as tf\nfrom libmg import Constant, PsiLocal, Phi, Sigma\n\n# Psi functions\nb_false = Constant(tf.constant(False))\nb_true = Constant(tf.constant(True))\nb_and = PsiLocal(lambda x: tf.math.reduce_all(x, axis=1, keepdims=True))\nb_or = PsiLocal(lambda x: tf.math.reduce_any(x, axis=1, keepdims=True))\nb_not = PsiLocal(tf.math.logical_not)\n\n# Phi functions\np1 = Phi(lambda i, e, j: i)\n\n# Sigma functions\nagg_or = Sigma(lambda m, i, n, x: tf.cast(tf.math.segment_max(tf.cast(m, tf.uint8), i), tf.bool))\n</code></pre> <p>We still need to define functions that recognize atomic propositions. To do so, each atomic proposition \\(p\\) is mapped to the corresponding one-hot vector using  the function <code>to_one_hot</code>, then using the <code>make</code> interface we define a <code>PsiLocal</code> function <code>ap</code> that is parametrized by the atomic  proposition. If, for example, in a \\(\\mu\\mathcal{G}\\) expression we write <code>ap[b]</code> the atomic proposition \\(b\\) is mapped to the one-hot vector \\((0, 1, 0)\\)  and then the dot product is computed with the label of the node. The result, once cast to Boolean values, is <code>True</code> if and only if the atomic proposition is present in the node.</p> <pre><code>def to_one_hot(label, label_set):\n    label_set = sorted(label_set)\n    index = label_set.index(label)\n    vec = [0] * len(label_set)\n    vec[index] = 1\n    return tf.constant(vec, dtype=tf.uint8)\n\nap = PsiLocal.make_parametrized('ap', lambda p, x: tf.cast(tf.math.reduce_sum(x * to_one_hot(p, dataset.atomic_propositions), axis=1, keepdims=True), dtype=tf.bool))\n</code></pre> <p>We can now instantiate the <code>MGCompiler</code> by providing the dictionaries of functions and its configuration. In the configuration we specify the shape and type of the node labels in input and the type of the adjacency matrix.</p> <pre><code>from libmg import MGCompiler, CompilerConfig, NodeConfig\n\npsi_functions = {'true': b_true, 'false': b_false,\n                 'not': b_not, 'and': b_and,\n                 'or': b_or, 'ap': ap}\nsigma_functions = {'or': agg_or}\nphi_functions = {'p1': p1}\n\nconfig = CompilerConfig.xa_config(NodeConfig(tf.uint8, len(dataset.atomic_propositions)), tf.uint8, tolerance={})\ncompiler = MGCompiler(psi_functions=psi_functions,\n                      sigma_functions=sigma_functions,\n                      phi_functions=phi_functions,\n                      config=config)\n</code></pre> <p>Next, we need to define a translation function that transforms a well-formed CTL formula into a valid \\(\\mu\\mathcal{G}\\) program. To help with that we will use an external library that provides a parser for CTL formulas, pyModelChecking. We also use a <code>var_generator</code>  function to generate fresh fixpoint variable names.</p> <pre><code>from pyModelChecking import CTL, Bool\nimport string\nimport itertools\n\n\ndef var_generator():\n    var_length = 1\n    gen = itertools.product(string.ascii_uppercase, repeat=1)\n    while True:\n        for v_tuple in gen:\n            yield ''.join(v_tuple)\n        var_length += 1\n        gen = itertools.product(string.ascii_uppercase, repeat=var_length)\n\ndef to_mG(expr):\n    def _to_mG(phi):\n        if isinstance(phi, CTL.Bool):\n            if phi == Bool(True):\n                return \"true\"\n            elif phi == Bool(False):\n                return \"false\"\n            else:\n                raise ValueError(\"Error Parsing formula: \" + phi)\n        elif isinstance(phi, CTL.AtomicProposition):\n            return str(phi)\n        elif isinstance(phi, CTL.Not):\n            return '(' + _to_mG(phi.subformula(0)) + ');not'\n        elif isinstance(phi, CTL.Or):\n            sub_formulas = list(set(phi.subformulas()))\n            return '(' + ' || '.join(['(' + _to_mG(sub_formula) + ')' for sub_formula in sub_formulas]) + ');or'\n        elif isinstance(phi, CTL.And):\n            sub_formulas = list(set(phi.subformulas()))\n            return '(' + ' || '.join(['(' + _to_mG(sub_formula) + ')' for sub_formula in sub_formulas]) + ');and'\n        elif isinstance(phi, CTL.Imply):\n            return _to_mG(CTL.Or(CTL.Not(phi.subformula(0)), phi.subformula(1)))\n        elif isinstance(phi, CTL.E):\n            sub_phi = phi.subformula(0)\n            if isinstance(sub_phi, CTL.X):\n                return \"(\" + _to_mG(sub_phi.subformula(0)) + \");|p1&gt;or\"\n            elif isinstance(sub_phi, CTL.G):\n                fixvar = next(fixvars)\n                return \"fix \" + fixvar + \" = true in (((\" + _to_mG(sub_phi.subformula(0)) + \") || (\" + fixvar + \";|p1&gt;or));and)\"\n            elif isinstance(sub_phi, CTL.F):\n                return _to_mG(CTL.EU(CTL.Bool(True), sub_phi.subformula(0)))\n            elif isinstance(sub_phi, CTL.U):\n                fixvar = next(fixvars)\n                return \"fix \" + fixvar + \" = false in (((((\" + _to_mG(sub_phi.subformula(0)) + \") || (\" + fixvar + \";|p1&gt;or));and) || (\" + _to_mG(sub_phi.subformula(1)) + \"));or)\"\n        elif isinstance(phi, CTL.A):\n            sub_phi = phi.subformula(0)\n            if isinstance(sub_phi, CTL.X):\n                return _to_mG(CTL.Not(CTL.EX(CTL.Not(sub_phi.subformula(0)))))\n            elif isinstance(sub_phi, CTL.G):\n                return _to_mG(CTL.Not(CTL.EU(CTL.Bool(True), CTL.Not(sub_phi.subformula(0)))))\n            elif isinstance(sub_phi, CTL.F):\n                return _to_mG(CTL.Not(CTL.EG(CTL.Not(sub_phi.subformula(0)))))\n            elif isinstance(sub_phi, CTL.U):\n                return _to_mG(CTL.Not(CTL.Or(CTL.EU(CTL.Not(sub_phi.subformula(1)),\n                                                    CTL.Not(CTL.Or(sub_phi.subformula(0), sub_phi.subformula(1)))),\n                                             CTL.EG(CTL.Not(sub_phi.subformula(1))))))\n        else:\n            raise ValueError(\"Error parsing formula \", phi)\n    fixvars = var_generator()\n    return _to_mG(CTL.Parser()(expr))\n</code></pre> <p>Finally, we can obtain our model by providing a CTL formula to the compiler. The property \\(\\neg(b \\lor c) \\lor EG (\\neg a \\land (b \\lor c))\\) is first translated into \\(\\mu\\mathcal{G}\\) using <code>to_mG</code>, and then it is compiled into a TensorFlow model.</p> <pre><code>expr = to_mG('~(b | c) | E G (~a &amp; (b | c))')\nmodel = compiler.compile(expr, memoize=True)\n</code></pre> <p>The function <code>to_mG</code> should have assigned <code>((fix A = true in ((((((a);not) || (((b) || (c));or));and) || (A;|p1&gt;or));and)) || ((((b) || (c));or);not));or</code>  to <code>expr</code>. Since we enabled memoization, the term corresponding to \\(b \\lor c\\) is computed only once, as can be seen by inspecting the model's structure using  the <code>plot_model</code> function from TensorFlow.</p> <pre><code>tf.keras.utils.plot_model(model)\n</code></pre> <p></p> <p>We can now run the model on the dataset. We use the <code>SingleGraphLoader</code> to convert the graph into a list of <code>Tensor</code> objects and run the model using the  <code>predict</code> API.</p> <pre><code>from libmg import SingleGraphLoader\n\nloader = SingleGraphLoader(dataset)\noutputs = model.predict(loader.load(), steps=loader.steps_per_epoch)\n</code></pre> <p>The expected output is <code>[[True], [True], [True], [False], [True]]</code> corresponding to the fact that only state \\(s_3\\) doesn't satisfy the property. We can also visualize this result using <code>print_layer</code> on the last layer:</p> <pre><code>from libmg import print_layer\n\ninputs, = next(iter(loader.load()))\nprint_layer(model, inputs, layer_idx=-1, engine='pyvis')\n</code></pre> <p></p> <p>Lastly, we can show how a certain node obtained its label. For example, node \\(s_0\\) has been affected only by its direct neighbours, since the fixed-point  computation converged in two steps:</p> <pre><code>from libmg import MGExplainer\n\nexplainer = MGExplainer(model)\nexplainer.explain(0, inputs)\n</code></pre> <p></p> <p>In this tutorial, we have learned how to implement a CTL model checker in libmg. We have seen all the main  capabilities of the library, spanning the definition of datasets, the definition of functions, the compilation of a model and its execution using the loaded  dataset. We have also seen how to visualize the outputs of the model and how to explain them.</p> <ol> <li> <p>This graph in the context of CTL model checking is called a Kripke structure.\u00a0\u21a9</p> </li> </ol>"},{"location":"tutorials/gcn/","title":"Graph Convolutional Networks","text":"<p>In this tutorial, you will create a Graph Convolutional Network (GCN) layer using libmg.</p> <p>A GCN layer performs a graph convolution by multiplying the node features with a matrix of weights, normalized using the adjacency matrix with self loops  and the corresponding degree matrix. The node features \\(Z \\in \\mathbb{R}^{N \\times C}\\) are computed according to the equation </p> \\[ Z = f(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}X\\Theta) \\] <p>where \\(\\tilde{A}\\) is the adjacency matrix \\(A\\) with the added self loops \\(\\tilde{A} = A + I\\), \\(\\tilde{D}\\) is its degree matrix, \\(X \\in \\mathbb{R}^{N \\times F}\\) is the node features matrix, and \\(\\Theta \\in \\mathbb{R}^{F \\times C}\\) is a matrix of trainable weights. The function \\(f\\) is the activation function, for example the \\(ReLU\\) function \\(ReLU(x) = \\max(0, x)\\).</p> <p>Since the term \\(A' = \\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}\\) is constant for each graph we can pre-compute it. The input graph will be modified so that each node has a self loop and have edge labels corresponding to the entries of \\(A'\\). </p> <p>We start by defining a <code>Dataset</code> that contains a single <code>Graph</code>. For this tutorial, we will simply obtain the graph on-the-fly using the <code>read</code> method. The  node features are three-dimensional one-hot vectors of type <code>float32</code>. The adjacency matrix will have <code>uint8</code> binary entries and every node will feature a  self loop.The edge features will be a single <code>float32</code> value obtained corresponding to the values of \\(A'\\) above.</p> <pre><code>import numpy as np\nfrom scipy.sparse import coo_matrix\nfrom libmg import Dataset, Graph\nclass MyDataset(Dataset):\n    def read(self):\n        X = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [1, 0, 1], [0, 0, 0]],\n                     dtype=np.float32)\n        A = coo_matrix(([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n                        ([0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 4, 4],\n                         [0, 1, 2, 1, 2, 3, 1, 2, 3, 3, 4, 1, 4])),\n                       shape=(5, 5), dtype=np.float32)\n        E = np.array([[0.3333333], [0.3333333], [0.3333333], [0.3333333],\n                      [0.3333333], [0.40824828], [0.3333333], [0.3333333],\n                      [0.40824828], [0.49999997], [0.49999997], [0.40824828],\n                      [0.49999997]], dtype=np.float32)\n        return [Graph(x=X, a=A, e=E)]\n</code></pre> <p>To implement the GCN, we define a \\(\\psi\\) function that implements the product of the node features \\(X\\) with the weight matrix \\(\\Theta\\) followed  by the application of the activation function \\(f\\), i.e. the classic dense neural network layer. For this tutorial, we will have 5 outputs features,  therefore we pass 5 to the <code>tf.keras.layers.Dense</code> constructor. Then we also need a \\(\\varphi\\) function that generates the  messages as the hadamard product between the edge labels and the node labels and a \\(\\sigma\\) function that aggregates the messages by summing them.</p> <pre><code>import tensorflow as tf\nfrom libmg import PsiLocal, Phi, Sigma\ndense = PsiLocal(tf.keras.layers.Dense(5))\nprod = Phi(lambda i, e, j: i * e)\nadd = Sigma(lambda m, i, n, x: tf.math.segment_sum(m, i))\n</code></pre> <p>We can now create the compiler instance. We will use <code>dense</code> to refer to the \\(\\psi\\) function, while we use the symbols \\(*\\) and \\(+\\) to denote the  \\(\\psi\\) function and the \\(\\sigma\\) function. The <code>CompilerConfig</code> object is created using the <code>xae_config</code> method (since we use the <code>SingleGraphLoader</code> for  datasets consisting of a single graph and we have edge labels), and it specifies the types of the node labels, edge labels and the adjacency matrix. We do  not specify any tolerance value since we will not be using fixpoints.</p> <p><pre><code>from libmg import MGCompiler, CompilerConfig, NodeConfig, EdgeConfig\n\ncompiler = MGCompiler(psi_functions={'dense': dense},\n                      phi_functions={'*': prod},\n                      sigma_functions={'+': add},\n                      config=CompilerConfig.xae_config(NodeConfig(tf.float32, 3),\n                                                       EdgeConfig(tf.float32, 1),\n                                                       tf.float32, {})\n                      )\n</code></pre> Now we can create our model, consisting of a single GCN layer, using the \\(\\mu\\mathcal{G}\\) expression \\(\\rhd_{+}^{*} ; \\mathtt{dense}\\):</p> <p><pre><code>model = compiler.compile('|*&gt;+ ; dense')\n</code></pre> Normally, this model would be trained on the data on the target node labels (which we didn't define for the dataset above). In this tutorial we skip this part  and show how the model can be run on the dataset we defined earlier. Since the dataset consists of a single graph, we will be using the <code>SingleGraphLoader</code>.</p> <pre><code>from libmg import SingleGraphLoader\n# We instantiate the dataset first\ndataset = MyDataset()\nloader = SingleGraphLoader(dataset)\n</code></pre> <p>We obtain our single graph instance, as a list of <code>Tensor</code> objects, by getting the next element from <code>loader.load()</code> and we pass it to our model using the  <code>call</code> API:</p> <p><pre><code>inputs = next(iter(loader.load()))\nprint(model.call(inputs))\n</code></pre> We should obtain in output a <code>Tensor</code> similar to this:</p> <pre><code>tf.Tensor(\n[[-0.10205704 -0.12557599  0.21571322 -0.48293048  0.28344738]\n [-0.40618476 -0.14519213  0.22650823 -0.6907434   0.54866683]\n [-0.40618476 -0.14519213  0.22650823 -0.6907434   0.54866683]\n [-0.10484731 -0.05232301  0.08413776 -0.3304627   0.15030748]\n [-0.03938639 -0.111077    0.19549546 -0.32164502  0.22442521]],\nshape=(5, 5), dtype=float32)\n</code></pre> <p>We can see that the model has trainable weights by calling <code>model.summary()</code>. You should obtain the following output:</p> <pre><code>Model: \"model\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n INPUT_X (InputLayer)           [(None, 3)]          0           []                               \n\n INPUT_A (InputLayer)           [(None, None)]       0           []                               \n\n INPUT_E (InputLayer)           [(None, 1)]          0           []                               \n\n post_image_Phi_Sigma (PostImag  (None, 3)           0           ['INPUT_X[0][0]',                \n e)                                                               'INPUT_A[0][0]',                \n                                                                  'INPUT_E[0][0]']                \n\n function_application_dense (Fu  (None, 5)           15          ['post_image_Phi_Sigma[0][0]']   \n nctionApplication)                                                                               \n\n==================================================================================================\nTotal params: 15\nTrainable params: 15\nNon-trainable params: 0\n__________________________________________________________________________________________________\n</code></pre> <p>We have implemented a GCN layer, a trainable component of many GNN applications. Instead of modifying the adjacency matrix, which is not allowed in \\(\\mu\\mathcal{G}\\), we have used edge labels to encode  the same information. We have defined some functions and composed them according to the rules of \\(\\mu\\mathcal{G}\\), obtaining a model that behaves in the  same way as a GCN layer.</p>"}]}
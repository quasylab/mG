"""Defines the mG compiler.

This module defines a compiler for mG programs and the data structures to instantiate it.

The module contains the following classes:
- ``LabelConfig``
- ``NodeConfig``
- ``EdgeConfig``
- ``FixVarConfig``
- ``CompilerConfig``
- ``Context``
- ``DummyDataset``
- ``IntermediateOutput``
- ``FixPointExpression``
- ``MGFunction``
- ``MGModel``
- ``MGCompiler``
"""
from __future__ import annotations

from typing import Callable, Tuple, Literal, Any

from lark import v_args, Tree, Token
from lark.visitors import Interpreter
import tensorflow as tf
import time
import re
import numpy as np
from scipy.sparse import coo_matrix
from spektral.data import Graph, Loader

from libmg.data.dataset import Dataset
from libmg.compiler.functions import FunctionDict, PsiNonLocal, Phi, Sigma, Psi, PsiLocal
from libmg.data.loaders import SingleGraphLoader, MultipleGraphLoader
from libmg.normalizer.normalizer import mg_normalizer
from libmg.language.grammar import mg_parser, mg_reconstructor
from libmg.compiler.layers import PreImage, PostImage, FunctionApplication, Ite, FixPoint, Repeat, Parallel


class LabelConfig:
    """Defines the signature of a label.

    Attributes:
        type: Type of the label.
        size: Dimension of the label.
    """

    def __init__(self, ltype: tf.DType, lsize: int):
        """Initializes the instance with the given type and dimension.

        Args:
            ltype: Type of the label.
            lsize: Dimension of the label
        """
        self.type = ltype
        self.size = lsize


class NodeConfig(LabelConfig):
    """Defines the signature of a node label.
    """

    def __init__(self, node_type: tf.DType, node_size: int):
        """Initializes the instance with the given type and dimension.

        Args:
            node_type: Type of the node labels.
            node_size: Dimension of the node labels.
        """
        super().__init__(node_type, node_size)


class EdgeConfig(LabelConfig):
    """Defines the signature of an edge label.
    """

    def __init__(self, edge_type: tf.DType, edge_size: int):
        """Initializes the instance with the given type and dimension.

        Args:
            edge_type: Type of the edge labels.
            edge_size: Dimension of the edge labels.
        """
        super().__init__(edge_type, edge_size)


class CompilerConfig:
    """Defines the configuration for the mG compiler.

    It is recommended to use the static constructor methods to instantiate this class.
    """

    def __init__(self, node_config: NodeConfig, edge_config: EdgeConfig | None, matrix_type: tf.DType, tolerance: dict[str, float], multiple_loader: bool):
        """Initializes the instance with the initial signature of the node labels and, if present, edge labels, the signature of the adjacency matrix, the
        tolerance values by data type and which loader will be used for the graphs.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            multiple_loader: True if the models generated by the mG compiler will receive their inputs by a ``MultipleGraphLoader``.
        """
        self._node_config = node_config
        self._edge_config = edge_config
        self._matrix_type = matrix_type
        self._tolerance = tolerance
        self._multiple_loader = multiple_loader

    @staticmethod
    def xa_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels but no edge labels in the graph
         and the use of the ``SingleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, None, matrix_type, tolerance, False)

    @staticmethod
    def xai_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect nodes labels but no edge labels in the graph
         and the use of the ``MultipleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, None, matrix_type, tolerance, True)

    @staticmethod
    def xae_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph
         and the use of the ``SingleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, edge_config, matrix_type, tolerance, False)

    @staticmethod
    def xaei_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph
         and the use of the ``MultipleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, edge_config, matrix_type, tolerance, True)

    # Aliases
    single_graph_no_edges_config = xa_config
    single_graph_with_edges_config = xae_config
    multiple_graphs_no_edges_config = xai_config
    multiple_graphs_with_edges_config = xaei_config

    @property
    def node_feature_type(self) -> tf.DType:
        """Returns the type of the node labels.
        """
        return self._node_config.type

    @property
    def node_feature_size(self) -> int:
        """Returns the dimension of the node labels.
        """
        return self._node_config.size

    @property
    def edge_feature_type(self) -> tf.DType | None:
        """Returns the type of the edge labels, if present.
        """
        return self._edge_config.type if self.use_edges and self._edge_config is not None else None

    @property
    def edge_feature_size(self) -> int | None:
        """Returns the dimension of the edge labels, if present.
        """
        return self._edge_config.size if self.use_edges and self._edge_config is not None else None

    @property
    def matrix_type(self) -> tf.DType:
        """Returns the type of the adjacency matrix.
        """
        return self._matrix_type

    @property
    def use_edges(self) -> bool:
        """Returns whether the mG compiler expects edge labels in the graph.
        """
        return self._edge_config is not None

    @property
    def tolerance(self) -> dict[str, float]:
        """Returns the mapping between types and tolerance values.
        """
        return self._tolerance

    @property
    def use_multiple_loader(self) -> bool:
        """Returns whether the mG compiler expects the usage of the ``MultipleGraphLoader``.
        """
        return self._multiple_loader

    @property
    def input_spec(self) -> tuple[tf.TensorSpec, ...]:
        """Returns the input signature that the mG compiler expects for every model it will produce.
        """
        specs = [tf.TensorSpec(shape=(None, self.node_feature_size), dtype=self.node_feature_type),
                 tf.SparseTensorSpec(shape=(None, None), dtype=self.matrix_type)]
        if self.use_edges:
            specs.append(tf.TensorSpec(shape=(None, self.edge_feature_size), dtype=self.edge_feature_type))
        if self.use_multiple_loader:
            specs.append(tf.TensorSpec(shape=(None,), dtype=tf.int64))
        return tuple(specs)


class Context:
    """Defines the context in which a mG expression is evaluated.

    Keeps track of the position in the sequential composition of expressions where the current expression is being evaluated.

    Attributes:
        context: The sequential composition expression that has been evaluated until this point.
        len: The number of sub-expressions in the sequential composition.
    """

    def __init__(self):
        """Initializes the instance.

        The context is initially empty with length 0.
        """
        self.context: Tree | None = None
        self.len = 0

    def clear(self):
        """Clears the context.

        Returns: Nothing.
        """
        self.context = None
        self.len = 0

    def get(self, expr: Tree) -> Tree:
        """Returns the contextualized expression.

        The expression is returned as-is if the context is empty, otherwise it is returned as the last expression of its sequential composition with the
        context.

        Args:
            expr: The mG expression to contextualize.
        """
        if self.context is None:
            return expr
        else:
            seq = mg_parser.parse('left ; right')
            seq.children[0] = self.context
            seq.children[1] = expr
            return seq

    def push(self, expr: Tree):
        """Adds a mG expression to the context.

        Args:
            expr: The mG expression to add to the context.

        Returns: Nothing.
        """
        if self.context is None:
            self.context = expr
            self.len = 1
        else:
            seq = mg_parser.parse('left ; right')
            seq.children[0] = self.context
            seq.children[1] = expr
            self.context = seq
            self.len += 1

    def pop(self):
        """Removes the most recent element in the context without returning it.

        Raises:
            IndexError: The context is empty.

        Returns: Nothing.
        """
        if self.len > 1 and self.context is not None:
            self.context = self.context.children[0]
            self.len -= 1
        elif self.len == 1:
            self.context = None
            self.len = 0
        else:
            raise IndexError("Attempting to pop an empty context!")


class DummyDataset(Dataset):
    """A dataset to be used for tracing the model.

    This dataset contains a single small graph with dummy data. It is meant to be run on a model by the compiler to trace the TensorFlow computation graph,
    before the model is used to handle real data.

    Attributes:
        num_node_features: Dimensionality of each node label.
        type_node_features: Type of each node label.
        type_adj_matrix: Type of the adjacency matrix.
        num_edge_features: Dimensionality of each edge label, if present.
        type_edge_features: Type of each edge label, if present.
    """

    def __init__(self, node_config: NodeConfig, type_adj_matrix: tf.DType, edge_config: EdgeConfig | None):
        """Initializes the instance with the dimension and type of node features, the type of the adjacency matrix, and, if present, the dimension and type
        of the edge features.

        Args:
            node_config: Type and dimension of each node label.
            type_adj_matrix: Type of the adjacency matrix.
            edge_config: Type and dimension of each edge label, if present.
        """
        self.num_node_features = node_config.size
        self.type_node_features = node_config.type
        self.type_adj_matrix = type_adj_matrix
        self.num_edge_features = edge_config.size if edge_config is not None else None
        self.type_edge_features = edge_config.type if edge_config is not None else None
        super().__init__('Dummy Dataset')

    def download(self):
        pass

    def read(self) -> list[Graph]:
        dummy_node_value = [0] + [0] * (self.num_node_features - 1)
        x = np.array([dummy_node_value, dummy_node_value], dtype=self.type_node_features.as_numpy_dtype)
        a = coo_matrix(([1, 1, 1, 1], ([0, 0, 1, 1], [0, 1, 0, 1])), shape=(2, 2),
                       dtype=self.type_adj_matrix.as_numpy_dtype)
        if self.num_edge_features is not None and self.type_edge_features is not None:
            dummy_edge_value = [0] + [0] * (self.num_edge_features - 1)
            e = np.array([dummy_edge_value, dummy_edge_value, dummy_edge_value, dummy_edge_value],
                         dtype=self.type_edge_features.as_numpy_dtype)
        else:
            e = None
        return [Graph(x=x, a=a, e=e)]


class IntermediateOutput:
    """Defines an intermediate output during the compilation of a model.

    Attributes:
        name: The mG expression corresponding to this output.
        x: The node labels.
        a: The adjacency matrix.
        e: The edge labels, if present.
        i: The index tensor provided by the ``MultipleGraphLoader``.
        memoize: Whether the node labels ``x`` are memoizable.
    """

    def __init__(self, name: Tree, x: tuple[tf.Tensor, ...], a: tf.SparseTensor, e: tf.Tensor | None, i: tf.Tensor | None, memoize: bool = True):
        """Initializes the instance with a name, the intermediate outputs and whether the node labels are to be memoized.

        Args:
            name: The mG expression corresponding to this output.
            x: The node labels.
            a: The adjacency matrix.
            e: The edge labels, if present.
            i: The index tensor provided by the ``MultipleGraphLoader``.
            memoize: Whether the node labels ``x`` are memoizable.
        """
        self.name = name
        self.x = x
        self.a = a
        self.e = e
        self.i = i
        self.memoize = memoize

    @property
    def full_inputs(self) -> tuple[tf.Tensor, ...]:
        """Returns all the data tensors that are not ``None``, in the order X, A, E, I.
        """
        output = self.x + (self.a,)  # type: tuple[tf.Tensor, ...]
        if self.e is not None:
            output = output + (self.e,)
        if self.i is not None:
            output = output + (self.i,)
        return output

    @property
    def psi_inputs(self) -> tuple[tf.Tensor, ...]:
        """Returns the node labels X and, if present, the index tensor I.

        Used by ``FunctionApplication`` layers.
        """
        output = self.x
        if self.i is not None:
            output = output + (self.i,)
        return output

    @property
    def img_inputs(self) -> tuple[tf.Tensor, ...]:
        """Returns the node labels X, the adjacency matrix A, and, if present, the edge labels E.

        Used by ``PreImage`` and ``PostImage`` layers.
        """
        output = self.x + (self.a,)
        if self.e is not None:
            output = output + (self.e,)
        return output

    def as_new_inputs(self) -> IntermediateOutput:
        """Returns a copy of this object where the node labels X have been substituted with a new ``tf.keras.layers.Input`` layer with the same signature as X.
        """
        sym_x = tuple(tf.keras.Input(shape=t.shape[1:], dtype=t.dtype) for t in self.x)
        return IntermediateOutput(self.name, sym_x, self.a, self.e, self.i, self.memoize)

    def step(self, name: Tree, x: tuple[tf.Tensor, ...] | tf.Tensor, memoize: bool | None = None) -> IntermediateOutput:
        """
        Returns a new ``IntermediateOutput`` object created from this object.

        The adjacency matrix, edge features and indexes are kept as is, as only the node features are allowed to change in mG. If this object represented a free
        variable in a mG expression, the hash of the node features X is updated with the new hash.

        Args:
            name: The mG expression corresponding to this output.
            x: The new node labels that will replace those of this instance.
            memoize: Whether the node labels ``x`` are memoizable. If ``None``, this property is inherited from the instance.
        """
        if memoize is None:
            memoize = self.memoize
        if not isinstance(x, tuple):
            x = (x,)
        return IntermediateOutput(name, x, self.a, self.e, self.i, memoize)


class MGFunction:
    """Container for a mG function defined by a ``def`` expression or a mG variable defined by a ``let`` expression.

    Attributes:
        name: Name of the function or variable.
        var_list: List of arguments of the function. Empty list in the case of variables.
        body_tree: The expression that is the body of the function or the value of the variable.
    """

    def __init__(self, name: str, var_list: list[str], body_tree: Tree):
        """Initializes the instance with the given name, list of arguments, and body of the expression.

        Args:
            name: Name of the function or variable.
            var_list: List of formal arguments (variable names) of the function. Empty list in the case of variables.
            body_tree: The expression that is the body of the function or the value of the variable.
        """
        self.name = name
        self.var_list = var_list  # list of names
        self.body_tree = body_tree

    # type checking could be done here someday
    def get_args(self, arguments: list[IntermediateOutput]) -> dict[str, IntermediateOutput]:
        """Returns the function argument bindings.

        Args:
            arguments: The list of values to bind.
        """
        ordered_args = self.var_list
        matched_args = {}
        for i in range(len(arguments)):
            matched_args[ordered_args[i]] = arguments[i]
        return matched_args

    @staticmethod
    def as_operator(function_name: str, n_arguments: int, is_operator: bool) -> MGFunction:
        """Returns an instance of ``MGFunction`` that applies a psi function named ``function_name`` to the parallel composition of ``n_arguments`` arguments.

        If the psi function is an operator, the number of arguments is given in square brackets.

        Args:
            function_name: The name of the psi function to apply.
            n_arguments: The number of arguments to which the psi function will be applied.
            is_operator: Whether the psi function is an operator.
        """
        name = 'op_k_' + function_name
        var_list = ['__X' + str(i) for i in range(n_arguments)]
        function_name = function_name + '[' + str(n_arguments) + ']' if is_operator else function_name
        body_tree = mg_parser.parse('(' + ' || '.join(var_list) + ') ; ' + function_name)
        return MGFunction(name, var_list, body_tree)


class MGModel(tf.keras.Model):
    """Defines a mG model.

    Subclasses a ``tf.keras.Model`` by adding some additional attributes and routing all calls to the base ``tf.keras.Model`` class. This model class ensures
    that the output is always a tuple, even when a single Tensor is returned (classic TensorFlow models unwrap tuples automatically if they contain a single
    Tensor)

    Attributes:
        expr: The mG expression that this model implements.
        mg_layers: A mapping of mG expression hashes to layers of the model. Contains all layers of the model.
        config: The configuration of the compiler that created this model.
        psi_functions: The psi functions that have been used in the model.
        phi_functions: The phi functions that have been used in the model.
        sigma_functions: The sigma functions that have been used in the model.
    """

    def __init__(self, inputs: list[tf.Tensor], outputs: tf.Tensor, expr: Tree[Token] | None, layers: dict[int, tf.keras.layers.Layer] | None,
                 config: CompilerConfig | None, psi_functions: dict[str, PsiNonLocal] | None, phi_functions: dict[str, Phi] | None,
                 sigma_functions: dict[str, Sigma] | None):
        """Initializes the instance with the inputs and outputs of the model, the expression that the model implements, the layers of the model indexed by
        expression, and the functions that have been used in it.

        Args:
            inputs: The inputs of the model.
            outputs: The outputs of the model.
            expr: The mG expression that this model implements.
            layers: A mapping of mG expression hashes to layers of the model. Contains all layers of the model.
            config: The configuration of the compiler that created this model.
            psi_functions: The psi functions that have been used in the model.
            phi_functions: The phi functions that have been used in the model.
            sigma_functions: The sigma functions that have been used in the model.
        """
        super().__init__(inputs=inputs, outputs=outputs)
        self.expr = expr
        self.mg_layers = layers
        self.config = config
        self.psi_functions = psi_functions
        self.phi_functions = phi_functions
        self.sigma_functions = sigma_functions

    def __call__(self, *args, **kwargs):
        o = super().__call__(*args, **kwargs)
        if not isinstance(o, (list, tuple)):
            return (o, )
        return o

    def call(self, *args, **kwargs):
        o = super().call(*args, **kwargs)
        if not isinstance(o, (list, tuple)):
            return (o, )
        return o

    def predict(self, *args, **kwargs):
        o = super().predict(*args, **kwargs)
        if not isinstance(o, (list, tuple)):
            return (o, )
        return o

    def pretty_print_expr(self) -> str:
        """Returns this model's mG expression as string
        """
        if self.expr is not None:
            return mg_reconstructor.reconstruct(self.expr)
        else:
            return ''


class MGCompiler:
    """The compiler for mG programs.

     A program is transformed into a TensorFlow model using the ``compile`` method.

     Attributes:
        config: The configuration for this compiler instance.
        model_inputs: The input layers for the models generated by the compiler.
        model_input_spec: The input signature for the models generated by the compiler.
        dummy_dataset: The dataset used to trace the models generated by the compiler.
        visitor: The visitor that traverses an expression tree to construct the model.
    """

    class _TreeToTF(Interpreter):
        """Visitor that walks a mG expression tree and builds the corresponding model.

        Attributes:
            psi_functions: The psi functions that can occur in an expression.
            sigma_functions: The sigma functions that can occur in an expression.
            phi_functions: The phi functions that can occur in an expression.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            use_memoization: If false, memoization is disabled globally.
            initial_inputs: The initial inputs of the model, consisting of ``tf.keras.layers.Input`` objects.
            inputs: The current intermediate outputs, which act as inputs for the next expression to be evaluated.
            context: The context in which the current expression is being evaluated.
            intermediate_outputs: The mapping of expression hashes to the corresponding memoized intermediate outputs.
            layers: The mapping of expression hashes to the layer responsible for computing their output.
            defined_functions: The mapping of def'ed function names to the expression tree of their body.
            defined_local_variables: The mapping of let'ed variable names to the expression tree of their value.
            var_input: The mapping of variable bindings for def'ed functions.
            used_psi: The psi functions that have actually occurred.
            used_phi: The phi functions that have actually occurred.
            used_sigma: The sigma functions that have actually occurred.
            eval_if_clause: A stack of fixpoint variable names, which is used to keep track of if-clauses inside fixpoint expressions.
        """
        supported_types = {'bool': tf.bool, 'int': tf.int32, 'float': tf.float32, 'uint8': tf.uint8, 'uint16': tf.uint16,
                           'uint32': tf.uint32, 'uint64': tf.uint64, 'int8': tf.int8, 'int16': tf.int16, 'int32': tf.int32,
                           'int64': tf.int64, 'float16': tf.float16, 'float32': tf.float32, 'float64': tf.float64,
                           'half': tf.float16, 'double': tf.float64}

        composite_name_generator = {
            'sequential_composition': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'parallel_composition': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'ite': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'choice': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'star': lambda t, c: Tree(data=t.data, meta=t.meta, children=[c[0].name]),
            'rep': lambda t, c: Tree(data=t.data, meta=t.meta, children=[c[0].name, c[-1]])
        }

        def __init__(self, psi_functions: FunctionDict, sigma_functions: FunctionDict, phi_functions: FunctionDict, tolerance: dict[str, float]):
            """Initializes the instance with the psi, phi, and sigma functions that can occur in the mG expressions and the tolerance values for the fixpoint
            computations.

            Args:
                psi_functions: The psi functions that can occur in an expression.
                sigma_functions: The sigma functions that can occur in an expression.
                phi_functions: The phi functions that can occur in an expression.
                tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            """
            super().__init__()
            self.psi_functions = psi_functions
            self.sigma_functions = sigma_functions
            self.phi_functions = phi_functions
            self.tolerance = tolerance
            # Initialization
            self.use_memoization: bool
            self.initial_inputs: IntermediateOutput
            self.inputs: IntermediateOutput
            self.context: Context = Context()
            self.intermediate_outputs: dict[int, IntermediateOutput] = {}
            self.layers: dict[int, tf.keras.layers.Layer] = {}
            self.defined_functions: dict[str, MGFunction] = {}
            self.defined_local_variables: dict[str, MGFunction] = {}
            self.var_input: dict[str, IntermediateOutput] = {}
            self.used_psi: dict[str, PsiNonLocal] = {}
            self.used_phi: dict[str, Phi] = {}
            self.used_sigma: dict[str, Sigma] = {}
            self.eval_if_clause: list[str] = []

        def initialize(self, initial_inputs: IntermediateOutput, use_memoization: bool) -> None:
            """Initializes and/or resets the instance. Sets the initial inputs for the next expression to compile.

            Args:
                initial_inputs: The initial inputs of the model for the next expression.
                use_memoization: If false, disables memoization globally.

            Returns:
                Nothing.
            """
            self.use_memoization = use_memoization
            self.initial_inputs = initial_inputs
            self.inputs = self.initial_inputs
            self.context.clear()
            self.intermediate_outputs = {}
            self.layers = {}
            self.defined_functions = {}
            self.defined_local_variables = {}
            self.var_input = {}
            self.used_psi = {}
            self.used_phi = {}
            self.used_sigma = {}
            self.eval_if_clause = []

        def clone(self, start_from: IntermediateOutput) -> MGCompiler._TreeToTF:
            """Returns a fresh copy of this visitor, except that:
                - The initial inputs are those specified in ``start_from``.
                - The defined functions and variables are carried over.
                - The variable bindings are carried over.
                - The fixpoint variable bindings are carried over.
                - The free fixpoint variable bindings are carried over.

            Args:
                start_from: The initial inputs for the new visitor instance.
            """
            clone = type(self)(self.psi_functions, self.sigma_functions, self.phi_functions, self.tolerance)
            clone.initialize(start_from, self.use_memoization)
            clone.defined_functions = self.defined_functions.copy()
            clone.defined_local_variables = self.defined_local_variables.copy()
            clone.var_input = self.var_input.copy()
            return clone

        def add_layer(self, intermediate_output: IntermediateOutput, op_layer: tf.keras.layers.Layer, ctx_name: Tree) -> None:
            """Saves an intermediate output under the hash key of the corresponding contextualized expression tree.

            It is saved only if the output is set as memoizable.

            Args:
                intermediate_output: The value to memoize.
                op_layer: The layer that generated the value.
                ctx_name: The expression tree corresponding to the value.

            Returns:
                Nothing.
            """
            hsh = hash(ctx_name)
            if intermediate_output.memoize:
                if self.use_memoization:
                    self.intermediate_outputs[hsh] = intermediate_output
                self.layers[hsh] = op_layer

        def get_layer(self, ctx_name: Tree) -> IntermediateOutput:
            """Returns a saved output.

            Args:
                ctx_name: The saved output to retrieve, identified by its contextualized expression tree.

            Raises:
                KeyError: The requested output has not been saved.
            """
            hsh = hash(ctx_name)
            if hsh in self.intermediate_outputs:
                return self.intermediate_outputs[hsh]
            else:
                raise KeyError("No layer with name:", str(ctx_name))

        def undef_layer(self, ctx_name: Tree) -> bool:
            """Returns whether an output has not been saved.

            Args:
                ctx_name: The contextualized name of the output.
            """
            if not self.use_memoization:
                return True
            else:
                hsh = hash(ctx_name)
                return not (hsh in self.intermediate_outputs)

        @staticmethod
        def get_composite_name(tree: Tree, interpreted_children: list[IntermediateOutput | Tree | int]) -> Tree:
            """Returns the expression tree of a composite mG expression by substituting variables with their corresponding expression tree.

            The sub-expressions of a composite expression may contain variables (from def or let expressions) which should replace the variable symbols once
            evaluated.

            Args:
                tree: The expression tree of the composite mG expression.
                interpreted_children: The sub-expressions of ``tree``.
            """
            return MGCompiler._TreeToTF.composite_name_generator[tree.data](tree, interpreted_children)

        def get_tolerance(self, _type: str) -> float | None:
            """Returns the tolerance for the given type, if present.

            Args:
                _type: The type for which to obtain the tolerance value.
            """
            if _type == 'float32':
                return self.tolerance.get(_type, self.tolerance.get('float', None))
            elif _type == 'int32':
                return self.tolerance.get(_type, self.tolerance.get('int', None))
            elif _type == 'float16':
                return self.tolerance.get(_type, self.tolerance.get('half', None))
            elif _type == 'float64':
                return self.tolerance.get(_type, self.tolerance.get('double', None))
            else:
                return self.tolerance.get(_type, None)

        @v_args(inline=True)
        def label(self, label: Token) -> str:
            """Evaluates a label.

            Args:
                label: The label to evaluate.

            Returns:
                The label.
            """
            return str(label)

        @v_args(inline=True)
        def label_decl(self, label_decl: Token) -> str:
            """Evaluates a label declaration.

            Args:
                label_decl: The label declaration to evaluate.

            Returns:
                The label declaration.
            """
            return str(label_decl)

        def id(self, tree):
            op_layer = FunctionApplication(PsiLocal(lambda *x: x))
            ctx_name = self.context.get(tree)
            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, op_layer(self.inputs.psi_inputs))
                self.add_layer(output, op_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def atom_op(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a psi function or a variable.

            This can be, in order of priority:
                - A fixpoint variable
                - A free fixpoint variable
                - A bound variable of a def'ed function
                - A bound variable of a let expression
                - A psi function

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: This expression doesn't correspond to any variable or psi function.
            """
            children = self.visit_children(tree)
            label = children[0]
            if label in self.var_input:  # we are inside a defined function being called (used only for operators)
                return self.inputs.step(self.var_input[label].name, self.var_input[label].x, self.free_fix_var)
            if label in self.psi_functions:  # the label matches a psi function
                op_layer = FunctionApplication(self.psi_functions[label])
                ctx_name = self.context.get(tree)
                self.used_psi[FunctionDict.parse_key(label)[0]] = self.psi_functions[label]
            elif re.search(r'^(p\d+|p\d+-|p-\d+|p\d+-\d+)$', label) is not None:  # the label is a projection function
                proj = [int(i) for i in re.findall(r'\d+', label)]
                if min(proj) <= 0:
                    raise SyntaxError('Invalid projection value: ' + str(min(proj)))
                if re.search(r'^p\d+-\d+$', label) is not None:
                    proj1, proj2 = proj
                    op_layer = FunctionApplication(PsiLocal(lambda *x: x[proj1 - 1: proj2]))
                elif re.search(r'^p-\d+$', label) is not None:
                    proj1, = proj
                    op_layer = FunctionApplication(PsiLocal(lambda *x: x[:proj1]))
                elif re.search(r'^p\d+-$', label) is not None:
                    proj1, = proj
                    op_layer = FunctionApplication(PsiLocal(lambda *x: x[proj1-1:]))
                elif re.search(r'^p\d+$', label) is not None:
                    proj1, = proj
                    op_layer = FunctionApplication(PsiLocal(lambda *x: x[proj1 - 1]))
                else:
                    raise ValueError('Invalid projection name: ' + label)
                ctx_name = self.context.get(tree)
            else:
                raise SyntaxError('Undeclared variable or function: ' + label)

            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, op_layer(self.inputs.psi_inputs))
                self.add_layer(output, op_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def lhd(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a pre-image expression.

            Args:
                tree: The expression tree.

            Returns:
                 The output of this expression.
            """
            ctx_name = self.context.get(tree)
            args = self.visit_children(tree)
            if len(args) == 2:
                edge_function, agg_function = args
                lhd_layer = PreImage(self.sigma_functions[agg_function], self.phi_functions[edge_function])
                self.used_sigma[FunctionDict.parse_key(agg_function)[0]] = self.sigma_functions[agg_function]
                self.used_phi[FunctionDict.parse_key(edge_function)[0]] = self.phi_functions[edge_function]
            else:
                agg_function, = args
                lhd_layer = PreImage(self.sigma_functions[agg_function])
                self.used_sigma[FunctionDict.parse_key(agg_function)[0]] = self.sigma_functions[agg_function]

            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, lhd_layer(self.inputs.img_inputs), self.free_fix_var)
                self.add_layer(output, lhd_layer, ctx_name)
                return output
            return self.get_layer(ctx_name)

        def rhd(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a post-image expression.

            Args:
                tree: The expression tree.

            Returns:
                 The output of this expression.
            """
            ctx_name = self.context.get(tree)
            args = self.visit_children(tree)
            if len(args) == 2:
                edge_function, agg_function = args
                rhd_layer = PostImage(self.sigma_functions[agg_function], self.phi_functions[edge_function])
                self.used_sigma[FunctionDict.parse_key(agg_function)[0]] = self.sigma_functions[agg_function]
                self.used_phi[FunctionDict.parse_key(edge_function)[0]] = self.phi_functions[edge_function]
            else:
                agg_function, = args
                rhd_layer = PostImage(self.sigma_functions[agg_function])
                self.used_sigma[FunctionDict.parse_key(agg_function)[0]] = self.sigma_functions[agg_function]

            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, rhd_layer(self.inputs.img_inputs), self.free_fix_var)
                self.add_layer(output, rhd_layer, ctx_name)
                return output
            return self.get_layer(ctx_name)

        def sequential_composition(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a sequential composition expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            left, right = tree.children
            current_inputs = self.inputs
            phi = self.visit(left)
            self.context.push(phi.name)
            self.inputs = phi
            psi = self.visit(right)
            self.context.pop()
            self.inputs = current_inputs
            return self.inputs.step(self.get_composite_name(tree, [phi, psi]), psi.x)

        def parallel_composition(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a parallel composition expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            terms = self.visit_children(tree)
            name = self.get_composite_name(tree, terms)
            ctx_name = self.context.get(name)
            op_layer = Parallel()
            if self.undef_layer(ctx_name):
                output = self.inputs.step(name, op_layer(sum([term.x for term in terms], ())))
                self.add_layer(output, op_layer, ctx_name)
                return output
            return self.get_layer(ctx_name)

        def choice(self, tree: Tree) -> Any:
            iftrue, iffalse = tree.children

            inputs = self.inputs.as_new_inputs().step(self.inputs.name.children[1], self.inputs.x[1:])
            clone = self.clone(inputs)
            iftrue = clone.visit(iftrue)
            iftrue_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iftrue.x)

            inputs = self.inputs.as_new_inputs().step(self.inputs.name.children[1], self.inputs.x[1:])
            clone = self.clone(inputs)
            iffalse = clone.visit(iffalse)
            iffalse_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iffalse.x)

            name = self.get_composite_name(tree, [iftrue, iffalse])

            ite_layer = Ite(iftrue_model, iffalse_model, False)
            ctx_name = self.context.get(name)
            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(name, ite_layer(self.inputs.full_inputs))
                self.add_layer(output, ite_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def fun_call(self, tree: Tree) -> Any:
            """Evaluates a psi function as a polish notation operator.
            E.g.: psi(a, b, c) is evaluated as the mG expression def op_k_psi(__X1, __X2, __X3){(__X1 || __X2 || __X3) ; psi} in op_k_psi(a, b, c)

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            args = tree.children
            function_name = self.visit(args[0])
            arguments = [self.visit(arg) for arg in args[1:]]

            if function_name in self.psi_functions:
                deferred_function = MGFunction.as_operator(function_name, len(arguments), self.psi_functions.is_operator(function_name))
            else:
                raise KeyError("Function " + function_name + " doesn't exist.")
            matched_args = deferred_function.get_args(arguments)  # match args
            self.var_input |= matched_args  # add the deferred function vars to var input
            f_layer = self.visit(deferred_function.body_tree)  # now visit the function body
            for k in matched_args:  # eliminate the variables of this function from var_input
                self.var_input.pop(k)
            return f_layer

        def ite(self, tree: Tree) -> IntermediateOutput:
            """Evaluates an if-then-else expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            test, iftrue, iffalse = tree.children
            test = self.visit(test)
            inputs = self.inputs.as_new_inputs()
            clone = self.clone(inputs)
            iftrue = clone.visit(iftrue)
            iftrue_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iftrue.x)
            inputs = self.inputs.as_new_inputs()
            clone = self.clone(inputs)
            iffalse = clone.visit(iffalse)
            iffalse_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iffalse.x)

            name = self.get_composite_name(tree, [test, iftrue, iffalse])
            ite_layer = Ite(iftrue_model, iffalse_model)
            ctx_name = self.context.get(name)
            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(name, ite_layer(test.x + self.inputs.full_inputs))
                self.add_layer(output, ite_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def evaluate_loop_expr(self, tree: Tree, op: Literal['star', 'rep']) -> IntermediateOutput:
            """Evaluates a mG loop expression, either a fixpoint expression or a repeat expression.

            Args:
                tree: The expression tree.
                op: The operation specified by the expression.

            Returns:
                The output of the expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the fixpoint expression doesn't contain any fixpoint variable.
            """
            body = None
            iters = None
            if op == 'star':
                body, = tree.children
            elif op == 'rep':
                body, n = tree.children
                assert isinstance(n, Token)
                iters = int(n)

            nx = self.visit(body)
            model = MGModel(list(self.inputs.full_inputs), nx.x, body, None, None, None, None, None)

            name = None
            fix_layer = None
            if op == 'star':
                tolerance = [self.get_tolerance(tf.as_dtype(t.dtype).name) for t in nx.x]
                fix_layer = FixPoint(model, tolerance, debug=False)
                name = self.get_composite_name(tree, [nx])
            elif op == 'rep':
                assert iters is not None
                fix_layer = Repeat(model, iters)
                name = self.get_composite_name(tree, [nx, iters])

            ctx_name = self.context.get(name)
            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(name, fix_layer(self.inputs.full_inputs), self.free_fix_var)
                self.add_layer(output, fix_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def star(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a fixpoint expression.

            Args:
                tree: The expression tree

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the fixpoint expression doesn't contain any fixpoint variable.
            """
            return self.evaluate_loop_expr(tree, 'star')

        def rep(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a repeat expression.

            Args:
                tree: The expression tree

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the repeat expression doesn't contain any fixpoint variable.
            """
            return self.evaluate_loop_expr(tree, 'rep')

    def __init__(self, psi_functions: dict[str, Psi | Callable[[], Psi] | Callable[[str], Psi]],
                 sigma_functions: dict[str, Sigma | Callable[[], Sigma] | Callable[[str], Sigma]],
                 phi_functions: dict[str, Phi | Callable[[], Phi] | Callable[[str], Phi]], config: CompilerConfig):
        """Initializes the instance with the psi, phi and sigma functions that this compiler will recognize and the compiler configuration.

        Args:
            psi_functions: The psi functions that this compiler will recognize.
            sigma_functions: The sigma functions that this compiler will recognize.
            phi_functions: The phi functions that this compiler will recognize.
            config: The configuration for the compiler.
        """
        if config.node_feature_type == tf.float64 or config.edge_feature_type == tf.float64:
            tf.keras.backend.set_floatx('float64')
        elif config.node_feature_type == tf.float16 or config.edge_feature_type == tf.float16:
            tf.keras.backend.set_floatx('float16')
        self.config = config
        self.model_inputs = [tf.keras.Input(shape=(config.node_feature_size, ), name="INPUT_X", dtype=config.node_feature_type),
                             tf.keras.Input(shape=(None,), sparse=True, name="INPUT_A", dtype=config.matrix_type)]
        if config.use_edges:
            self.model_inputs.append(tf.keras.Input(shape=(config.edge_feature_size, ), name="INPUT_E", dtype=config.edge_feature_type))
        if config.use_multiple_loader:
            self.model_inputs.append(tf.keras.Input(shape=(), name="INPUT_I", dtype=tf.int64))
        self.model_input_spec = config.input_spec
        self.dummy_dataset = DummyDataset(NodeConfig(config.node_feature_type, config.node_feature_size), config.matrix_type,
                                          EdgeConfig(config.edge_feature_type, config.edge_feature_size) if config.use_edges else None)  # type: ignore
        self.visitor = self._TreeToTF(FunctionDict(psi_functions), FunctionDict(sigma_functions), FunctionDict(phi_functions), config.tolerance)

    @staticmethod
    def _graph_mode_constructor(model: MGModel, input_spec: tuple[tf.TensorSpec, ...],
                                method: Literal["call", "predict", "predict_on_batch"]) -> Callable[[list[tf.Tensor]], tuple[tf.Tensor, ...]] | MGModel:
        """Prepares a model for tracing.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to prepare for tracing.
            input_spec: The input signature for the model.
            method: The TensorFlow API used to run the model. Options are ``call``, ``predict`` or ``predict_on_batch``. This should be the same as the API
                that is later used to run the model.

        Returns:
            If ``method`` was ``call``, returns a ``tf.function`` that runs the model. Otherwise, returns the same model but with its ``predict_func`` wrapped
            in a ``tf.function``.
        """
        if method == 'call':
            @tf.function(input_signature=[input_spec])
            def serve(x: list[tf.Tensor]) -> tuple[tf.Tensor, ...]:
                return model(x, training=False)  # type: ignore

            return serve  # type: ignore
        else:
            # model.run_eagerly = True  # type: ignore
            # predict_func = model.make_predict_function()
            # TODO: something changed here
            # model.predict_function = tf.function(predict_func, input_signature=[tf.data.IteratorSpec((input_spec,))])  # type: ignore
            # model.run_eagerly = False  # type: ignore
            return model

    @staticmethod
    def _dummy_run(model: MGModel | Callable[[list[tf.Tensor]], tf.Tensor], dummy_loader: Loader,
                   method: Literal["call", "predict", "predict_on_batch"]) -> float:
        """Runs the model on the dummy dataset.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to run on the dummy dataset.
            dummy_loader: The loader of the dummy dataset.
            method: The TensorFlow API used to run the model. Options are ``call``, ``predict`` or ``predict_on_batch``. This should be the same as the API
                that is later used to run the model.

        Returns:
            The duration in seconds of the dummy run.
        """
        elapsed = 0.0
        if method == 'call':
            for x, in dummy_loader.load():
                start = time.perf_counter()
                model(x)
                end = time.perf_counter()
                elapsed = end - start
                print("Tracing completed in ", elapsed, "s", sep='')
                break
        elif method == 'predict':
            assert isinstance(model, MGModel)
            start = time.perf_counter()
            model.predict(dummy_loader.load(), steps=dummy_loader.steps_per_epoch)
            end = time.perf_counter()
            elapsed = end - start
            print("Tracing completed in ", elapsed, "s", sep='')
        else:
            assert isinstance(model, MGModel)
            for x, in dummy_loader.load():
                start = time.perf_counter()
                model.predict_on_batch(x)
                end = time.perf_counter()
                elapsed = end - start
                print("Tracing completed in ", elapsed, "s", sep='')
                break
        return elapsed

    def compile(self, expr: str | Tree, verbose: bool = False, memoize: bool = False) -> MGModel:
        """Compiles a mG program into a TensorFlow model.

        Args:
            expr: The mG program to compile.
            verbose: If true, prints some debugging information during the compilation step.
            memoize: If true, memoize intermediate outputs during compilation.

        Returns:
            The TensorFlow model that implements ``expr``.
        """
        x, a = self.model_inputs[:2]
        e, i = None, None
        if self.config.use_edges and self.config.use_multiple_loader:
            e = self.model_inputs[-2]
            i = self.model_inputs[-1]
        elif self.config.use_edges:
            e = self.model_inputs[-1]
            i = None
        elif self.config.use_multiple_loader:
            e = None
            i = self.model_inputs[-1]

        self.visitor.initialize(IntermediateOutput(mg_parser.parse('__INPUT__'), (x,), a, e, i), memoize)
        tf.keras.backend.clear_session()
        normalized_expr_tree = mg_normalizer.normalize(expr if isinstance(expr, Tree) else mg_parser.parse(expr))
        outputs = self.visitor.visit(normalized_expr_tree)
        model = MGModel(self.model_inputs, outputs.x, normalized_expr_tree, self.visitor.layers, self.config,
                        self.visitor.used_psi, self.visitor.used_phi, self.visitor.used_sigma)
        if verbose is True:
            model.summary(expand_nested=True, show_trainable=True)
        return model

    def trace(self, model: MGModel, api: Literal["call", "predict", "predict_on_batch"]) -> Tuple[MGModel | Callable, float]:
        """Performs tracing on the model, and returns it, together with the time in seconds elapsed for tracing.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to trace.
            api: The TensorFlow API intended to be used with the model. Options are ``call`` for the ``model()`` API,
                ``predict`` for the ``model.predict()`` API and ``predict_on_batch`` for the ``model.predict_on_batch()`` API.

        Returns:
            The model and the elapsed time in seconds for tracing.

        Raises:
            ValueError: The ``predict_on_batch`` API has been selected and the compiler's configuration is set for graphs with edge labels.
        """
        if api == 'predict_on_batch' and self.config.use_edges:
            raise ValueError("The predict_on_batch API, as of TF2.4, isn't compatible with graphs with edge labels.")
        if self.config.use_multiple_loader:
            dummy_loader = MultipleGraphLoader(self.dummy_dataset, batch_size=1, shuffle=False, epochs=1)
        else:
            dummy_loader = SingleGraphLoader(self.dummy_dataset, epochs=1)
        traced_model = MGCompiler._graph_mode_constructor(model, self.model_input_spec, api)
        compile_time = MGCompiler._dummy_run(traced_model, dummy_loader, api)
        return traced_model, compile_time

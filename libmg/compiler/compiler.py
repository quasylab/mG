"""Defines the mG compiler.

This module defines a compiler for mG programs and the data structures to instantiate it.

The module contains the following classes:
- ``LabelConfig``
- ``NodeConfig``
- ``EdgeConfig``
- ``FixVarConfig``
- ``CompilerConfig``
- ``Context``
- ``DummyDataset``
- ``IntermediateOutput``
- ``FixPointExpression``
- ``MGFunction``
- ``MGModel``
- ``MGCompiler``
"""
from __future__ import annotations

from typing import Callable, Tuple, Literal, Any
from tensorflow.python.keras import backend

from bidict import bidict
from lark import v_args, Tree, Token
from lark.visitors import Interpreter
import tensorflow as tf
import time
import numpy as np
from scipy.sparse import coo_matrix
from spektral.data import Graph, Loader
from tensorflow.python.util.object_identity import Reference

from libmg.data.dataset import Dataset
from libmg.compiler.functions import FunctionDict, PsiNonLocal, Phi, Sigma, Psi
from libmg.data.loaders import SingleGraphLoader, MultipleGraphLoader
from libmg.normalizer.normalizer import var_occurs, mg_normalizer
from libmg.language.grammar import mg_parser, mg_reconstructor
from libmg.compiler.layers import PreImage, PostImage, FunctionApplication, Ite, FixPoint, Repeat


class LabelConfig:
    """Defines the signature of a label.

    Attributes:
        ltype: Type of the label.
        lsize: Dimension of the label.
    """

    def __init__(self, ltype: tf.DType, lsize: int):
        """Initializes the instance with the given type and dimension.

        Args:
            ltype: Type of the label.
            lsize: Dimension of the label
        """
        self.type = ltype
        self.size = lsize


class NodeConfig(LabelConfig):
    """Defines the signature of a node label.
    """

    def __init__(self, node_type: tf.DType, node_size: int):
        """Initializes the instance with the given type and dimension.

        Args:
            node_type: Type of the node labels.
            node_size: Dimension of the node labels.
        """
        super().__init__(node_type, node_size)


class EdgeConfig(LabelConfig):
    """Defines the signature of an edge label.
    """

    def __init__(self, edge_type: tf.DType, edge_size: int):
        """Initializes the instance with the given type and dimension.

        Args:
            edge_type: Type of the edge labels.
            edge_size: Dimension of the edge labels.
        """
        super().__init__(edge_type, edge_size)


class FixVarConfig(LabelConfig):
    """Defines the signature of a fixpoint variable.

    Attributes:
        signature: The signature of this fixpoint variable.
    """

    def __init__(self, var_type: tf.DType, var_size: int):
        """Initializes the instance with the given type and dimension.

        Args:
            var_type: Type of the variable labels.
            var_size: Dimension of the variable labels.
        """
        super().__init__(var_type, var_size)
        self.signature = tf.keras.Input(shape=self.size, dtype=self.type, name=backend.unique_object_name('var', zero_based=True))


class CompilerConfig:
    """Defines the configuration for the mG compiler.

    It is recommended to use the static constructor methods to instantiate this class.
    """

    def __init__(self, node_config: NodeConfig, edge_config: EdgeConfig | None, matrix_type: tf.DType, tolerance: dict[str, float], multiple_loader: bool):
        """Initializes the instance with the initial signature of the node labels and, if present, edge labels, the signature of the adjacency matrix, the
        tolerance values by data type and which loader will be used for the graphs.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            multiple_loader: True if the models generated by the mG compiler will receive their inputs by a ``MultipleGraphLoader``.
        """
        self._node_config = node_config
        self._edge_config = edge_config
        self._matrix_type = matrix_type
        self._tolerance = tolerance
        self._multiple_loader = multiple_loader

    @staticmethod
    def xa_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels but no edge labels in the graph
         and the use of the ``SingleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, None, matrix_type, tolerance, False)

    @staticmethod
    def xai_config(node_config: NodeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect nodes labels but no edge labels in the graph
         and the use of the ``MultipleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, None, matrix_type, tolerance, True)

    @staticmethod
    def xae_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph
         and the use of the ``SingleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, edge_config, matrix_type, tolerance, False)

    @staticmethod
    def xaei_config(node_config: NodeConfig, edge_config: EdgeConfig, matrix_type: tf.DType, tolerance: dict[str, float]) -> CompilerConfig:
        """Returns a ``CompilationConfig`` object that tells the mG compiler to expect node labels and edge labels in the graph
         and the use of the ``MultipleGraphLoader`` for the inputs to the model.

        Args:
            node_config: The signature of the initial node labels of the graphs.
            edge_config: The signature of the initial edge labels of the graphs.
            matrix_type: The signature of the adjacency matrix of the graphs.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
        """
        return CompilerConfig(node_config, edge_config, matrix_type, tolerance, True)

    # Aliases
    single_graph_no_edges_config = xa_config
    single_graph_with_edges_config = xae_config
    multiple_graphs_no_edges_config = xai_config
    multiple_graphs_with_edges_config = xaei_config

    @property
    def node_feature_type(self) -> tf.DType:
        """Returns the type of the node labels.
        """
        return self._node_config.type

    @property
    def node_feature_size(self) -> int:
        """Returns the dimension of the node labels.
        """
        return self._node_config.size

    @property
    def edge_feature_type(self) -> tf.DType | None:
        """Returns the type of the edge labels, if present.
        """
        return self._edge_config.type if self.use_edges and self._edge_config is not None else None

    @property
    def edge_feature_size(self) -> int | None:
        """Returns the dimension of the edge labels, if present.
        """
        return self._edge_config.size if self.use_edges and self._edge_config is not None else None

    @property
    def matrix_type(self) -> tf.DType:
        """Returns the type of the adjacency matrix.
        """
        return self._matrix_type

    @property
    def use_edges(self) -> bool:
        """Returns whether the mG compiler expects edge labels in the graph.
        """
        return self._edge_config is not None

    @property
    def tolerance(self) -> dict[str, float]:
        """Returns the mapping between types and tolerance values.
        """
        return self._tolerance

    @property
    def use_multiple_loader(self) -> bool:
        """Returns whether the mG compiler expects the usage of the ``MultipleGraphLoader``.
        """
        return self._multiple_loader

    @property
    def input_spec(self) -> tuple[tf.TensorSpec, ...]:
        """Returns the input signature that the mG compiler expects for every model it will produce.
        """
        specs = [tf.TensorSpec(shape=(None, self.node_feature_size), dtype=self.node_feature_type),
                 tf.SparseTensorSpec(shape=(None, None), dtype=self.matrix_type)]
        if self.use_edges:
            specs.append(tf.TensorSpec(shape=(None, self.edge_feature_size), dtype=self.edge_feature_type))
        if self.use_multiple_loader:
            specs.append(tf.TensorSpec(shape=(None,), dtype=tf.int64))
        return tuple(specs)


class Context:
    """Defines the context in which a mG expression is evaluated.

    Keeps track of the position in the sequential composition of expressions where the current expression is being evaluated.

    Attributes:
        context: The sequential composition expression that has been evaluated until this point.
        len: The number of sub-expressions in the sequential composition.
    """

    def __init__(self):
        """Initializes the instance.

        The context is initially empty with length 0.
        """
        self.context: Tree | None = None
        self.len = 0

    def clear(self):
        """Clears the context.

        Returns: Nothing.
        """
        self.context = None
        self.len = 0

    def get(self, expr: Tree) -> Tree:
        """Returns the contextualized expression.

        The expression is returned as-is if the context is empty, otherwise it is returned as the last expression of its sequential composition with the
        context.

        Args:
            expr: The mG expression to contextualize.
        """
        if self.context is None:
            return expr
        else:
            seq = mg_parser.parse('left ; right')
            seq.children[0] = self.context
            seq.children[1] = expr
            return seq

    def push(self, expr: Tree):
        """Adds a mG expression to the context.

        Args:
            expr: The mG expression to add to the context.

        Returns: Nothing.
        """
        if self.context is None:
            self.context = expr
            self.len = 1
        else:
            seq = mg_parser.parse('left ; right')
            seq.children[0] = self.context
            seq.children[1] = expr
            self.context = seq
            self.len += 1

    def pop(self):
        """Removes the most recent element in the context without returning it.

        Raises:
            IndexError: The context is empty.

        Returns: Nothing.
        """
        if self.len > 1 and self.context is not None:
            self.context = self.context.children[0]
            self.len -= 1
        elif self.len == 1:
            self.context = None
            self.len = 0
        else:
            raise IndexError("Attempting to pop an empty context!")


class DummyDataset(Dataset):
    """A dataset to be used for tracing the model.

    This dataset contains a single small graph with dummy data. It is meant to be run on a model by the compiler to trace the TensorFlow computation graph,
    before the model is used to handle real data.

    Attributes:
        num_node_features: Dimensionality of each node label.
        type_node_features: Type of each node label.
        type_adj_matrix: Type of the adjacency matrix.
        num_edge_features: Dimensionality of each edge label, if present.
        type_edge_features: Type of each edge label, if present.
    """

    def __init__(self, node_config: NodeConfig, type_adj_matrix: tf.DType, edge_config: EdgeConfig | None):
        """Initializes the instance with the dimension and type of node features, the type of the adjacency matrix, and, if present, the dimension and type
        of the edge features.

        Args:
            node_config: Type and dimension of each node label.
            type_adj_matrix: Type of the adjacency matrix.
            edge_config: Type and dimension of each edge label, if present.
        """
        self.num_node_features = node_config.size
        self.type_node_features = node_config.type
        self.type_adj_matrix = type_adj_matrix
        self.num_edge_features = edge_config.size if edge_config is not None else None
        self.type_edge_features = edge_config.type if edge_config is not None else None
        super().__init__('Dummy Dataset')

    def download(self):
        pass

    def read(self) -> list[Graph]:
        dummy_node_value = [0] + [0] * (self.num_node_features - 1)
        x = np.array([dummy_node_value, dummy_node_value], dtype=self.type_node_features.as_numpy_dtype)
        a = coo_matrix(([1, 1, 1, 1], ([0, 0, 1, 1], [0, 1, 0, 1])), shape=(2, 2),
                       dtype=self.type_adj_matrix.as_numpy_dtype)
        if self.num_edge_features is not None and self.type_edge_features is not None:
            dummy_edge_value = [0] + [0] * (self.num_edge_features - 1)
            e = np.array([dummy_edge_value, dummy_edge_value, dummy_edge_value, dummy_edge_value],
                         dtype=self.type_edge_features.as_numpy_dtype)
        else:
            e = None
        return [Graph(x=x, a=a, e=e)]


class IntermediateOutput:
    """Defines an intermediate output during the compilation of a model.

    Attributes:
        name: The mG expression corresponding to this output.
        x: The node labels.
        a: The adjacency matrix.
        e: The edge labels, if present.
        i: The index tensor provided by the ``MultipleGraphLoader``.
        memoize: Whether the node labels ``x`` are memoizable.
    """

    def __init__(self, name: Tree, x: tf.Tensor, a: tf.SparseTensor, e: tf.Tensor | None, i: tf.Tensor | None, memoize: bool = True):
        """Initializes the instance with a name, the intermediate outputs and whether the node labels are to be memoized.

        Args:
            name: The mG expression corresponding to this output.
            x: The node labels.
            a: The adjacency matrix.
            e: The edge labels, if present.
            i: The index tensor provided by the ``MultipleGraphLoader``.
            memoize: Whether the node labels ``x`` are memoizable.
        """
        self.name = name
        self.x = x
        self.a = a
        self.e = e
        self.i = i
        self.memoize = memoize

    @property
    def full_inputs(self) -> list[tf.Tensor]:
        """Returns all the data tensors that are not ``None``, in the order X, A, E, I.
        """
        output = [self.x, self.a]
        if self.e is not None:
            output.append(self.e)
        if self.i is not None:
            output.append(self.i)
        return output

    @property
    def psi_inputs(self) -> list[tf.Tensor]:
        """Returns the node labels X and, if present, the index tensor I.

        Used by ``FunctionApplication`` layers.
        """
        output = [self.x]
        if self.i is not None:
            output.append(self.i)
        return output

    @property
    def img_inputs(self) -> list[tf.Tensor]:
        """Returns the node labels X, the adjacency matrix A, and, if present, the edge labels E.

        Used by ``PreImage`` and ``PostImage`` layers.
        """
        output = [self.x, self.a]
        if self.e is not None:
            output.append(self.e)
        return output

    @property
    def fixpoint_inputs(self) -> list[tf.Tensor]:
        """Returns the node the adjacency matrix A, and, if present, the edge labels E and index tensor I.

        Used by ``FixPoint`` and ``Repeat`` layers.
        """
        output = [self.a]
        if self.e is not None:
            output.append(self.e)
        if self.i is not None:
            output.append(self.i)
        return output

    def as_new_inputs(self) -> IntermediateOutput:
        """Returns a copy of this object where the node labels X have been substituted with a new ``tf.keras.layers.Input`` layer with the same signature as X.
        """
        sym_x = tf.keras.Input(type_spec=self.x.type_spec)
        return IntermediateOutput(self.name, sym_x, self.a, self.e, self.i, self.memoize)

    def step(self, name: Tree, x: tf.Tensor, free_vars: bidict, memoize: bool | None = None) -> IntermediateOutput:
        """
        Returns a new ``IntermediateOutput`` object created from this object.

        The adjacency matrix, edge features and indexes are kept as is, as only the node features are allowed to change in mG. If this object represented a free
        variable in a mG expression, the hash of the node features X is updated with the new hash.

        Args:
            name: The mG expression corresponding to this output.
            x: The new node labels that will replace those of this instance.
            free_vars: The free variables that have been encountered so far.
            memoize: Whether the node labels ``x`` are memoizable. If ``None``, this property is inherited from the instance.
        """
        if self.x.ref() in free_vars:
            free_vars[x.ref()] = free_vars.pop(self.x.ref())
        if memoize is None:
            memoize = self.memoize
        return IntermediateOutput(name, x, self.a, self.e, self.i, memoize)


class FixPointExpression:
    """Defines the model of a mG fixpoint expression.

    Attributes:
        name: The mG fixpoint expression.
        input_signature: The inputs to the model that runs the fixpoint expression.
        output_signature: The node labels generated by the model that runs the fixpoint expression.
        model: The model that implements the fixpoint expression.
        args: Extra pre-computed arguments that are given in input to the model.
    """

    def __init__(self, name: Tree, inputs: list[tf.Tensor], output: tf.Tensor):
        """Initializes the instance with the given name, list of inputs and the output to be computed from the inputs.

        Args:
            name: The mG fixpoint expression.
            inputs: The inputs to the model that runs the fixpoint expression.
            output: The node labels generated by the model that runs the fixpoint expression.
        """
        self.name = name
        self.input_signature = inputs
        self.output_signature = output
        self.model = tf.keras.Model(inputs=self.input_signature, outputs=self.output_signature)
        self.args: list[tf.Tensor] = []  # initially no arguments


class MGFunction:
    """Container for a mG function defined by a ``def`` expression or a mG variable defined by a ``let`` expression.

    Attributes:
        name: Name of the function or variable.
        var_list: List of arguments of the function. Empty list in the case of variables.
        body_tree: The expression that is the body of the function or the value of the variable.
    """

    def __init__(self, name: str, var_list: list[str], body_tree: Tree):
        """Initializes the instance with the given name, list of arguments, and body of the expression.

        Args:
            name: Name of the function or variable.
            var_list: List of formal arguments (variable names) of the function. Empty list in the case of variables.
            body_tree: The expression that is the body of the function or the value of the variable.
        """
        self.name = name
        self.var_list = var_list  # list of names
        self.body_tree = body_tree

    # type checking could be done here someday
    def get_args(self, arguments: list[IntermediateOutput]) -> dict[str, IntermediateOutput]:
        """Returns the function argument bindings.

        Args:
            arguments: The list of values to bind.
        """
        ordered_args = self.var_list
        matched_args = {}
        for i in range(len(arguments)):
            matched_args[ordered_args[i]] = arguments[i]
        return matched_args

    @staticmethod
    def as_operator(function_name: str, n_arguments: int, is_operator: bool) -> MGFunction:
        """Returns an instance of ``MGFunction`` that applies a psi function named ``function_name`` to the parallel composition of ``n_arguments`` arguments.

        If the psi function is an operator, the number of arguments is given in square brackets.

        Args:
            function_name: The name of the psi function to apply.
            n_arguments: The number of arguments to which the psi function will be applied.
            is_operator: Whether the psi function is an operator.
        """
        name = 'op_k_' + function_name
        var_list = ['__X' + str(i) for i in range(n_arguments)]
        function_name = function_name + '[' + str(n_arguments) + ']' if is_operator else function_name
        body_tree = mg_parser.parse('(' + ' || '.join(var_list) + ') ; ' + function_name)
        return MGFunction(name, var_list, body_tree)


class MGModel:
    """Defines a mG model.

    Wraps a ``tf.keras.Model`` by adding some additional attributes and routing all calls to the wrapped ``tf.keras.Model`` instance.

    Attributes:
        expr: The mG expression that this model implements.
        mg_layers: A mapping of mG expression hashes to layers of the model. Contains all layers of the model.
        config: The configuration of the compiler that created this model.
        psi_functions: The psi functions that have been used in the model.
        phi_functions: The phi functions that have been used in the model.
        sigma_functions: The sigma functions that have been used in the model.
    """

    def __init__(self, inputs: list[tf.Tensor], outputs: tf.Tensor, expr: Tree, layers: dict[int, tf.keras.layers.Layer],
                 config: CompilerConfig, psi_functions: dict[str, PsiNonLocal], phi_functions: dict[str, Phi], sigma_functions: dict[str, Sigma]):
        """Initializes the instance with the inputs and outputs of the model, the expression that the model implements, the layers of the model indexed by
        expression, and the functions that have been used in it.

        Args:
            inputs: The inputs of the model.
            outputs: The outputs of the model.
            expr: The mG expression that this model implements.
            layers: A mapping of mG expression hashes to layers of the model. Contains all layers of the model.
            config: The configuration of the compiler that created this model.
            psi_functions: The psi functions that have been used in the model.
            phi_functions: The phi functions that have been used in the model.
            sigma_functions: The sigma functions that have been used in the model.
        """
        self._model = tf.keras.Model(inputs=inputs, outputs=outputs)
        self.expr = expr
        self.mg_layers = layers
        self.config = config
        self.psi_functions = psi_functions
        self.phi_functions = phi_functions
        self.sigma_functions = sigma_functions

    def __getattr__(self, item: str) -> Any:
        return getattr(self._model, item)

    def __setattr__(self, key, value):
        if key in {'_model', 'expr', 'mg_layers', 'config', 'psi_functions', 'phi_functions', 'sigma_functions'}:
            object.__setattr__(self, key, value)
        else:
            object.__setattr__(self._model, key, value)

    def __call__(self, *args, **kwargs):
        return self._model(*args, **kwargs)

    def pretty_print_expr(self):
        return mg_reconstructor.reconstruct(self.expr)


class MGCompiler:
    """The compiler for mG programs.

     A program is transformed into a TensorFlow model using the ``compile`` method.

     Attributes:
        config: The configuration for this compiler instance.
        model_inputs: The input layers for the models generated by the compiler.
        model_input_spec: The input signature for the models generated by the compiler.
        dummy_dataset: The dataset used to trace the models generated by the compiler.
        visitor: The visitor that traverses an expression tree to construct the model.
    """

    class _TreeToTF(Interpreter):
        """Visitor that walks a mG expression tree and builds the corresponding model.

        Attributes:
            psi_functions: The psi functions that can occur in an expression.
            sigma_functions: The sigma functions that can occur in an expression.
            phi_functions: The phi functions that can occur in an expression.
            tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            use_memoization: If false, memoization is disabled globally.
            initial_inputs: The initial inputs of the model, consisting of ``tf.keras.layers.Input`` objects.
            inputs: The current intermediate outputs, which act as inputs for the next expression to be evaluated.
            fix_var: The fixpoint variable bindings.
            free_fix_var: The bidirectional mapping of free fixpoint variable bindings.
            context: The context in which the current expression is being evaluated.
            intermediate_outputs: The mapping of expression hashes to the corresponding memoized intermediate outputs.
            layers: The mapping of expression hashes to the layer responsible for computing their output.
            defined_functions: The mapping of def'ed function names to the expression tree of their body.
            defined_local_variables: The mapping of let'ed variable names to the expression tree of their value.
            var_input: The mapping of variable bindings for def'ed functions.
            used_psi: The psi functions that have actually occurred.
            used_phi: The phi functions that have actually occurred.
            used_sigma: The sigma functions that have actually occurred.
            eval_if_clause: A stack of fixpoint variable names, which is used to keep track of if-clauses inside fixpoint expressions.
        """
        supported_types = {'bool': tf.bool, 'int': tf.int32, 'float': tf.float32, 'uint8': tf.uint8, 'uint16': tf.uint16,
                           'uint32': tf.uint32, 'uint64': tf.uint64, 'int8': tf.int8, 'int16': tf.int16, 'int32': tf.int32,
                           'int64': tf.int64, 'float16': tf.float16, 'float32': tf.float32, 'float64': tf.float64,
                           'half': tf.float16, 'double': tf.float64}

        composite_name_generator = {
            'sequential_composition': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'parallel_composition': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'ite': lambda t, c: Tree(data=t.data, meta=t.meta, children=[child.name for child in c]),
            'fix': lambda t, c: Tree(data=t.data, meta=t.meta, children=c[:1] + [child.name for child in c[1:]]),
            'repeat': lambda t, c: Tree(data=t.data, meta=t.meta, children=c[:1] + [child.name for child in c[1:-1]] + c[-1:])
        }

        def __init__(self, psi_functions: FunctionDict, sigma_functions: FunctionDict, phi_functions: FunctionDict, tolerance: dict[str, float]):
            """Initializes the instance with the psi, phi, and sigma functions that can occur in the mG expressions and the tolerance values for the fixpoint
            computations.

            Args:
                psi_functions: The psi functions that can occur in an expression.
                sigma_functions: The sigma functions that can occur in an expression.
                phi_functions: The phi functions that can occur in an expression.
                tolerance: The tolerance values for the data types (typically floats) that require an approximate fixed point solution.
            """
            super().__init__()
            self.psi_functions = psi_functions
            self.sigma_functions = sigma_functions
            self.phi_functions = phi_functions
            self.tolerance = tolerance
            # Initialization
            self.use_memoization: bool
            self.initial_inputs: IntermediateOutput
            self.inputs: IntermediateOutput
            self.fix_var: dict[str, FixVarConfig] = {}
            self.free_fix_var: bidict[Reference, str] = bidict({})
            self.context: Context = Context()
            self.intermediate_outputs: dict[int, IntermediateOutput] = {}
            self.layers: dict[int, tf.keras.layers.Layer] = {}
            self.defined_functions: dict[str, MGFunction] = {}
            self.defined_local_variables: dict[str, MGFunction] = {}
            self.var_input: dict[str, IntermediateOutput] = {}
            self.used_psi: dict[str, PsiNonLocal] = {}
            self.used_phi: dict[str, Phi] = {}
            self.used_sigma: dict[str, Sigma] = {}
            self.eval_if_clause: list[str] = []

        def initialize(self, initial_inputs: IntermediateOutput, use_memoization: bool) -> None:
            """Initializes and/or resets the instance. Sets the initial inputs for the next expression to compile.

            Args:
                initial_inputs: The initial inputs of the model for the next expression.
                use_memoization: If false, disables memoization globally.

            Returns:
                Nothing.
            """
            self.use_memoization = use_memoization
            self.initial_inputs = initial_inputs
            self.inputs = self.initial_inputs
            self.fix_var = {}
            self.free_fix_var = bidict({})
            self.context.clear()
            self.intermediate_outputs = {}
            self.layers = {}
            self.defined_functions = {}
            self.defined_local_variables = {}
            self.var_input = {}
            self.used_psi = {}
            self.used_phi = {}
            self.used_sigma = {}
            self.eval_if_clause = []

        def clone(self, start_from: IntermediateOutput) -> MGCompiler._TreeToTF:
            """Returns a fresh copy of this visitor, except that:
                - The initial inputs are those specified in ``start_from``.
                - The defined functions and variables are carried over.
                - The variable bindings are carried over.
                - The fixpoint variable bindings are carried over.
                - The free fixpoint variable bindings are carried over.

            Args:
                start_from: The initial inputs for the new visitor instance.
            """
            clone = type(self)(self.psi_functions, self.sigma_functions, self.phi_functions, self.tolerance)
            clone.initialize(start_from, self.use_memoization)
            clone.defined_functions = self.defined_functions.copy()
            clone.defined_local_variables = self.defined_local_variables.copy()
            clone.var_input = self.var_input.copy()
            clone.fix_var = self.fix_var.copy()
            clone.free_fix_var = self.free_fix_var.copy()
            return clone

        def add_layer(self, intermediate_output: IntermediateOutput, op_layer: tf.keras.layers.Layer, ctx_name: Tree) -> None:
            """Saves an intermediate output under the hash key of the corresponding contextualized expression tree.

            It is saved only if the output is set as memoizable.

            Args:
                intermediate_output: The value to memoize.
                op_layer: The layer that generated the value.
                ctx_name: The expression tree corresponding to the value.

            Returns:
                Nothing.
            """
            hsh = hash(ctx_name)
            if intermediate_output.memoize:
                if self.use_memoization:
                    self.intermediate_outputs[hsh] = intermediate_output
                self.layers[hsh] = op_layer

        def get_layer(self, ctx_name: Tree) -> IntermediateOutput:
            """Returns a saved output.

            Args:
                ctx_name: The saved output to retrieve, identified by its contextualized expression tree.

            Raises:
                KeyError: The requested output has not been saved.
            """
            hsh = hash(ctx_name)
            if hsh in self.intermediate_outputs:
                return self.intermediate_outputs[hsh]
            else:
                raise KeyError("No layer with name:", str(ctx_name))

        def undef_layer(self, ctx_name: Tree) -> bool:
            """Returns whether an output has not been saved.

            Args:
                ctx_name: The contextualized name of the output.
            """
            if not self.use_memoization:
                return True
            else:
                hsh = hash(ctx_name)
                return not (hsh in self.intermediate_outputs)

        @staticmethod
        def get_composite_name(tree: Tree, interpreted_children: list[IntermediateOutput | FixPointExpression | Tree | int]) -> Tree:
            """Returns the expression tree of a composite mG expression by substituting variables with their corresponding expression tree.

            The sub-expressions of a composite expression may contain variables (from def or let expressions) which should replace the variable symbols once
            evaluated.

            Args:
                tree: The expression tree of the composite mG expression.
                interpreted_children: The sub-expressions of ``tree``.
            """
            return MGCompiler._TreeToTF.composite_name_generator[tree.data](tree, interpreted_children)

        def get_tolerance(self, _type: str) -> float | None:
            """Returns the tolerance for the given type, if present.

            Args:
                _type: The type for which to obtain the tolerance value.
            """
            if _type == 'float32':
                return self.tolerance.get(_type, self.tolerance.get('float', None))
            elif _type == 'int32':
                return self.tolerance.get(_type, self.tolerance.get('int', None))
            elif _type == 'float16':
                return self.tolerance.get(_type, self.tolerance.get('half', None))
            elif _type == 'float64':
                return self.tolerance.get(_type, self.tolerance.get('double', None))
            else:
                return self.tolerance.get(_type, None)

        def current_fix_var(self) -> str:
            """Returns the most recent fixpoint variable name (that of the closest enclosing fixpoint expression).
            """
            return next(reversed(self.fix_var))

        def current_fix_var_config(self):
            """Returns the most recent fixpoint variable config (that of the closest enclosing fixpoint expression).
            """
            return self.fix_var[next(reversed(self.fix_var))]

        def is_evaluating_if(self) -> bool:
            """Returns whether an if-then-else clause that contains fixpoint variables is being evaluated.
            """
            return len(self.eval_if_clause) > 0 and self.current_fix_var() == self.eval_if_clause[-1]

        def start_eval_if(self) -> None:
            """Marks the start of the evaluation of an if-then-else clause that contains fixpoint variables.

            Returns:
                Nothing.
            """
            self.eval_if_clause.append(self.current_fix_var())

        def stop_eval_if(self) -> None:
            """Marks the end of the evaluation of an if-then-else clause that contains fixpoint variables.

            Returns:
                Nothing.
            """
            self.eval_if_clause.pop()

        @v_args(inline=True)
        def label(self, label: Token) -> str:
            """Evaluates a label.

            Args:
                label: The label to evaluate.

            Returns:
                The label.
            """
            return str(label)

        @v_args(inline=True)
        def label_decl(self, label_decl: Token) -> str:
            """Evaluates a label declaration.

            Args:
                label_decl: The label declaration to evaluate.

            Returns:
                The label declaration.
            """
            return str(label_decl)

        def atom_op(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates a psi function or a variable.

            This can be, in order of priority:
                - A fixpoint variable
                - A free fixpoint variable
                - A bound variable of a def'ed function
                - A bound variable of a let expression
                - A psi function

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: This expression doesn't correspond to any variable or psi function.
            """
            children = self.visit_children(tree)
            label = children[0]
            if len(self.fix_var) > 0 and label == self.current_fix_var() and not self.is_evaluating_if():
                # we are inside a fixpoint op and the label matches the fixpoint var
                var_signature = self.current_fix_var_config().signature
                return FixPointExpression(tree, inputs=[var_signature] + self.inputs.full_inputs[1:], output=var_signature)
            elif len(self.fix_var) > 0 and label == self.current_fix_var() and self.is_evaluating_if():
                return self.inputs.step(tree, self.current_fix_var_config().signature, self.free_fix_var)
            elif label in self.fix_var:  # the label is a fix_var, but not the current one
                output = self.inputs.step(tree, self.fix_var[label].signature, self.free_fix_var, memoize=False)
                self.free_fix_var[output.x.ref()] = label
                return output
            elif label in self.var_input:  # we are inside a defined function being called
                if isinstance(self.var_input[label], FixPointExpression):
                    return self.var_input[label]
                else:
                    return self.inputs.step(self.var_input[label].name, self.var_input[label].x, self.free_fix_var)
            elif label in self.defined_local_variables:  # the label matches a local variable
                deferred_function = self.defined_local_variables[label]
                output = self.visit(deferred_function.body_tree)
                return output
            elif label in self.psi_functions:  # the label matches a psi function
                op_layer = FunctionApplication(self.psi_functions[label])
                ctx_name = self.context.get(tree)
                self.used_psi[label] = self.psi_functions[label]
            else:
                raise SyntaxError('Undeclared variable or function: ' + label)
            # execution continues here only in the last case of the if-elif-else
            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, op_layer(self.inputs.psi_inputs), self.free_fix_var)
                self.add_layer(output, op_layer, ctx_name)
                return output
            else:
                return self.get_layer(ctx_name)

        def lhd(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a pre-image expression.

            Args:
                tree: The expression tree.

            Returns:
                 The output of this expression.
            """
            ctx_name = self.context.get(tree)
            args = self.visit_children(tree)
            if len(args) == 2:
                edge_function, agg_function = args
                lhd_layer = PreImage(self.sigma_functions[agg_function], self.phi_functions[edge_function])
                self.used_sigma[agg_function] = self.sigma_functions[agg_function]
                self.used_phi[edge_function] = self.phi_functions[edge_function]
            else:
                agg_function, = args
                lhd_layer = PreImage(self.sigma_functions[agg_function])
                self.used_sigma[agg_function] = self.sigma_functions[agg_function]

            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, lhd_layer(self.inputs.img_inputs), self.free_fix_var)
                self.add_layer(output, lhd_layer, ctx_name)
                return output
            return self.get_layer(ctx_name)

        def rhd(self, tree: Tree) -> IntermediateOutput:
            """Evaluates a post-image expression.

            Args:
                tree: The expression tree.

            Returns:
                 The output of this expression.
            """
            ctx_name = self.context.get(tree)
            args = self.visit_children(tree)
            if len(args) == 2:
                edge_function, agg_function = args
                rhd_layer = PostImage(self.sigma_functions[agg_function], self.phi_functions[edge_function])
                self.used_sigma[agg_function] = self.sigma_functions[agg_function]
                self.used_phi[edge_function] = self.phi_functions[edge_function]
            else:
                agg_function, = args
                rhd_layer = PostImage(self.sigma_functions[agg_function])
                self.used_sigma[agg_function] = self.sigma_functions[agg_function]

            if self.undef_layer(ctx_name):
                # noinspection PyCallingNonCallable
                output = self.inputs.step(tree, rhd_layer(self.inputs.img_inputs), self.free_fix_var)
                self.add_layer(output, rhd_layer, ctx_name)
                return output
            return self.get_layer(ctx_name)

        def sequential_composition(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates a sequential composition expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            left, right = tree.children
            current_inputs = self.inputs
            phi = self.visit(left)
            self.context.push(phi.name)
            if isinstance(phi, FixPointExpression):
                self.inputs = current_inputs.step(phi.name, phi.output_signature, self.free_fix_var, memoize=False)
                psi = self.visit(right)
                self.context.pop()

                new_expr = FixPointExpression(self.get_composite_name(tree, [phi, psi]), phi.input_signature, psi.x)
                new_expr.args = phi.args
                self.inputs = current_inputs
                return new_expr
            else:
                self.inputs = phi
                psi = self.visit(right)
                self.context.pop()
                self.inputs = current_inputs
                return self.inputs.step(self.get_composite_name(tree, [phi, psi]), psi.x, self.free_fix_var)

        @staticmethod
        def make_fixpoint_expr_par(outputs: list[IntermediateOutput | FixPointExpression], name: Tree) -> FixPointExpression:
            """Returns the fixpoint expression for the case when the fixpoint variable occurs in a parallel composition expression.

            Args:
                outputs: The outputs or fixpoint expressions to compose in parallel in a new fixpoint expression.
                name: The name for the fixpoint expression.
            """
            op_layer = tf.keras.layers.Concatenate()
            new_model_inputs = []
            to_concatenate = []
            new_args = []
            fixpoints_input_signatures: dict[tf.python.util.object_identity.Reference, None] = {}
            fixpoints_saved_args: dict[tf.python.util.object_identity.Reference, None] = {}
            for layer in outputs:
                if isinstance(layer, FixPointExpression):
                    to_concatenate.append(layer.output_signature)
                    for input_signature in layer.input_signature:
                        fixpoints_input_signatures.pop(input_signature.ref(), None)
                        fixpoints_input_signatures[input_signature.ref()] = None
                    for saved_arg in layer.args:
                        fixpoints_saved_args.pop(saved_arg.ref(), None)
                        fixpoints_saved_args[saved_arg.ref()] = None
                else:
                    symbolic_layer = tf.keras.Input(type_spec=layer.x.type_spec)
                    new_model_inputs.append(symbolic_layer)
                    to_concatenate.append(symbolic_layer)
                    new_args.append(layer.x)

            old_expr_input_signatures = [input_sig.deref() for input_sig in fixpoints_input_signatures.keys()]
            old_expr_args = [saved_arg.deref() for saved_arg in fixpoints_saved_args.keys()]
            new_expr = FixPointExpression(name, inputs=new_model_inputs + old_expr_input_signatures,
                                          output=op_layer(to_concatenate))
            new_expr.args = new_args + old_expr_args
            return new_expr

        def parallel_composition(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates a parallel composition expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            children = self.visit_children(tree)
            name = self.get_composite_name(tree, children)
            ctx_name = self.context.get(name)
            has_var = False
            for layer in children:
                if type(layer) is FixPointExpression:
                    has_var = True
                    break
            if has_var:
                return self.make_fixpoint_expr_par(children, name)
            else:
                op_layer = tf.keras.layers.Concatenate()
                if self.undef_layer(ctx_name):
                    output = self.inputs.step(name, op_layer([arg.x for arg in children]), self.free_fix_var)
                    self.add_layer(output, op_layer, ctx_name)
                    return output
                return self.get_layer(ctx_name)

        def fun_def(self, tree: Tree) -> Any:
            """Evaluates a def expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            args = tree.children
            function_name = self.visit(args[0])
            var_input = []
            for i in range(len(args[1:-2])):
                var_name = self.visit(args[1 + i])
                var_input.append(var_name)
            function_tree = args[-2]  # we are not evaluating the function right now
            deferred_function = MGFunction(function_name, var_input, function_tree)
            self.defined_functions[function_name] = deferred_function
            return self.visit(args[-1])

        def fun_call(self, tree: Tree) -> Any:
            """Evaluates a function call expression.

            If the function name doesn't match a def'ed function, but matches a psi function, it is evaluated as a polish notation operator.
            E.g.: psi(a, b, c) is evaluated as the mG expression def op_k_psi(__X1, __X2, __X3){(__X1 || __X2 || __X3) ; psi} in op_k_psi(a, b, c)

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            args = tree.children
            function_name = self.visit(args[0])
            arguments = [self.visit(arg) for arg in args[1:]]

            deferred_function = self.defined_functions.get(function_name)
            if deferred_function is None:
                if function_name in self.psi_functions:
                    deferred_function = MGFunction.as_operator(function_name, len(arguments), self.psi_functions.is_operator(function_name))
                else:
                    raise KeyError("Function " + function_name + " doesn't exist.")
            matched_args = deferred_function.get_args(arguments)  # match args
            self.var_input |= matched_args  # add the deferred function vars to var input
            f_layer = self.visit(deferred_function.body_tree)  # now visit the function body
            for k in matched_args:  # eliminate the variables of this function from var_input
                self.var_input.pop(k)
            return f_layer

        def local_var_expr(self, tree: Tree) -> Any:
            """Evaluates a let expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            args = tree.children
            local_vars = []
            for i in range(len(args[0:-1]) // 2):
                var_name = self.visit(args[i * 2])
                function_tree = args[i * 2 + 1]
                deferred_function = MGFunction(var_name, [], function_tree)
                local_vars.append(var_name)
                self.defined_local_variables[var_name] = deferred_function
            expr = self.visit(tree.children[-1])
            for k in local_vars:  # eliminate the variables defined by this expression
                self.defined_local_variables.pop(k)
            return expr

        def ite(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates an if-then-else expression.

            Args:
                tree: The expression tree.

            Returns:
                The output of this expression or a fixpoint expression.
            """
            test, iftrue, iffalse = tree.children
            test = self.visit(test)
            # where do we have fixpoint variables?
            if len(self.fix_var) > 0:
                fixpoint_idx = [isinstance(test, FixPointExpression), var_occurs(iftrue, self.current_fix_var()), var_occurs(iffalse, self.current_fix_var())]
            else:
                fixpoint_idx = [False, False, False]

            if fixpoint_idx[1] is True:
                inputs = self.inputs.as_new_inputs()
                clone = self.clone(inputs)  # Parse the clause using a new visitor.
                clone.start_eval_if()
                iftrue = clone.visit(iftrue)
                clone.stop_eval_if()
                iftrue_model = tf.keras.Model(inputs=inputs.full_inputs[:1] + [self.current_fix_var_config().signature] + inputs.full_inputs[1:],
                                              outputs=iftrue.x)
            else:
                inputs = self.inputs.as_new_inputs()
                clone = self.clone(inputs)
                iftrue = clone.visit(iftrue)
                iftrue_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iftrue.x)
            if fixpoint_idx[2] is True:
                inputs = self.inputs.as_new_inputs()
                clone = self.clone(inputs)  # Parse the clause using a new visitor.
                clone.start_eval_if()
                iffalse = clone.visit(iffalse)
                clone.stop_eval_if()
                iffalse_model = tf.keras.Model(inputs=inputs.full_inputs[:1] + [self.current_fix_var_config().signature] + inputs.full_inputs[1:],
                                               outputs=iffalse.x)
            else:
                inputs = self.inputs.as_new_inputs()
                clone = self.clone(inputs)
                iffalse = clone.visit(iffalse)
                iffalse_model = tf.keras.Model(inputs=inputs.full_inputs, outputs=iffalse.x)

            name = self.get_composite_name(tree, [test, iftrue, iffalse])
            # Fixpoint variables both in condition and one or both clauses
            if (fixpoint_idx[0] and fixpoint_idx[1]) or (fixpoint_idx[0] and fixpoint_idx[2]) or all(fixpoint_idx):
                ite_layer = Ite(iftrue_model, iffalse_model)
                if self.is_evaluating_if():
                    # noinspection PyCallingNonCallable
                    return self.inputs.step(name, ite_layer([test.output_signature] + self.inputs.full_inputs[:1] + [self.current_fix_var_config().signature]
                                                            + self.inputs.full_inputs[1:]), self.free_fix_var)
                else:
                    # noinspection PyCallingNonCallable
                    new_expr = FixPointExpression(name, inputs=self.inputs.full_inputs[:1] + test.input_signature,
                                                  output=ite_layer([test.output_signature] + self.inputs.full_inputs[:1] +
                                                                   [self.current_fix_var_config().signature] + self.inputs.full_inputs[1:]))
                    new_expr.args = self.inputs.full_inputs[:1] + test.args
                    return new_expr
            elif fixpoint_idx[1] or fixpoint_idx[2]:  # Fixpoint variables only in one or both clauses
                ite_layer = Ite(iftrue_model, iffalse_model)
                if self.is_evaluating_if():
                    # noinspection PyCallingNonCallable
                    return self.inputs.step(name, ite_layer([test.x] + self.inputs.full_inputs[:1] + [self.current_fix_var_config().signature]
                                                            + self.inputs.full_inputs[1:]), self.free_fix_var)
                else:
                    # noinspection PyCallingNonCallable
                    new_expr = FixPointExpression(name,
                                                  inputs=[test.x] + self.inputs.full_inputs[:1] +
                                                         [self.current_fix_var_config().signature] + self.inputs.full_inputs[1:],
                                                  output=ite_layer([test.x] + self.inputs.full_inputs[:1] + [self.current_fix_var_config().signature]
                                                                   + self.inputs.full_inputs[1:]))
                    new_expr.args = [test.x] + self.inputs.full_inputs[:1]
                    return new_expr
            elif fixpoint_idx[0]:  # Fixpoint variables only in the condition
                ite_layer = Ite(iftrue_model, iffalse_model)
                # noinspection PyCallingNonCallable
                new_expr = FixPointExpression(name, inputs=self.inputs.full_inputs[:1] + test.input_signature,
                                              output=ite_layer([test.output_signature] + self.inputs.full_inputs))
                new_expr.args = self.inputs.full_inputs[:1] + test.args
                return new_expr
            else:
                ite_layer = Ite(iftrue_model, iffalse_model)
                ctx_name = self.context.get(name)
                if self.undef_layer(ctx_name):
                    # noinspection PyCallingNonCallable
                    output = self.inputs.step(name, ite_layer([test.x] + self.inputs.full_inputs), self.free_fix_var)
                    self.add_layer(output, ite_layer, ctx_name)
                    return output
                else:
                    return self.get_layer(ctx_name)

        @staticmethod
        def analyze_tensor(t: IntermediateOutput | FixPointExpression) -> tuple[int, tf.DType]:
            """Returns the dimension and type of the node labels in ``t``.

            Args:
                t: An output or fixpoint expression that contain a node labels tensor.
            """
            if isinstance(t, FixPointExpression):
                return t.output_signature.shape[1], t.output_signature.dtype
            else:
                return t.x.shape[1], t.x.dtype

        def evaluate_loop_expr(self, tree: Tree, op: Literal['fix', 'repeat']) -> IntermediateOutput | FixPointExpression:
            """Evaluates a mG loop expression, either a fixpoint expression or a repeat expression.

            Args:
                tree: The expression tree.
                op: The operation specified by the expression.

            Returns:
                The output of the expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the fixpoint expression doesn't contain any fixpoint variable.
            """
            if op == 'fix':
                variable_decl, initial_var_gnn, body = tree.children
                iters = None
            else:
                variable_decl, initial_var_gnn, body, n = tree.children
                assert isinstance(n, Token)
                iters = int(n)
            var_name = self.visit(variable_decl)
            initial_var_gnn = self.visit(initial_var_gnn)
            initial_var_gnn_dimension, initial_gnn_var_type = self.analyze_tensor(initial_var_gnn)
            fixvar_config = FixVarConfig(initial_gnn_var_type, initial_var_gnn_dimension)
            self.fix_var[var_name] = fixvar_config
            nx = self.visit(body)
            if not isinstance(nx, FixPointExpression):
                raise SyntaxError('Invalid fixpoint expression')
            if op == 'fix':
                tolerance = self.get_tolerance(initial_gnn_var_type.name)
                fix_layer = FixPoint(nx.model, tolerance, debug=False)
                name = self.get_composite_name(tree, [variable_decl, initial_var_gnn, nx])
            else:
                assert iters is not None
                fix_layer = Repeat(nx.model, iters)
                name = self.get_composite_name(tree, [variable_decl, initial_var_gnn, nx, iters])

            ctx_name = self.context.get(name)
            self.fix_var.pop(var_name)
            self.free_fix_var.inverse.pop(var_name, None)
            if len(self.free_fix_var) == 0 and not isinstance(initial_var_gnn, FixPointExpression):
                if self.undef_layer(ctx_name):
                    # noinspection PyCallingNonCallable
                    output = self.inputs.step(name, fix_layer(nx.args + [initial_var_gnn.x] + self.inputs.fixpoint_inputs), self.free_fix_var)
                    self.add_layer(output, fix_layer, ctx_name)
                    return output
                else:
                    return self.get_layer(ctx_name)
            else:
                # take all the free vars and remove them from nx.args
                freevars = []
                outputs = []
                for i, t in enumerate(nx.args):
                    if self.free_fix_var[t.ref()] == self.current_fix_var():
                        freevars.append(self.current_fix_var_config())
                        outputs.append(t)
                        nx.args.pop(i)
                        break

                model = tf.keras.Model(inputs=[freevar.signature for freevar in freevars] + self.inputs.fixpoint_inputs, outputs=outputs)

                if isinstance(initial_var_gnn, FixPointExpression):  # Free fixpoint variables in the initialization
                    if self.is_evaluating_if():
                        # noinspection PyCallingNonCallable
                        return self.inputs.step(name, fix_layer(nx.args + initial_var_gnn.args +
                                                                [model([freevar.signature for freevar in freevars] + self.inputs.fixpoint_inputs)] +
                                                                [initial_var_gnn.output_signature] + self.inputs.fixpoint_inputs), self.free_fix_var)
                    else:
                        # noinspection PyCallingNonCallable
                        new_expr = FixPointExpression(name, nx.args + initial_var_gnn.args + [freevar.signature for freevar in freevars]
                                                      + initial_var_gnn.input_signature,
                                                      fix_layer(nx.args + initial_var_gnn.args +
                                                                [model([freevar.signature for freevar in freevars] + self.inputs.fixpoint_inputs)] +
                                                                [initial_var_gnn.output_signature] + self.inputs.fixpoint_inputs))
                        new_expr.args = nx.args + initial_var_gnn.args
                        return new_expr
                else:  # Free fixpoint variables in the body
                    if self.is_evaluating_if():
                        # noinspection PyCallingNonCallable
                        return self.inputs.step(name, fix_layer(nx.args + [model([freevar.signature for freevar in freevars] + self.inputs.fixpoint_inputs)] +
                                                                [initial_var_gnn.x] + self.inputs.fixpoint_inputs), self.free_fix_var)
                    else:
                        # noinspection PyCallingNonCallable
                        new_expr = FixPointExpression(name, nx.args + [initial_var_gnn.x] + [freevar.signature for freevar in freevars] +
                                                      self.inputs.fixpoint_inputs,
                                                      fix_layer(nx.args + [model([freevar.signature for freevar in freevars] + self.inputs.fixpoint_inputs)] +
                                                                [initial_var_gnn.x] + self.inputs.fixpoint_inputs))
                        new_expr.args = nx.args + [initial_var_gnn.x]
                        return new_expr

        def fix(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates a fixpoint expression.

            Args:
                tree: The expression tree

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the fixpoint expression doesn't contain any fixpoint variable.
            """
            return self.evaluate_loop_expr(tree, 'fix')

        def repeat(self, tree: Tree) -> IntermediateOutput | FixPointExpression:
            """Evaluates a repeat expression.

            Args:
                tree: The expression tree

            Returns:
                The output of this expression or a fixpoint expression.

            Raises:
                SyntaxError: The body of the repeat expression doesn't contain any fixpoint variable.
            """
            return self.evaluate_loop_expr(tree, 'repeat')

    def __init__(self, psi_functions: dict[str, Psi | Callable[[], Psi] | Callable[[str], Psi]],
                 sigma_functions: dict[str, Sigma | Callable[[], Sigma] | Callable[[str], Sigma]],
                 phi_functions: dict[str, Phi | Callable[[], Phi] | Callable[[str], Phi]], config: CompilerConfig):
        """Initializes the instance with the psi, phi and sigma functions that this compiler will recognize and the compiler configuration.

        Args:
            psi_functions: The psi functions that this compiler will recognize.
            sigma_functions: The sigma functions that this compiler will recognize.
            phi_functions: The phi functions that this compiler will recognize.
            config: The configuration for the compiler.
        """
        if config.node_feature_type == tf.float64 or config.edge_feature_type == tf.float64:
            tf.keras.backend.set_floatx('float64')
        elif config.node_feature_type == tf.float16 or config.edge_feature_type == tf.float16:
            tf.keras.backend.set_floatx('float16')
        self.config = config
        self.model_inputs = [tf.keras.Input(shape=config.node_feature_size, name="INPUT_X", dtype=config.node_feature_type),
                             tf.keras.Input(shape=(None,), sparse=True, name="INPUT_A", dtype=config.matrix_type)]
        if config.use_edges:
            self.model_inputs.append(tf.keras.Input(shape=config.edge_feature_size, name="INPUT_E", dtype=config.edge_feature_type))
        if config.use_multiple_loader:
            self.model_inputs.append(tf.keras.Input(shape=(), name="INPUT_I", dtype=tf.int64))
        self.model_input_spec = config.input_spec
        self.dummy_dataset = DummyDataset(NodeConfig(config.node_feature_type, config.node_feature_size), config.matrix_type,
                                          EdgeConfig(config.edge_feature_type, config.edge_feature_size) if config.use_edges else None)  # type: ignore
        self.visitor = self._TreeToTF(FunctionDict(psi_functions), FunctionDict(sigma_functions), FunctionDict(phi_functions), config.tolerance)

    @staticmethod
    def _graph_mode_constructor(model: MGModel, input_spec: tuple[tf.TensorSpec, ...],
                                method: Literal["call", "predict", "predict_on_batch"]) -> Callable[[list[tf.Tensor]], tf.Tensor] | MGModel:
        """Prepares a model for tracing.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to prepare for tracing.
            input_spec: The input signature for the model.
            method: The TensorFlow API used to run the model. Options are ``call``, ``predict`` or ``predict_on_batch``. This should be the same as the API
                that is later used to run the model.

        Returns:
            If ``method`` was ``call``, returns a ``tf.function`` that runs the model. Otherwise, returns the same model but with its ``predict_func`` wrapped
            in a ``tf.function``.
        """
        if method == 'call':
            @tf.function(input_signature=[input_spec])
            def serve(x: list[tf.Tensor]) -> tf.Tensor:
                return model(x, training=False)

            return serve  # type: ignore
        else:
            model.run_eagerly = True  # type: ignore
            predict_func = model.make_predict_function()
            model.predict_function = tf.function(predict_func, input_signature=[tf.data.IteratorSpec((input_spec,))])  # type: ignore
            model.run_eagerly = False  # type: ignore
            return model

    @staticmethod
    def _dummy_run(model: MGModel | Callable[[list[tf.Tensor]], tf.Tensor], dummy_loader: Loader,
                   method: Literal["call", "predict", "predict_on_batch"]) -> float:
        """Runs the model on the dummy dataset.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to run on the dummy dataset.
            dummy_loader: The loader of the dummy dataset.
            method: The TensorFlow API used to run the model. Options are ``call``, ``predict`` or ``predict_on_batch``. This should be the same as the API
                that is later used to run the model.

        Returns:
            The duration in seconds of the dummy run.
        """
        elapsed = 0.0
        if method == 'call':
            for x, in dummy_loader.load():
                start = time.perf_counter()
                model(x)
                end = time.perf_counter()
                elapsed = end - start
                print("Tracing completed in ", elapsed, "s", sep='')
                break
        elif method == 'predict':
            assert isinstance(model, MGModel)
            start = time.perf_counter()
            model.predict(dummy_loader.load(), steps=dummy_loader.steps_per_epoch)
            end = time.perf_counter()
            elapsed = end - start
            print("Tracing completed in ", elapsed, "s", sep='')
        else:
            assert isinstance(model, MGModel)
            for x, in dummy_loader.load():
                start = time.perf_counter()
                model.predict_on_batch(x)
                end = time.perf_counter()
                elapsed = end - start
                print("Tracing completed in ", elapsed, "s", sep='')
                break
        return elapsed

    def compile(self, expr: str | Tree, verbose: bool = False, memoize: bool = False) -> MGModel:
        """Compiles a mG program into a TensorFlow model.

        Args:
            expr: The mG program to compile.
            verbose: If true, prints some debugging information during the compilation step.
            memoize: If true, memoize intermediate outputs during compilation.

        Returns:
            The TensorFlow model that implements ``expr``.
        """
        x, a = self.model_inputs[:2]
        e, i = None, None
        if self.config.use_edges and self.config.use_multiple_loader:
            e = self.model_inputs[-2]
            i = self.model_inputs[-1]
        elif self.config.use_edges:
            e = self.model_inputs[-1]
            i = None
        elif self.config.use_multiple_loader:
            e = None
            i = self.model_inputs[-1]

        self.visitor.initialize(IntermediateOutput(mg_parser.parse('__INPUT__'), x, a, e, i), memoize)
        tf.keras.backend.clear_session()
        normalized_expr_tree = mg_normalizer.normalize(expr if isinstance(expr, Tree) else mg_parser.parse(expr))
        outputs = self.visitor.visit(normalized_expr_tree)
        model = MGModel(self.model_inputs, outputs.x, normalized_expr_tree, self.visitor.layers, self.config,
                        self.visitor.used_psi, self.visitor.used_phi, self.visitor.used_sigma)
        if verbose is True:
            model.summary(expand_nested=True, show_trainable=True)
        return model

    def trace(self, model: MGModel, api: Literal["call", "predict", "predict_on_batch"]) -> Tuple[MGModel | Callable, float]:
        """Performs tracing on the model, and returns it, together with the time in seconds elapsed for tracing.

        The ``predict_on_batch`` API cannot be used if the graphs have edge labels as of TF 2.4.

        Args:
            model: The model to trace.
            api: The TensorFlow API intended to be used with the model. Options are ``call`` for the ``model()`` API,
                ``predict`` for the ``model.predict()`` API and ``predict_on_batch`` for the ``model.predict_on_batch()`` API.

        Returns:
            The model and the elapsed time in seconds for tracing.

        Raises:
            ValueError: The ``predict_on_batch`` API has been selected and the compiler's configuration is set for graphs with edge labels.
        """
        if api == 'predict_on_batch' and self.config.use_edges:
            raise ValueError("The predict_on_batch API, as of TF2.4, isn't compatible with graphs with edge labels.")
        if self.config.use_multiple_loader:
            dummy_loader = MultipleGraphLoader(self.dummy_dataset, batch_size=1, shuffle=False, epochs=1)
        else:
            dummy_loader = SingleGraphLoader(self.dummy_dataset, epochs=1)
        traced_model = MGCompiler._graph_mode_constructor(model, self.model_input_spec, api)
        compile_time = MGCompiler._dummy_run(traced_model, dummy_loader, api)
        return traced_model, compile_time
